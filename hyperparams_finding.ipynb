{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheS1n233/Distributed-Learning-Project5/blob/experiments/hyperparams_finding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Jn8pJTtuK_fI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb8025f-f5b9-4802-8fd4-7af3d0a19957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision matplotlib\n",
        "!pip install --upgrade torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X1mwT7cjK_fJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8PUoANc3K_fJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc84351e-b4b5-4adf-8ad2-bfd873f8f614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8_BgOMi8K_fK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing old\n"
      ],
      "metadata": {
        "id": "1wf-SQnnllr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Cutout\n",
        "\"\"\"class Cutout(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "        y1 = np.clip(y - self.size // 2, 0, h)\n",
        "        y2 = np.clip(y + self.size // 2, 0, h)\n",
        "        x1 = np.clip(x - self.size // 2, 0, w)\n",
        "        x2 = np.clip(x + self.size // 2, 0, w)\n",
        "        if len(img.shape) == 2:  # Handle grayscale images\n",
        "            img = img * mask\n",
        "        else:\n",
        "            img = img * mask[:, :, np.newaxis]\n",
        "\n",
        "        return Image.fromarray(np.uint8(img))\"\"\"\n",
        "\n",
        "\"\"\"# Mixup function\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    if alpha > 0.0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.0\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Data transformations with additional augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomRotation(15),\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    #transforms.RandomGrayscale(p=0.1),\n",
        "    #transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)),\n",
        "    #Cutout(size=8),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "start_time = time.time()\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Split training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Debugging: Check DataLoader outputs\n",
        "for i, (inputs, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "    if i == 10:  # Test first 10 batches\n",
        "        break\n",
        "print(f\"Data loading for 10 batches completed.\")"
      ],
      "metadata": {
        "id": "v2_0CSXml_zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing new"
      ],
      "metadata": {
        "id": "HW4AFSyol9_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6hkXaaVbK_fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3eb8973-6249-45d5-9efe-60e6a1e7d89d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset loading time: 2.10 seconds\n",
            "Batch 0: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 1: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 2: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 3: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 4: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 5: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 6: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 7: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 8: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 9: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Batch 10: inputs shape: torch.Size([64, 3, 24, 24]), labels shape: torch.Size([64])\n",
            "Data loading for 10 batches completed.\n"
          ]
        }
      ],
      "source": [
        "# # Define the transform to only convert the images to tensors (without normalization yet)\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "# # Load the CIFAR-100 training dataset\n",
        "# train_dataset = torchvision.datasets.CIFAR100(\n",
        "#     root='./data',\n",
        "#     train=True,\n",
        "#     download=True,\n",
        "#     transform=transform\n",
        "# )\n",
        "\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# # Initialize sums for calculating mean and std\n",
        "# mean = torch.zeros(3)\n",
        "# std = torch.zeros(3)\n",
        "\n",
        "# for images, _ in train_loader:\n",
        "#     # Compute mean and std for each channel\n",
        "#     mean += images.mean(dim=[0, 2, 3])  # Mean per channel (R, G, B)\n",
        "#     std += images.std(dim=[0, 2, 3])  # Std per channel (R, G, B)\n",
        "\n",
        "# mean /= len(train_loader)\n",
        "# std /= len(train_loader)\n",
        "\n",
        "# print(\"Mean: \", mean)\n",
        "# print(\"Std: \", std)\n",
        "\n",
        "\n",
        "# transform_train = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "#                          std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "# ])\n",
        "\n",
        "\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "#                          std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "# ])\n",
        "\n",
        "# Data preprocessing: training set\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(24),  # Randomly crop the image to (24, 24, 3)\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],  # Standard mean for CIFAR-100\n",
        "               std=[0.2023, 0.1994, 0.2010])  # Standard std for CIFAR-100\n",
        "])\n",
        "\n",
        "# Data preprocessing: test set\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.CenterCrop(24),  # Center crop the image to (24, 24, 3)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],  # Standard mean for CIFAR-100\n",
        "               std=[0.2023, 0.1994, 0.2010])  # Standard std for CIFAR-100\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "start_time = time.time()\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Split training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Debugging: Check DataLoader outputs\n",
        "for i, (inputs, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "    if i == 10:  # Test first 10 batches\n",
        "        break\n",
        "print(f\"Data loading for 10 batches completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define LeNet-5"
      ],
      "metadata": {
        "id": "bkwD0eWYmzlA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eXGl5EX6K_fK"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        # Layer convolutivi\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)  # 3 input channels, 64 output channels\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)  # 64 input channels, 64 output channels\n",
        "\n",
        "        # Layer fully connected\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)  # Dimensione calcolata per input 32x32 con due conv e max-pooling\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)  # Classificatore lineare per CIFAR-100\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer convolutivi con ReLU e max-pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
        "\n",
        "        # Flatten per i layer fully connected\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Layer fully connected con ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Classificatore lineare\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # Softmax per probabilit√†\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching"
      ],
      "metadata": {
        "id": "xiC1dWq4m79C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Cku-7tmkK_fL"
      },
      "outputs": [],
      "source": [
        "# Early Stopping Class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, delta=0, path='/content/drive/MyDrive/Early1checkpoint.pt', verbose=False):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, optimizer=None):\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
        "        checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss_min': val_loss\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "kvKHU3ohK_fL"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_random(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device, type_optimizer):\n",
        "\n",
        "    model.train()\n",
        "    scaler = GradScaler(device='cuda')  # Initialize mixed precision scaler\n",
        "    accumulation_steps = 4  # Gradient accumulation\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Training loop\n",
        "        train_loss_total = 0\n",
        "        train_correct, train_total = 0, 0\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            # inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, alpha)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast(device_type='cuda'):  # Mixed precision forward pass\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "            train_loss_total += loss.item() * accumulation_steps\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss_total / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "                with autocast(device_type='cuda'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_acc = 100. * correct / total\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        if epoch == 2 and train_acc < 1.0:\n",
        "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
        "            return None, None, None\n",
        "        if epoch == 3 and train_acc < 5.0:\n",
        "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
        "            return None, None, None\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Adjust learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    return train_loss, val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2CfN2CT0K_fL"
      },
      "outputs": [],
      "source": [
        "def random_search(train_loader, val_loader, model_class, device, num_trials, num_epochs, type_optimizer):\n",
        "    if type_optimizer == 'AdamW':\n",
        "        param_space = {\n",
        "            'lr': [1e-5, 1e-4, 5e-4, 1e-3, 5e-3],\n",
        "            'weight_decay': [4e-5, 1e-4, 1e-3, 1e-2],\n",
        "            'eps': [1e-8, 1e-4, 1e-6]\n",
        "        }\n",
        "    elif type_optimizer == 'SDGM':\n",
        "        param_space = {\n",
        "            'lr': [1e-4, 5e-4, 1e-3, 1e-2, 5e-2],\n",
        "            'momentum': [0.9, 0.95],\n",
        "            'weight_decay': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for trial in range(num_trials):\n",
        "\n",
        "        model = model_class().to(device)\n",
        "\n",
        "        if type_optimizer == 'AdamW':\n",
        "            lr = random.choice(param_space['lr'])\n",
        "            weight_decay = random.choice(param_space['weight_decay'])\n",
        "            eps = random.choice(param_space['eps'])\n",
        "            optimizer = optim.AdamW(\n",
        "                model.parameters(),\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                eps=eps\n",
        "            )\n",
        "\n",
        "            print(f\"Trial {trial + 1}/{num_trials} | lr: {lr} | weight_decay: {weight_decay} | eps: {eps}\")\n",
        "\n",
        "\n",
        "        elif type_optimizer == 'SDGM':\n",
        "            lr = random.choice(param_space['lr'])\n",
        "            weight_decay = random.choice(param_space['weight_decay'])\n",
        "            momentum = random.choice(param_space['momentum'])\n",
        "            optimizer = optim.SGD(\n",
        "                model.parameters(),\n",
        "                lr=lr,\n",
        "                momentum=momentum,\n",
        "                weight_decay=weight_decay\n",
        "            )\n",
        "\n",
        "            print(f\"Trial {trial + 1}/{num_trials} | lr: {lr} | weight_decay: {weight_decay} | momentum: {momentum} \")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
        "\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        train_loss, val_loss, val_acc = train_and_evaluate_random(\n",
        "            model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device=device,type_optimizer=type_optimizer\n",
        "        )\n",
        "\n",
        "        if train_loss is None:\n",
        "            print(f\"Skipping trial {trial + 1} due to low accuracy.\")\n",
        "            continue\n",
        "\n",
        "        if type_optimizer == 'AdamW':\n",
        "            results.append({\n",
        "                'lr': lr,\n",
        "                'weight_decay': weight_decay,\n",
        "                'eps': eps,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc\n",
        "            })\n",
        "            results.sort(key=lambda x: x['val_acc'], reverse=True)\n",
        "            best_hyperparams = results[0]\n",
        "\n",
        "            with open('/content/drive/MyDrive/best_hyperparams_AdamW.json', 'w') as f:\n",
        "                json.dump(best_hyperparams, f)\n",
        "            print(\"\\nBest Hyperparameters saved to /content/drive/MyDrive/best_hyperparams_AdamW.json\")\n",
        "\n",
        "            return best_hyperparams\n",
        "\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            results.append({\n",
        "                'lr': lr,\n",
        "                'weight_decay': weight_decay,\n",
        "                'momentum': momentum,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc\n",
        "            })\n",
        "\n",
        "            results.sort(key=lambda x: x['val_acc'], reverse=True)\n",
        "            best_hyperparams = results[0]\n",
        "\n",
        "            with open('/content/drive/MyDrive/best_hyperparams_SGDM.json', 'w') as f:\n",
        "                json.dump(best_hyperparams, f)\n",
        "            print(\"\\nBest Hyperparameters saved to /content/drive/MyDrive/best_hyperparams_SGDM.json\")\n",
        "\n",
        "            return best_hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "G6iAaDNOK_fM"
      },
      "outputs": [],
      "source": [
        "def check_initial_loss(model, train_loader, device):\n",
        "    \"\"\"Step 1: Check initial loss at initialization\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs, labels = next(iter(train_loader))\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        expected_loss = math.log(100)\n",
        "        print(f\"Initial loss: {loss.item():.4f}\")\n",
        "        print(f\"Expected loss: {expected_loss:.4f}\")\n",
        "        return abs(loss.item() - expected_loss) < 1.0\n",
        "\n",
        "def overfit_small_sample(model, train_loader, device, type_optimizer, max_iterations=1000):\n",
        "    \"\"\"Step 2: Overfit a small sample\"\"\"\n",
        "    # Get small sample (5 minibatches)\n",
        "    small_dataset = []\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        if i >= 5:\n",
        "            break\n",
        "        small_dataset.extend(list(zip(inputs, labels)))\n",
        "\n",
        "    small_loader = torch.utils.data.DataLoader(small_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    if type_optimizer == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "    elif type_optimizer == 'SGDM':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in small_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        accuracy = 100. * correct / total\n",
        "        avg_loss = total_loss / len(small_loader)\n",
        "\n",
        "        if iteration % 50 == 0:\n",
        "            print(f\"Iteration {iteration}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        if accuracy > 99:\n",
        "            print(\"Successfully overfit small sample!\")\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def find_initial_lr(model, train_loader, device, type_optimizer, lrs=[1e-1, 1e-2, 1e-3, 1e-4]):\n",
        "    \"\"\"Step 3: Find LR that makes loss go down\"\"\"\n",
        "    best_lr = None\n",
        "    min_loss_decrease = float('inf')\n",
        "\n",
        "    for lr in lrs:\n",
        "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "\n",
        "        if type_optimizer == 'AdamW':\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "        initial_loss = None\n",
        "        final_loss = None\n",
        "\n",
        "        print(f\"\\nTrying learning rate: {lr}\")\n",
        "\n",
        "        for iteration in range(100):\n",
        "            inputs, labels = next(iter(train_loader))\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            if iteration == 0:\n",
        "                initial_loss = loss.item()\n",
        "            final_loss = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 20 == 0:\n",
        "                print(f\"Iteration {iteration}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        loss_decrease = initial_loss - final_loss\n",
        "        print(f\"Loss decrease: {loss_decrease:.4f}\")\n",
        "\n",
        "        if loss_decrease > 0 and loss_decrease < min_loss_decrease:\n",
        "            min_loss_decrease = loss_decrease\n",
        "            best_lr = lr\n",
        "\n",
        "    return best_lr\n",
        "\n",
        "def coarse_grid_search(model, train_loader, val_loader, device, best_lr, type_optimizer):\n",
        "    \"\"\"Step 4: Coarse grid search\"\"\"\n",
        "\n",
        "\n",
        "    lrs = [best_lr / 2, best_lr, best_lr * 2]\n",
        "    weight_decays = [0, 1e-5, 1e-4]\n",
        "    momentum_arr = [0.9, 0.95]\n",
        "    eps =  [1e-8, 1e-4, 1e-6]\n",
        "    results = []\n",
        "\n",
        "\n",
        "    for lr in lrs:\n",
        "        for wd in weight_decays:\n",
        "            model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "\n",
        "            if type_optimizer == 'AdamW':\n",
        "                eps = random.choice(eps)\n",
        "                print(f\"\\nTrying LR: {lr}, Weight Decay: {wd}, Eps: {eps}\")\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, eps=eps)\n",
        "            elif type_optimizer == 'SGDM':\n",
        "                print(f\"\\nTrying LR: {lr}, Weight Decay: {wd}, Momentum: {momentum}\")\n",
        "                momentum = random.choice(momentum_arr)\n",
        "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "\n",
        "\n",
        "            train_losses = []\n",
        "            val_accuracies = []\n",
        "\n",
        "            for epoch in range(5):\n",
        "                # Training\n",
        "                model.train()\n",
        "                total_loss = 0\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                # Validation\n",
        "                model.eval()\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in val_loader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        _, predicted = outputs.max(1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                train_losses.append(total_loss / len(train_loader))\n",
        "                val_accuracies.append(100. * correct / total)\n",
        "\n",
        "                print(f\"Epoch {epoch + 1}, Loss: {train_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "\n",
        "            if type_optimizer == 'AdamW':\n",
        "                results.append({\n",
        "                    'lr': lr,\n",
        "                    'weight_decay': wd,\n",
        "                    'eps': eps,\n",
        "                    'final_loss': train_losses[-1],\n",
        "                    'final_val_acc': val_accuracies[-1],\n",
        "                    'momentum': momentum\n",
        "                })\n",
        "            elif type_optimizer == 'SGDM':\n",
        "                results.append({\n",
        "                    'lr': lr,\n",
        "                    'weight_decay': wd,\n",
        "                    'momentum': momentum,\n",
        "                    'final_loss': train_losses[-1],\n",
        "                    'final_val_acc': val_accuracies[-1],\n",
        "                    'momentum': momentum\n",
        "                })\n",
        "\n",
        "    results.sort(key=lambda x: x['final_val_acc'], reverse=True)\n",
        "    return results[0]\n",
        "\n",
        "def train_model_with_early_stopping(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs, device, patience=5):\n",
        "    \"\"\"Train model with early stopping\"\"\"\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        train_accuracies.append(100. * correct / total)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "        val_accuracies.append(100. * correct / total)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, \"\n",
        "              f\"Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
        "              f\"Val Loss: {val_losses[-1]:.4f}, \"\n",
        "              f\"Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "AMiB-6BlK_fM"
      },
      "outputs": [],
      "source": [
        "def use_grid_search(model, device, train_loader, val_loader, test_loader, type_optimizer='AdamW'):\n",
        "\n",
        "    if type_optimizer != 'AdamW' and type_optimizer != 'SGDM':\n",
        "        raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
        "\n",
        "    print(\"Starting grid search...\")\n",
        "    # Step 1: Check initial loss\n",
        "    if check_initial_loss(model, train_loader, device):\n",
        "        print(\"Initial loss check passed.\")\n",
        "\n",
        "    # Step 2: Overfit small sample\n",
        "    if overfit_small_sample(model, train_loader, device, type_optimizer):\n",
        "        print(\"Successfully overfit small sample.\")\n",
        "\n",
        "    # Step 3: Find learning rate\n",
        "    best_lr = find_initial_lr(model, train_loader, device, type_optimizer)\n",
        "    print(f\"Best learning rate: {best_lr}\")\n",
        "\n",
        "    # Step 4: Coarse grid search for LR and weight decay\n",
        "    best_model_params = coarse_grid_search(model, train_loader, val_loader, device, best_lr, type_optimizer)\n",
        "    print(f\"Best model parameters: {best_model_params}\")\n",
        "\n",
        "    for improvement in range(1, 3):\n",
        "        # Step 5: Refine grid search and train for longer\n",
        "        # Train with the best parameters found\n",
        "        best_lr = best_model_params['lr']\n",
        "        best_weight_decay = best_model_params['weight_decay']\n",
        "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "        if type_optimizer == 'AdamW':\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay, eps=best_model_params['eps'])\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=best_lr, weight_decay=best_weight_decay, momentum=best_model_params['momentum'])\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=20*improvement)\n",
        "\n",
        "\n",
        "        # Train the model for 10-20 epochs\n",
        "        model, train_losses, val_losses, train_accuracies, val_accuracies = train_model_with_early_stopping(\n",
        "            model, optimizer, F.cross_entropy, scheduler, train_loader, val_loader, num_epochs=20*improvement, device=device, patience=5\n",
        "        )\n",
        "\n",
        "    # Step 6: Look at learning curves\n",
        "    # Plot training and validation losses and accuracies\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
        "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ih1KvTcrK_fM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "14acefde-8b8f-4a2f-f36a-05779b4d252a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Trial 1/10 | lr: 0.0005 | weight_decay: 0.01 | eps: 1e-08\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (64x576 and 1600x384)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b0e8e2aad196>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbest_hyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfind_alg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random_search'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mbest_hyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLeNet5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-d7e4fcbb0fae>\u001b[0m in \u001b[0;36mrandom_search\u001b[0;34m(train_loader, val_loader, model_class, device, num_trials, num_epochs, type_optimizer)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         train_loss, val_loss, val_acc = train_and_evaluate_random(\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-31-2b5b1d3a9cb6>\u001b[0m in \u001b[0;36mtrain_and_evaluate_random\u001b[0;34m(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device, type_optimizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Mixed precision forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-490326c0e52c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Layer fully connected con ReLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x576 and 1600x384)"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    find_alg = 'random_search' # grid_search or random_search\n",
        "    type_optimizer = 'AdamW'  # AdamW or SDGM\n",
        "\n",
        "    model = LeNet5().to(device)\n",
        "    if find_alg == 'grid_search':\n",
        "        best_hyperparams = use_grid_search(model, device, train_loader, val_loader, test_loader, type_optimizer=type_optimizer)\n",
        "    elif find_alg == 'random_search':\n",
        "        best_hyperparams = random_search(train_loader, val_loader, model_class=LeNet5, device=device, num_trials=10, num_epochs=5, type_optimizer=type_optimizer)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}