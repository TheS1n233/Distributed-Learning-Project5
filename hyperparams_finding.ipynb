{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheS1n233/Distributed-Learning-Project5/blob/experiments/hyperparams_finding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Jn8pJTtuK_fI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fb8025f-f5b9-4802-8fd4-7af3d0a19957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision matplotlib\n",
        "!pip install --upgrade torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X1mwT7cjK_fJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8PUoANc3K_fJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc84351e-b4b5-4adf-8ad2-bfd873f8f614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8_BgOMi8K_fK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing old\n"
      ],
      "metadata": {
        "id": "1wf-SQnnllr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Cutout\n",
        "\"\"\"class Cutout(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "        y1 = np.clip(y - self.size // 2, 0, h)\n",
        "        y2 = np.clip(y + self.size // 2, 0, h)\n",
        "        x1 = np.clip(x - self.size // 2, 0, w)\n",
        "        x2 = np.clip(x + self.size // 2, 0, w)\n",
        "        if len(img.shape) == 2:  # Handle grayscale images\n",
        "            img = img * mask\n",
        "        else:\n",
        "            img = img * mask[:, :, np.newaxis]\n",
        "\n",
        "        return Image.fromarray(np.uint8(img))\"\"\"\n",
        "\n",
        "\"\"\"# Mixup function\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    if alpha > 0.0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.0\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Data transformations with additional augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    #transforms.RandomRotation(15),\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    #transforms.RandomGrayscale(p=0.1),\n",
        "    #transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)),\n",
        "    #Cutout(size=8),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "start_time = time.time()\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Split training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Debugging: Check DataLoader outputs\n",
        "for i, (inputs, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "    if i == 10:  # Test first 10 batches\n",
        "        break\n",
        "print(f\"Data loading for 10 batches completed.\")"
      ],
      "metadata": {
        "id": "v2_0CSXml_zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing new"
      ],
      "metadata": {
        "id": "HW4AFSyol9_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "6hkXaaVbK_fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5820eb26-d597-4f4c-c6f9-93beea803059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset loading time: 1.86 seconds\n",
            "Batch 0: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 1: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 2: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 3: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 4: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 5: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 6: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 7: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 8: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 9: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 10: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Data loading for 10 batches completed.\n"
          ]
        }
      ],
      "source": [
        "# # Define the transform to only convert the images to tensors (without normalization yet)\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "# # Load the CIFAR-100 training dataset\n",
        "# train_dataset = torchvision.datasets.CIFAR100(\n",
        "#     root='./data',\n",
        "#     train=True,\n",
        "#     download=True,\n",
        "#     transform=transform\n",
        "# )\n",
        "\n",
        "\n",
        "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# # Initialize sums for calculating mean and std\n",
        "# mean = torch.zeros(3)\n",
        "# std = torch.zeros(3)\n",
        "\n",
        "# for images, _ in train_loader:\n",
        "#     # Compute mean and std for each channel\n",
        "#     mean += images.mean(dim=[0, 2, 3])  # Mean per channel (R, G, B)\n",
        "#     std += images.std(dim=[0, 2, 3])  # Std per channel (R, G, B)\n",
        "\n",
        "# mean /= len(train_loader)\n",
        "# std /= len(train_loader)\n",
        "\n",
        "# print(\"Mean: \", mean)\n",
        "# print(\"Std: \", std)\n",
        "\n",
        "\n",
        "# transform_train = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "#                          std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "# ])\n",
        "\n",
        "\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "#                          std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "# ])\n",
        "\n",
        "# Data preprocessing: training set\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop((32, 32), padding=4),  # Randomly crop the image to (24, 24, 3)\n",
        "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],  # Standard mean for CIFAR-100\n",
        "               std=[0.2023, 0.1994, 0.2010])  # Standard std for CIFAR-100\n",
        "])\n",
        "\n",
        "# Data preprocessing: test set\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.CenterCrop((32, 32)),  # Center crop the image to (24, 24, 3)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],  # Standard mean for CIFAR-100\n",
        "               std=[0.2023, 0.1994, 0.2010])  # Standard std for CIFAR-100\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "start_time = time.time()\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Split training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Debugging: Check DataLoader outputs\n",
        "for i, (inputs, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "    if i == 10:  # Test first 10 batches\n",
        "        break\n",
        "print(f\"Data loading for 10 batches completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define LeNet-5"
      ],
      "metadata": {
        "id": "bkwD0eWYmzlA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "eXGl5EX6K_fK"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        # Layer convolutivi\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)  # 3 input channels, 64 output channels\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)  # 64 input channels, 64 output channels\n",
        "\n",
        "        # Layer fully connected\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)  # Dimensione calcolata per input 32x32 con due conv e max-pooling\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)  # Classificatore lineare per CIFAR-100\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer convolutivi con ReLU e max-pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
        "\n",
        "        # Flatten per i layer fully connected\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Layer fully connected con ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Classificatore lineare\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # Softmax per probabilit√†\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching"
      ],
      "metadata": {
        "id": "xiC1dWq4m79C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Cku-7tmkK_fL"
      },
      "outputs": [],
      "source": [
        "# Early Stopping Class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, delta=0, path='/content/drive/MyDrive/Early1checkpoint.pt', verbose=False):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, optimizer=None):\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
        "        checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss_min': val_loss\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)\n",
        "        self.val_loss_min = val_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "kvKHU3ohK_fL"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_random(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device, type_optimizer):\n",
        "\n",
        "    model.train()\n",
        "    scaler = GradScaler(device='cuda')  # Initialize mixed precision scaler\n",
        "    accumulation_steps = 4  # Gradient accumulation\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Training loop\n",
        "        train_loss_total = 0\n",
        "        train_correct, train_total = 0, 0\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            # inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, alpha)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast(device_type='cuda'):  # Mixed precision forward pass\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels) / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "\n",
        "            train_loss_total += loss.item() * accumulation_steps\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss_total / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "                with autocast(device_type='cuda'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_acc = 100. * correct / total\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        if epoch == 2 and train_acc < 1.0:\n",
        "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
        "            return None, None, None\n",
        "        if epoch == 3 and train_acc < 5.0:\n",
        "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
        "            return None, None, None\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Adjust learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "    return train_loss, val_loss, val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "2CfN2CT0K_fL"
      },
      "outputs": [],
      "source": [
        "def random_search(train_loader, val_loader, model_class, device, num_trials, num_epochs, type_optimizer):\n",
        "    if type_optimizer == 'AdamW':\n",
        "        param_space = {\n",
        "            'lr': [1e-5, 1e-4, 5e-4, 1e-3, 5e-3],\n",
        "            'weight_decay': [7e-3, 1e-2, 5e-2, 1e-1],\n",
        "            'eps': [1e-8, 1e-4, 1e-6]\n",
        "        }\n",
        "    elif type_optimizer == 'SDGM':\n",
        "        param_space = {\n",
        "            'lr': [1e-4, 5e-4, 1e-3, 1e-2, 5e-2],\n",
        "            'momentum': [0.9, 0.95],\n",
        "            'weight_decay': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for trial in range(num_trials):\n",
        "\n",
        "        model = model_class().to(device)\n",
        "\n",
        "        if type_optimizer == 'AdamW':\n",
        "            lr = random.choice(param_space['lr'])\n",
        "            weight_decay = random.choice(param_space['weight_decay'])\n",
        "            eps = random.choice(param_space['eps'])\n",
        "            optimizer = optim.AdamW(\n",
        "                model.parameters(),\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                eps=eps\n",
        "            )\n",
        "\n",
        "            print(f\"Trial {trial + 1}/{num_trials} | lr: {lr} | weight_decay: {weight_decay} | eps: {eps}\")\n",
        "\n",
        "\n",
        "        elif type_optimizer == 'SDGM':\n",
        "            lr = random.choice(param_space['lr'])\n",
        "            weight_decay = random.choice(param_space['weight_decay'])\n",
        "            momentum = random.choice(param_space['momentum'])\n",
        "            optimizer = optim.SGD(\n",
        "                model.parameters(),\n",
        "                lr=lr,\n",
        "                momentum=momentum,\n",
        "                weight_decay=weight_decay\n",
        "            )\n",
        "\n",
        "            print(f\"Trial {trial + 1}/{num_trials} | lr: {lr} | weight_decay: {weight_decay} | momentum: {momentum} \")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
        "\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        train_loss, val_loss, val_acc = train_and_evaluate_random(\n",
        "            model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device=device,type_optimizer=type_optimizer\n",
        "        )\n",
        "\n",
        "        if train_loss is None:\n",
        "            print(f\"Skipping trial {trial + 1} due to low accuracy.\")\n",
        "            continue\n",
        "\n",
        "        if type_optimizer == 'AdamW':\n",
        "            results.append({\n",
        "                'lr': lr,\n",
        "                'weight_decay': weight_decay,\n",
        "                'eps': eps,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc\n",
        "            })\n",
        "\n",
        "\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            results.append({\n",
        "                'lr': lr,\n",
        "                'weight_decay': weight_decay,\n",
        "                'momentum': momentum,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc\n",
        "            })\n",
        "\n",
        "    results.sort(key=lambda x: x['val_acc'], reverse=True)\n",
        "    best_hyperparams = results[0]\n",
        "\n",
        "    save_path = 'best_hyperparams_'+ type_optimizer\n",
        "    with open('/content/drive/MyDrive/' + save_path+ '.json', 'w') as f:\n",
        "                json.dump(best_hyperparams, f)\n",
        "    print(\"\\nBest Hyperparameters saved to /content/drive/MyDrive/best_hyperparams_SGDM.json\")\n",
        "\n",
        "    return best_hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "G6iAaDNOK_fM"
      },
      "outputs": [],
      "source": [
        "def check_initial_loss(model, train_loader, device):\n",
        "    \"\"\"Step 1: Check initial loss at initialization\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs, labels = next(iter(train_loader))\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        expected_loss = math.log(100)\n",
        "        print(f\"Initial loss: {loss.item():.4f}\")\n",
        "        print(f\"Expected loss: {expected_loss:.4f}\")\n",
        "        return abs(loss.item() - expected_loss) < 1.0\n",
        "\n",
        "def overfit_small_sample(model, train_loader, device, type_optimizer, max_iterations=1000):\n",
        "    \"\"\"Step 2: Overfit a small sample\"\"\"\n",
        "    # Get small sample (5 minibatches)\n",
        "    small_dataset = []\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        if i >= 5:\n",
        "            break\n",
        "        small_dataset.extend(list(zip(inputs, labels)))\n",
        "\n",
        "    small_loader = torch.utils.data.DataLoader(small_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    if type_optimizer == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "    elif type_optimizer == 'SGDM':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in small_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        accuracy = 100. * correct / total\n",
        "        avg_loss = total_loss / len(small_loader)\n",
        "\n",
        "        if iteration % 50 == 0:\n",
        "            print(f\"Iteration {iteration}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        if accuracy > 99:\n",
        "            print(\"Successfully overfit small sample!\")\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def find_initial_lr(model, train_loader, device, type_optimizer, lrs=[1e-5, 1e-4, 5e-4, 1e-3, 5e-3]):\n",
        "    \"\"\"Step 3: Find LR that makes loss go down\"\"\"\n",
        "    best_lr = None\n",
        "    min_loss_decrease = float('inf')\n",
        "\n",
        "    for lr in lrs:\n",
        "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "\n",
        "        if type_optimizer == 'AdamW':\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "        initial_loss = None\n",
        "        final_loss = None\n",
        "\n",
        "        print(f\"\\nTrying learning rate: {lr}\")\n",
        "\n",
        "        for iteration in range(100):\n",
        "            inputs, labels = next(iter(train_loader))\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            if iteration == 0:\n",
        "                initial_loss = loss.item()\n",
        "            final_loss = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 20 == 0:\n",
        "                print(f\"Iteration {iteration}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        loss_decrease = initial_loss - final_loss\n",
        "        print(f\"Loss decrease: {loss_decrease:.4f}\")\n",
        "\n",
        "        if loss_decrease > 0 and loss_decrease < min_loss_decrease:\n",
        "            min_loss_decrease = loss_decrease\n",
        "            best_lr = lr\n",
        "\n",
        "    return best_lr\n",
        "\n",
        "def coarse_grid_search(model, train_loader, val_loader, device, best_lr, type_optimizer):\n",
        "    \"\"\"Step 4: Coarse grid search\"\"\"\n",
        "\n",
        "\n",
        "    lrs = [best_lr / 2, best_lr, best_lr * 2]\n",
        "    weight_decays = [4e-5, 1e-4, 1e-3, 1e-2]\n",
        "    momentum_arr = [0.9, 0.95]\n",
        "    eps =  [1e-8, 1e-4, 1e-6]\n",
        "    results = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for lr in lrs:\n",
        "        for wd in weight_decays:\n",
        "            model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "\n",
        "            if type_optimizer == 'AdamW':\n",
        "                eps = random.choice(eps)\n",
        "                print(f\"\\nTrying LR: {lr}, Weight Decay: {wd}, Eps: {eps}\")\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, eps=eps)\n",
        "            elif type_optimizer == 'SGDM':\n",
        "                print(f\"\\nTrying LR: {lr}, Weight Decay: {wd}, Momentum: {momentum}\")\n",
        "                momentum = random.choice(momentum_arr)\n",
        "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "\n",
        "\n",
        "            train_losses = []\n",
        "            val_accuracies = []\n",
        "\n",
        "            for epoch in range(5):\n",
        "                # Training\n",
        "                model.train()\n",
        "                total_loss = 0\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                # Validation\n",
        "                model.eval()\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in val_loader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        _, predicted = outputs.max(1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                train_losses.append(total_loss / len(train_loader))\n",
        "                val_accuracies.append(100. * correct / total)\n",
        "\n",
        "                print(f\"Epoch {epoch + 1}, Loss: {train_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "\n",
        "            if type_optimizer == 'AdamW':\n",
        "                results.append({\n",
        "                    'lr': lr,\n",
        "                    'weight_decay': wd,\n",
        "                    'eps': eps,\n",
        "                    'final_loss': train_losses[-1],\n",
        "                    'final_val_acc': val_accuracies[-1],\n",
        "                    'momentum': momentum\n",
        "                })\n",
        "            elif type_optimizer == 'SGDM':\n",
        "                results.append({\n",
        "                    'lr': lr,\n",
        "                    'weight_decay': wd,\n",
        "                    'momentum': momentum,\n",
        "                    'final_loss': train_losses[-1],\n",
        "                    'final_val_acc': val_accuracies[-1],\n",
        "                    'momentum': momentum\n",
        "                })\n",
        "\n",
        "    results.sort(key=lambda x: x['final_val_acc'], reverse=True)\n",
        "    return results[0]\n",
        "\n",
        "def train_model_with_early_stopping(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs, device, patience=5):\n",
        "    \"\"\"Train model with early stopping\"\"\"\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        train_accuracies.append(100. * correct / total)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "        val_accuracies.append(100. * correct / total)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, \"\n",
        "              f\"Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
        "              f\"Val Loss: {val_losses[-1]:.4f}, \"\n",
        "              f\"Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "AMiB-6BlK_fM"
      },
      "outputs": [],
      "source": [
        "def use_grid_search(model, device, train_loader, val_loader, test_loader, type_optimizer='AdamW'):\n",
        "\n",
        "    if type_optimizer != 'AdamW' and type_optimizer != 'SGDM':\n",
        "        raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
        "\n",
        "    print(\"Starting grid search...\")\n",
        "    # Step 1: Check initial loss\n",
        "    if check_initial_loss(model, train_loader, device):\n",
        "        print(\"Initial loss check passed.\")\n",
        "\n",
        "    # Step 2: Overfit small sample\n",
        "    if overfit_small_sample(model, train_loader, device, type_optimizer):\n",
        "        print(\"Successfully overfit small sample.\")\n",
        "\n",
        "    # Step 3: Find learning rate\n",
        "    best_lr = find_initial_lr(model, train_loader, device, type_optimizer)\n",
        "    print(f\"Best learning rate: {best_lr}\")\n",
        "\n",
        "    # Step 4: Coarse grid search for LR and weight decay\n",
        "    best_model_params = coarse_grid_search(model, train_loader, val_loader, device, best_lr, type_optimizer)\n",
        "    print(f\"Best model parameters: {best_model_params}\")\n",
        "\n",
        "    for improvement in range(1, 3):\n",
        "        # Step 5: Refine grid search and train for longer\n",
        "        # Train with the best parameters found\n",
        "        best_lr = best_model_params['lr']\n",
        "        best_weight_decay = best_model_params['weight_decay']\n",
        "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "        if type_optimizer == 'AdamW':\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay, eps=best_model_params['eps'])\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=best_lr, weight_decay=best_weight_decay, momentum=best_model_params['momentum'])\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=20*improvement)\n",
        "\n",
        "\n",
        "        # Train the model for 10-20 epochs\n",
        "        model, train_losses, val_losses, train_accuracies, val_accuracies = train_model_with_early_stopping(\n",
        "            model, optimizer, F.cross_entropy, scheduler, train_loader, val_loader, num_epochs=20*improvement, device=device, patience=5\n",
        "        )\n",
        "\n",
        "    # Step 6: Look at learning curves\n",
        "    # Plot training and validation losses and accuracies\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
        "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ih1KvTcrK_fM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "feaa1114-c727-4baf-84a2-f38ed5c8418b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Trial 1/30 | lr: 0.001 | weight_decay: 0.007 | eps: 0.0001\n",
            "Epoch 1/5 | Train Loss: 4.3807 | Train Acc: 4.37% | Val Loss: 4.2113 | Val Acc: 6.65%\n",
            "Epoch 2/5 | Train Loss: 4.0883 | Train Acc: 8.76% | Val Loss: 4.0141 | Val Acc: 9.78%\n",
            "Epoch 3/5 | Train Loss: 3.9265 | Train Acc: 12.23% | Val Loss: 3.9000 | Val Acc: 12.85%\n",
            "Epoch 4/5 | Train Loss: 3.7901 | Train Acc: 15.08% | Val Loss: 3.7675 | Val Acc: 15.27%\n",
            "Epoch 5/5 | Train Loss: 3.7001 | Train Acc: 17.25% | Val Loss: 3.7001 | Val Acc: 17.25%\n",
            "Trial 2/30 | lr: 0.001 | weight_decay: 0.1 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.3605 | Train Acc: 4.67% | Val Loss: 4.1873 | Val Acc: 7.06%\n",
            "Epoch 2/5 | Train Loss: 4.0974 | Train Acc: 8.73% | Val Loss: 4.0555 | Val Acc: 9.46%\n",
            "Epoch 3/5 | Train Loss: 3.9365 | Train Acc: 11.89% | Val Loss: 3.8850 | Val Acc: 12.86%\n",
            "Epoch 4/5 | Train Loss: 3.8010 | Train Acc: 14.78% | Val Loss: 3.7867 | Val Acc: 15.34%\n",
            "Epoch 5/5 | Train Loss: 3.7236 | Train Acc: 16.75% | Val Loss: 3.7252 | Val Acc: 16.65%\n",
            "Trial 3/30 | lr: 0.0005 | weight_decay: 0.05 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.3838 | Train Acc: 4.17% | Val Loss: 4.2253 | Val Acc: 6.44%\n",
            "Epoch 2/5 | Train Loss: 4.1023 | Train Acc: 8.82% | Val Loss: 4.0147 | Val Acc: 10.86%\n",
            "Epoch 3/5 | Train Loss: 3.9338 | Train Acc: 12.04% | Val Loss: 3.9024 | Val Acc: 12.92%\n",
            "Epoch 4/5 | Train Loss: 3.8159 | Train Acc: 14.63% | Val Loss: 3.8083 | Val Acc: 15.37%\n",
            "Epoch 5/5 | Train Loss: 3.7430 | Train Acc: 16.51% | Val Loss: 3.7525 | Val Acc: 16.36%\n",
            "Trial 4/30 | lr: 0.0001 | weight_decay: 0.007 | eps: 0.0001\n",
            "Epoch 1/5 | Train Loss: 4.5725 | Train Acc: 1.84% | Val Loss: 4.4854 | Val Acc: 2.76%\n",
            "Epoch 2/5 | Train Loss: 4.3496 | Train Acc: 5.13% | Val Loss: 4.2751 | Val Acc: 5.37%\n",
            "Epoch 3/5 | Train Loss: 4.2023 | Train Acc: 7.62% | Val Loss: 4.1763 | Val Acc: 7.84%\n",
            "Epoch 4/5 | Train Loss: 4.1337 | Train Acc: 8.66% | Val Loss: 4.1284 | Val Acc: 9.03%\n",
            "Epoch 5/5 | Train Loss: 4.1029 | Train Acc: 9.48% | Val Loss: 4.1120 | Val Acc: 9.35%\n",
            "Trial 5/30 | lr: 0.005 | weight_decay: 0.05 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.5752 | Train Acc: 1.59% | Val Loss: 4.5033 | Val Acc: 1.80%\n",
            "Epoch 2/5 | Train Loss: 4.4749 | Train Acc: 2.48% | Val Loss: 4.4510 | Val Acc: 2.65%\n",
            "Epoch 3/5 | Train Loss: 4.4255 | Train Acc: 2.97% | Val Loss: 4.4131 | Val Acc: 2.63%\n",
            "Epoch 4: Train accuracy 3.77% too low, skipping to next trial...\n",
            "Skipping trial 5 due to low accuracy.\n",
            "Trial 6/30 | lr: 0.005 | weight_decay: 0.1 | eps: 1e-06\n",
            "Epoch 1/5 | Train Loss: 4.6208 | Train Acc: 1.01% | Val Loss: 4.6072 | Val Acc: 0.93%\n",
            "Epoch 2/5 | Train Loss: 4.6065 | Train Acc: 0.99% | Val Loss: 4.6075 | Val Acc: 1.14%\n",
            "Epoch 3: Train accuracy 0.96% too low, skipping to next trial...\n",
            "Skipping trial 6 due to low accuracy.\n",
            "Trial 7/30 | lr: 0.0005 | weight_decay: 0.01 | eps: 0.0001\n",
            "Epoch 1/5 | Train Loss: 4.4093 | Train Acc: 4.09% | Val Loss: 4.2193 | Val Acc: 6.63%\n",
            "Epoch 2/5 | Train Loss: 4.1149 | Train Acc: 8.60% | Val Loss: 4.0281 | Val Acc: 10.40%\n",
            "Epoch 3/5 | Train Loss: 3.9687 | Train Acc: 11.54% | Val Loss: 3.9273 | Val Acc: 13.09%\n",
            "Epoch 4/5 | Train Loss: 3.8658 | Train Acc: 13.76% | Val Loss: 3.8379 | Val Acc: 14.53%\n",
            "Epoch 5/5 | Train Loss: 3.7981 | Train Acc: 15.26% | Val Loss: 3.8064 | Val Acc: 15.26%\n",
            "Trial 8/30 | lr: 0.005 | weight_decay: 0.007 | eps: 0.0001\n",
            "Epoch 1/5 | Train Loss: 4.5091 | Train Acc: 2.04% | Val Loss: 4.4489 | Val Acc: 2.40%\n",
            "Epoch 2/5 | Train Loss: 4.4161 | Train Acc: 2.88% | Val Loss: 4.4267 | Val Acc: 2.87%\n",
            "Epoch 3/5 | Train Loss: 4.3567 | Train Acc: 3.73% | Val Loss: 4.3302 | Val Acc: 3.87%\n",
            "Epoch 4: Train accuracy 4.84% too low, skipping to next trial...\n",
            "Skipping trial 8 due to low accuracy.\n",
            "Trial 9/30 | lr: 0.001 | weight_decay: 0.1 | eps: 0.0001\n",
            "Epoch 1/5 | Train Loss: 4.3827 | Train Acc: 4.14% | Val Loss: 4.2297 | Val Acc: 6.41%\n",
            "Epoch 2/5 | Train Loss: 4.1088 | Train Acc: 8.28% | Val Loss: 4.0413 | Val Acc: 10.23%\n",
            "Epoch 3/5 | Train Loss: 3.9357 | Train Acc: 11.90% | Val Loss: 3.9002 | Val Acc: 12.54%\n",
            "Epoch 4/5 | Train Loss: 3.7950 | Train Acc: 15.08% | Val Loss: 3.7743 | Val Acc: 15.97%\n",
            "Epoch 5/5 | Train Loss: 3.6960 | Train Acc: 17.41% | Val Loss: 3.7119 | Val Acc: 17.57%\n",
            "Trial 10/30 | lr: 0.0001 | weight_decay: 0.05 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.5094 | Train Acc: 2.81% | Val Loss: 4.3722 | Val Acc: 4.98%\n",
            "Epoch 2/5 | Train Loss: 4.2666 | Train Acc: 6.25% | Val Loss: 4.2151 | Val Acc: 7.63%\n",
            "Epoch 3/5 | Train Loss: 4.1531 | Train Acc: 8.21% | Val Loss: 4.1329 | Val Acc: 8.83%\n",
            "Epoch 4/5 | Train Loss: 4.0832 | Train Acc: 9.99% | Val Loss: 4.0846 | Val Acc: 9.77%\n",
            "Epoch 5/5 | Train Loss: 4.0524 | Train Acc: 10.71% | Val Loss: 4.0689 | Val Acc: 10.28%\n",
            "Trial 11/30 | lr: 0.0001 | weight_decay: 0.007 | eps: 1e-06\n",
            "Epoch 1/5 | Train Loss: 4.5070 | Train Acc: 3.17% | Val Loss: 4.3630 | Val Acc: 4.65%\n",
            "Epoch 2/5 | Train Loss: 4.2496 | Train Acc: 6.64% | Val Loss: 4.1923 | Val Acc: 7.76%\n",
            "Epoch 3/5 | Train Loss: 4.1315 | Train Acc: 8.94% | Val Loss: 4.1129 | Val Acc: 10.01%\n",
            "Epoch 4/5 | Train Loss: 4.0665 | Train Acc: 10.55% | Val Loss: 4.0592 | Val Acc: 11.11%\n",
            "Epoch 5/5 | Train Loss: 4.0320 | Train Acc: 11.35% | Val Loss: 4.0391 | Val Acc: 11.27%\n",
            "Trial 12/30 | lr: 0.005 | weight_decay: 0.01 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.7258 | Train Acc: 1.04% | Val Loss: 4.6066 | Val Acc: 0.82%\n",
            "Epoch 2/5 | Train Loss: 4.6065 | Train Acc: 0.99% | Val Loss: 4.6078 | Val Acc: 0.78%\n",
            "Epoch 3/5 | Train Loss: 4.6062 | Train Acc: 1.05% | Val Loss: 4.6071 | Val Acc: 0.78%\n",
            "Epoch 4: Train accuracy 1.07% too low, skipping to next trial...\n",
            "Skipping trial 12 due to low accuracy.\n",
            "Trial 13/30 | lr: 0.0001 | weight_decay: 0.05 | eps: 0.0001\n",
            "Epoch 1/5 | Train Loss: 4.5725 | Train Acc: 1.68% | Val Loss: 4.4902 | Val Acc: 3.55%\n",
            "Epoch 2/5 | Train Loss: 4.3544 | Train Acc: 5.35% | Val Loss: 4.2756 | Val Acc: 6.50%\n",
            "Epoch 3/5 | Train Loss: 4.2153 | Train Acc: 7.16% | Val Loss: 4.1969 | Val Acc: 7.79%\n",
            "Epoch 4/5 | Train Loss: 4.1503 | Train Acc: 8.52% | Val Loss: 4.1500 | Val Acc: 8.74%\n",
            "Epoch 5/5 | Train Loss: 4.1178 | Train Acc: 9.26% | Val Loss: 4.1392 | Val Acc: 8.99%\n",
            "Trial 14/30 | lr: 0.0001 | weight_decay: 0.007 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.5123 | Train Acc: 3.17% | Val Loss: 4.3610 | Val Acc: 4.93%\n",
            "Epoch 2/5 | Train Loss: 4.2682 | Train Acc: 6.47% | Val Loss: 4.2047 | Val Acc: 7.99%\n",
            "Epoch 3/5 | Train Loss: 4.1498 | Train Acc: 8.68% | Val Loss: 4.1297 | Val Acc: 9.62%\n",
            "Epoch 4/5 | Train Loss: 4.0814 | Train Acc: 10.12% | Val Loss: 4.0854 | Val Acc: 10.44%\n",
            "Epoch 5/5 | Train Loss: 4.0502 | Train Acc: 10.83% | Val Loss: 4.0658 | Val Acc: 10.79%\n",
            "Trial 15/30 | lr: 0.0005 | weight_decay: 0.01 | eps: 0.0001\n",
            "Epoch 1/5 | Train Loss: 4.4178 | Train Acc: 3.91% | Val Loss: 4.2446 | Val Acc: 6.47%\n",
            "Epoch 2/5 | Train Loss: 4.1332 | Train Acc: 8.31% | Val Loss: 4.0578 | Val Acc: 10.26%\n",
            "Epoch 3/5 | Train Loss: 3.9738 | Train Acc: 11.44% | Val Loss: 3.9371 | Val Acc: 12.74%\n",
            "Epoch 4/5 | Train Loss: 3.8644 | Train Acc: 13.62% | Val Loss: 3.8540 | Val Acc: 14.38%\n",
            "Epoch 5/5 | Train Loss: 3.7865 | Train Acc: 15.56% | Val Loss: 3.8001 | Val Acc: 15.17%\n",
            "Trial 16/30 | lr: 0.0001 | weight_decay: 0.05 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.5102 | Train Acc: 2.98% | Val Loss: 4.3548 | Val Acc: 4.90%\n",
            "Epoch 2/5 | Train Loss: 4.2479 | Train Acc: 6.69% | Val Loss: 4.1858 | Val Acc: 8.17%\n",
            "Epoch 3/5 | Train Loss: 4.1236 | Train Acc: 8.80% | Val Loss: 4.0965 | Val Acc: 9.96%\n",
            "Epoch 4/5 | Train Loss: 4.0525 | Train Acc: 10.54% | Val Loss: 4.0564 | Val Acc: 10.70%\n",
            "Epoch 5/5 | Train Loss: 4.0173 | Train Acc: 11.36% | Val Loss: 4.0293 | Val Acc: 11.46%\n",
            "Trial 17/30 | lr: 0.005 | weight_decay: 0.1 | eps: 1e-06\n",
            "Epoch 1/5 | Train Loss: 4.6222 | Train Acc: 0.97% | Val Loss: 4.6072 | Val Acc: 0.97%\n",
            "Epoch 2/5 | Train Loss: 4.6063 | Train Acc: 0.95% | Val Loss: 4.6074 | Val Acc: 0.94%\n",
            "Epoch 3/5 | Train Loss: 4.6061 | Train Acc: 1.05% | Val Loss: 4.6070 | Val Acc: 0.95%\n",
            "Epoch 4: Train accuracy 1.04% too low, skipping to next trial...\n",
            "Skipping trial 17 due to low accuracy.\n",
            "Trial 18/30 | lr: 0.005 | weight_decay: 0.007 | eps: 1e-06\n",
            "Epoch 1/5 | Train Loss: 4.6404 | Train Acc: 0.94% | Val Loss: 4.6059 | Val Acc: 1.06%\n",
            "Epoch 2/5 | Train Loss: 4.6063 | Train Acc: 0.98% | Val Loss: 4.6070 | Val Acc: 1.01%\n",
            "Epoch 3/5 | Train Loss: 4.6060 | Train Acc: 1.00% | Val Loss: 4.6066 | Val Acc: 0.90%\n",
            "Epoch 4: Train accuracy 0.97% too low, skipping to next trial...\n",
            "Skipping trial 18 due to low accuracy.\n",
            "Trial 19/30 | lr: 0.0001 | weight_decay: 0.05 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.4925 | Train Acc: 3.27% | Val Loss: 4.3514 | Val Acc: 4.60%\n",
            "Epoch 2/5 | Train Loss: 4.2611 | Train Acc: 6.67% | Val Loss: 4.2228 | Val Acc: 7.70%\n",
            "Epoch 3/5 | Train Loss: 4.1519 | Train Acc: 8.86% | Val Loss: 4.1284 | Val Acc: 9.64%\n",
            "Epoch 4/5 | Train Loss: 4.0839 | Train Acc: 10.30% | Val Loss: 4.0861 | Val Acc: 10.86%\n",
            "Epoch 5/5 | Train Loss: 4.0507 | Train Acc: 11.01% | Val Loss: 4.0699 | Val Acc: 10.79%\n",
            "Trial 20/30 | lr: 0.005 | weight_decay: 0.1 | eps: 1e-06\n",
            "Epoch 1/5 | Train Loss: 4.6855 | Train Acc: 1.00% | Val Loss: 4.6065 | Val Acc: 0.90%\n",
            "Epoch 2/5 | Train Loss: 4.6064 | Train Acc: 0.93% | Val Loss: 4.6070 | Val Acc: 1.01%\n",
            "Epoch 3/5 | Train Loss: 4.6061 | Train Acc: 1.00% | Val Loss: 4.6067 | Val Acc: 1.07%\n",
            "Epoch 4: Train accuracy 0.98% too low, skipping to next trial...\n",
            "Skipping trial 20 due to low accuracy.\n",
            "Trial 21/30 | lr: 0.001 | weight_decay: 0.007 | eps: 1e-06\n",
            "Epoch 1/5 | Train Loss: 4.3788 | Train Acc: 4.19% | Val Loss: 4.2249 | Val Acc: 6.04%\n",
            "Epoch 2/5 | Train Loss: 4.1338 | Train Acc: 7.81% | Val Loss: 4.0476 | Val Acc: 9.89%\n",
            "Epoch 3/5 | Train Loss: 3.9794 | Train Acc: 11.01% | Val Loss: 3.9477 | Val Acc: 12.07%\n",
            "Epoch 4/5 | Train Loss: 3.8585 | Train Acc: 13.48% | Val Loss: 3.8433 | Val Acc: 14.26%\n",
            "Epoch 5/5 | Train Loss: 3.7708 | Train Acc: 15.62% | Val Loss: 3.7761 | Val Acc: 15.87%\n",
            "Trial 22/30 | lr: 0.001 | weight_decay: 0.007 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.3836 | Train Acc: 4.18% | Val Loss: 4.2644 | Val Acc: 5.13%\n",
            "Epoch 2/5 | Train Loss: 4.1098 | Train Acc: 8.21% | Val Loss: 4.0502 | Val Acc: 9.06%\n",
            "Epoch 3/5 | Train Loss: 3.9563 | Train Acc: 11.02% | Val Loss: 3.9174 | Val Acc: 12.61%\n",
            "Epoch 4/5 | Train Loss: 3.8314 | Train Acc: 14.01% | Val Loss: 3.8164 | Val Acc: 14.73%\n",
            "Epoch 5/5 | Train Loss: 3.7532 | Train Acc: 15.74% | Val Loss: 3.7744 | Val Acc: 15.70%\n",
            "Trial 23/30 | lr: 0.0005 | weight_decay: 0.05 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.3985 | Train Acc: 3.99% | Val Loss: 4.2231 | Val Acc: 6.38%\n",
            "Epoch 2/5 | Train Loss: 4.1082 | Train Acc: 8.74% | Val Loss: 4.0337 | Val Acc: 10.40%\n",
            "Epoch 3/5 | Train Loss: 3.9432 | Train Acc: 12.28% | Val Loss: 3.9015 | Val Acc: 13.26%\n",
            "Epoch 4/5 | Train Loss: 3.8250 | Train Acc: 14.81% | Val Loss: 3.8056 | Val Acc: 14.75%\n",
            "Epoch 5/5 | Train Loss: 3.7462 | Train Acc: 16.26% | Val Loss: 3.7584 | Val Acc: 16.29%\n",
            "Trial 24/30 | lr: 1e-05 | weight_decay: 0.05 | eps: 1e-06\n",
            "Epoch 1/5 | Train Loss: 4.6036 | Train Acc: 0.89% | Val Loss: 4.6002 | Val Acc: 0.92%\n",
            "Epoch 2/5 | Train Loss: 4.5941 | Train Acc: 1.04% | Val Loss: 4.5874 | Val Acc: 1.04%\n",
            "Epoch 3/5 | Train Loss: 4.5768 | Train Acc: 1.71% | Val Loss: 4.5656 | Val Acc: 1.88%\n",
            "Epoch 4: Train accuracy 2.01% too low, skipping to next trial...\n",
            "Skipping trial 24 due to low accuracy.\n",
            "Trial 25/30 | lr: 1e-05 | weight_decay: 0.007 | eps: 1e-06\n",
            "Epoch 1/5 | Train Loss: 4.6021 | Train Acc: 1.26% | Val Loss: 4.5968 | Val Acc: 1.25%\n",
            "Epoch 2/5 | Train Loss: 4.5875 | Train Acc: 1.87% | Val Loss: 4.5777 | Val Acc: 2.15%\n",
            "Epoch 3/5 | Train Loss: 4.5654 | Train Acc: 2.82% | Val Loss: 4.5558 | Val Acc: 2.83%\n",
            "Epoch 4: Train accuracy 3.13% too low, skipping to next trial...\n",
            "Skipping trial 25 due to low accuracy.\n",
            "Trial 26/30 | lr: 0.005 | weight_decay: 0.05 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.6227 | Train Acc: 0.96% | Val Loss: 4.6066 | Val Acc: 1.02%\n",
            "Epoch 2/5 | Train Loss: 4.6063 | Train Acc: 0.96% | Val Loss: 4.6072 | Val Acc: 0.91%\n",
            "Epoch 3: Train accuracy 0.95% too low, skipping to next trial...\n",
            "Skipping trial 26 due to low accuracy.\n",
            "Trial 27/30 | lr: 0.0001 | weight_decay: 0.01 | eps: 0.0001\n",
            "Epoch 1/5 | Train Loss: 4.5528 | Train Acc: 2.51% | Val Loss: 4.4421 | Val Acc: 3.84%\n",
            "Epoch 2/5 | Train Loss: 4.3261 | Train Acc: 5.50% | Val Loss: 4.2692 | Val Acc: 5.75%\n",
            "Epoch 3/5 | Train Loss: 4.2020 | Train Acc: 7.32% | Val Loss: 4.1756 | Val Acc: 8.69%\n",
            "Epoch 4/5 | Train Loss: 4.1322 | Train Acc: 9.08% | Val Loss: 4.1419 | Val Acc: 8.35%\n",
            "Epoch 5/5 | Train Loss: 4.1001 | Train Acc: 9.79% | Val Loss: 4.1100 | Val Acc: 9.84%\n",
            "Trial 28/30 | lr: 0.0005 | weight_decay: 0.1 | eps: 0.0001\n",
            "Epoch 1/5 | Train Loss: 4.4177 | Train Acc: 3.77% | Val Loss: 4.2659 | Val Acc: 5.24%\n",
            "Epoch 2/5 | Train Loss: 4.1533 | Train Acc: 7.68% | Val Loss: 4.1033 | Val Acc: 9.11%\n",
            "Epoch 3/5 | Train Loss: 3.9960 | Train Acc: 10.94% | Val Loss: 3.9475 | Val Acc: 11.58%\n",
            "Epoch 4/5 | Train Loss: 3.8830 | Train Acc: 13.43% | Val Loss: 3.8739 | Val Acc: 13.66%\n",
            "Epoch 5/5 | Train Loss: 3.8155 | Train Acc: 15.19% | Val Loss: 3.8131 | Val Acc: 15.36%\n",
            "Trial 29/30 | lr: 0.0005 | weight_decay: 0.01 | eps: 1e-06\n",
            "Epoch 1/5 | Train Loss: 4.3935 | Train Acc: 4.04% | Val Loss: 4.2259 | Val Acc: 6.03%\n",
            "Epoch 2/5 | Train Loss: 4.1268 | Train Acc: 8.41% | Val Loss: 4.0556 | Val Acc: 9.52%\n",
            "Epoch 3/5 | Train Loss: 3.9662 | Train Acc: 11.43% | Val Loss: 3.9132 | Val Acc: 12.67%\n",
            "Epoch 4/5 | Train Loss: 3.8439 | Train Acc: 14.13% | Val Loss: 3.8350 | Val Acc: 14.30%\n",
            "Epoch 5/5 | Train Loss: 3.7735 | Train Acc: 16.04% | Val Loss: 3.7810 | Val Acc: 15.50%\n",
            "Trial 30/30 | lr: 0.005 | weight_decay: 0.01 | eps: 1e-08\n",
            "Epoch 1/5 | Train Loss: 4.5767 | Train Acc: 1.72% | Val Loss: 4.4804 | Val Acc: 2.29%\n",
            "Epoch 2/5 | Train Loss: 4.4505 | Train Acc: 2.70% | Val Loss: 4.4586 | Val Acc: 2.66%\n",
            "Epoch 3/5 | Train Loss: 4.3971 | Train Acc: 3.27% | Val Loss: 4.3800 | Val Acc: 3.95%\n",
            "Epoch 4: Train accuracy 4.17% too low, skipping to next trial...\n",
            "Skipping trial 30 due to low accuracy.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'best_hyperparams' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-fa6661632ee1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbest_hyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfind_alg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random_search'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mbest_hyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLeNet5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-53-19b9fbedcbbf>\u001b[0m in \u001b[0;36mrandom_search\u001b[0;34m(train_loader, val_loader, model_class, device, num_trials, num_epochs, type_optimizer)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'best_hyperparams_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtype_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_hyperparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nBest Hyperparameters saved to /content/drive/MyDrive/best_hyperparams_SGDM.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'best_hyperparams' referenced before assignment"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    find_alg = 'random_search' # grid_search or random_search\n",
        "    type_optimizer = 'AdamW'  # AdamW or SDGM\n",
        "\n",
        "    model = LeNet5().to(device)\n",
        "    if find_alg == 'grid_search':\n",
        "        best_hyperparams = use_grid_search(model, device, train_loader, val_loader, test_loader, type_optimizer=type_optimizer)\n",
        "    elif find_alg == 'random_search':\n",
        "        best_hyperparams = random_search(train_loader, val_loader, model_class=LeNet5, device=device, num_trials=30, num_epochs=5, type_optimizer=type_optimizer)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1wf-SQnnllr6",
        "bkwD0eWYmzlA"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}