{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheS1n233/Distributed-Learning-Project5/blob/experiments/hyperparams_finding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Jn8pJTtuK_fI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d00dd45-4c9f-4147-c199-6a0ca822300e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision matplotlib\n",
        "!pip install --upgrade torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X1mwT7cjK_fJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8PUoANc3K_fJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f220630-6b62-4665-dfec-e036c784f591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "_E8Mnde3SV8u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing new"
      ],
      "metadata": {
        "id": "HW4AFSyol9_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6hkXaaVbK_fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171beb38-373e-4fd8-df34-695d71d88c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Mean:  tensor([0.5071, 0.4865, 0.4409])\n",
            "Std:  tensor([0.2667, 0.2558, 0.2754])\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset loading time: 1.76 seconds\n",
            "Batch 0: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 1: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 2: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 3: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 4: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 5: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 6: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 7: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 8: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 9: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 10: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Data loading for 10 batches completed.\n"
          ]
        }
      ],
      "source": [
        "# Define the transform to only convert the images to tensors (without normalization yet)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the CIFAR-100 training dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Initialize sums for calculating mean and std\n",
        "mean = torch.zeros(3)\n",
        "std = torch.zeros(3)\n",
        "\n",
        "for images, _ in train_loader:\n",
        "    # Compute mean and std for each channel\n",
        "    mean += images.mean(dim=[0, 2, 3])  # Mean per channel (R, G, B)\n",
        "    std += images.std(dim=[0, 2, 3])  # Std per channel (R, G, B)\n",
        "\n",
        "mean /= len(train_loader)\n",
        "std /= len(train_loader)\n",
        "\n",
        "print(\"Mean: \", mean)\n",
        "print(\"Std: \", std)\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop((32,32),padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                         std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                         std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "start_time = time.time()\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "# Split training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Debugging: Check DataLoader outputs\n",
        "for i, (inputs, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "    if i == 10:  # Test first 10 batches\n",
        "        break\n",
        "print(f\"Data loading for 10 batches completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define LeNet-5"
      ],
      "metadata": {
        "id": "bkwD0eWYmzlA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eXGl5EX6K_fK"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        # Layer convolutivi\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)  # 3 input channels, 64 output channels\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)  # 64 input channels, 64 output channels\n",
        "\n",
        "        # Layer fully connected\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)  # Dimensione calcolata per input 32x32 con due conv e max-pooling\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)  # Classificatore lineare per CIFAR-100\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer convolutivi con ReLU e max-pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
        "\n",
        "        # Flatten per i layer fully connected\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Layer fully connected con ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        # Classificatore lineare\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # Softmax per probabilit√†\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching"
      ],
      "metadata": {
        "id": "xiC1dWq4m79C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kvKHU3ohK_fL"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_random(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device, type_optimizer):\n",
        "\n",
        "    model.train()\n",
        "    accumulation_steps = 4  # Gradient accumulation\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        train_loss_total = 0\n",
        "        train_correct, train_total = 0, 0\n",
        "\n",
        "        # Training loop\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            # inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, alpha)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels) / accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss_total += loss.item() * accumulation_steps\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss_total / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_acc = 100. * correct / total\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        if epoch == 2 and train_acc < 1.0:\n",
        "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
        "            return None, None, None\n",
        "        if epoch == 3 and train_acc < 5.0:\n",
        "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
        "            return None, None, None\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Adjust learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    return train_loss, val_loss, val_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random search"
      ],
      "metadata": {
        "id": "hAnco-8w_jNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2CfN2CT0K_fL"
      },
      "outputs": [],
      "source": [
        "def random_search(train_loader, val_loader, model_class, device, num_trials, num_epochs, type_optimizer):\n",
        "    if type_optimizer == 'AdamW':\n",
        "        param_space = {\n",
        "            'lr': [8e-4, 1e-3, 2e-3, 3e-3, 5e-3],\n",
        "            'weight_decay': [1e-2, 5e-2, 1e-1],\n",
        "            'eps': [1e-8]\n",
        "        }\n",
        "    elif type_optimizer == 'SGDM':\n",
        "        param_space = {\n",
        "            'lr': [1e-4, 5e-4, 1e-3, 1e-2, 5e-2],\n",
        "            'momentum': [0.9, 0.95],\n",
        "            'weight_decay': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
        "\n",
        "    results = []\n",
        "    used_combinations = set()\n",
        "    for trial in range(num_trials):\n",
        "        model = model_class().to(device)\n",
        "\n",
        "        if type_optimizer == 'AdamW':\n",
        "            while True:\n",
        "                lr = random.choice(param_space['lr'])\n",
        "                weight_decay = random.choice(param_space['weight_decay'])\n",
        "                eps = random.choice(param_space['eps'])\n",
        "                params = (lr, weight_decay, eps)\n",
        "                if params not in used_combinations:\n",
        "                    used_combinations.add(params)\n",
        "                    break\n",
        "\n",
        "            optimizer = optim.AdamW(\n",
        "                model.parameters(),\n",
        "                lr=lr,\n",
        "                weight_decay=weight_decay,\n",
        "                eps=eps\n",
        "            )\n",
        "\n",
        "            print(f\"Trial {trial + 1}/{num_trials} | lr: {lr} | weight_decay: {weight_decay} | eps: {eps}\")\n",
        "\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            while True:\n",
        "                lr = random.choice(param_space['lr'])\n",
        "                weight_decay = random.choice(param_space['weight_decay'])\n",
        "                momentum = random.choice(param_space['momentum'])\n",
        "                params = (lr, weight_decay, momentum)\n",
        "                if params not in used_combinations:\n",
        "                    used_combinations.add(params)\n",
        "                    break\n",
        "\n",
        "            optimizer = optim.SGD(\n",
        "                model.parameters(),\n",
        "                lr=lr,\n",
        "                momentum=momentum,\n",
        "                weight_decay=weight_decay\n",
        "            )\n",
        "\n",
        "            print(f\"Trial {trial + 1}/{num_trials} | lr: {lr} | weight_decay: {weight_decay} | momentum: {momentum} \")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
        "\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        train_loss, val_loss, val_acc = train_and_evaluate_random(\n",
        "            model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device=device,type_optimizer=type_optimizer\n",
        "        )\n",
        "\n",
        "        if train_loss is None:\n",
        "            print(f\"Skipping trial {trial + 1} due to low accuracy.\")\n",
        "            continue\n",
        "\n",
        "        if type_optimizer == 'AdamW':\n",
        "            results.append({\n",
        "                'lr': lr,\n",
        "                'weight_decay': weight_decay,\n",
        "                'eps': eps,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc\n",
        "            })\n",
        "\n",
        "\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            results.append({\n",
        "                'lr': lr,\n",
        "                'weight_decay': weight_decay,\n",
        "                'momentum': momentum,\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc\n",
        "            })\n",
        "\n",
        "    results.sort(key=lambda x: x['val_acc'], reverse=True)\n",
        "    best_hyperparams = results[0]\n",
        "\n",
        "    save_path = 'best_hyperparams_'+ type_optimizer\n",
        "    with open('/content/drive/MyDrive/' + save_path+ '.json', 'w') as f:\n",
        "                json.dump(best_hyperparams, f)\n",
        "    print('\\nBest Hyperparameters saved to /content/drive/MyDrive/' + save_path +'.json')\n",
        "\n",
        "    return best_hyperparams"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid search"
      ],
      "metadata": {
        "id": "VBYcSH9f_ppE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "G6iAaDNOK_fM"
      },
      "outputs": [],
      "source": [
        "def check_initial_loss(model, train_loader, device):\n",
        "    \"\"\"Step 1: Check initial loss at initialization\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs, labels = next(iter(train_loader))\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        expected_loss = math.log(100)\n",
        "        print(f\"Initial loss: {loss.item():.4f}\")\n",
        "        print(f\"Expected loss: {expected_loss:.4f}\")\n",
        "        return abs(loss.item() - expected_loss) < 1.0\n",
        "\n",
        "def overfit_small_sample(model, train_loader, device, type_optimizer, max_iterations=1000):\n",
        "    \"\"\"Step 2: Overfit a small sample\"\"\"\n",
        "    # Get small sample (5 minibatches)\n",
        "    small_dataset = []\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        if i >= 5:\n",
        "            break\n",
        "        small_dataset.extend(list(zip(inputs, labels)))\n",
        "\n",
        "    small_loader = torch.utils.data.DataLoader(small_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    if type_optimizer == 'AdamW':\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
        "    elif type_optimizer == 'SGDM':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in small_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        accuracy = 100. * correct / total\n",
        "        avg_loss = total_loss / len(small_loader)\n",
        "\n",
        "        if iteration % 50 == 0:\n",
        "            print(f\"Iteration {iteration}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        if accuracy > 99:\n",
        "            print(\"Successfully overfit small sample!\")\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def find_initial_lr(model, train_loader, device, type_optimizer, lrs=[8e-4, 1e-3, 2e-3, 3e-3, 5e-3]):\n",
        "    \"\"\"Step 3: Find LR that makes loss go down\"\"\"\n",
        "    best_lr = None\n",
        "    min_loss_decrease = float('inf')\n",
        "\n",
        "    for lr in lrs:\n",
        "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "\n",
        "        if type_optimizer == 'AdamW':\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "        initial_loss = None\n",
        "        final_loss = None\n",
        "\n",
        "        print(f\"\\nTrying learning rate: {lr}\")\n",
        "\n",
        "        for iteration in range(100):\n",
        "            inputs, labels = next(iter(train_loader))\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "            if iteration == 0:\n",
        "                initial_loss = loss.item()\n",
        "            final_loss = loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iteration % 20 == 0:\n",
        "                print(f\"Iteration {iteration}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        loss_decrease = initial_loss - final_loss\n",
        "        print(f\"Loss decrease: {loss_decrease:.4f}\")\n",
        "\n",
        "        if loss_decrease > 0 and loss_decrease < min_loss_decrease:\n",
        "            min_loss_decrease = loss_decrease\n",
        "            best_lr = lr\n",
        "\n",
        "    return best_lr\n",
        "\n",
        "def coarse_grid_search(model, train_loader, val_loader, device, best_lr, type_optimizer):\n",
        "    \"\"\"Step 4: Coarse grid search\"\"\"\n",
        "\n",
        "\n",
        "    lrs = [best_lr / 2, best_lr, best_lr * 2]\n",
        "    weight_decays = [5e-3, 7e-3, 1e-2, 5e-2, 1e-1]\n",
        "    # momentum_arr = [0.9, 0.95]\n",
        "    eps =  [1e-8]\n",
        "    results = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for lr in lrs:\n",
        "        for wd in weight_decays:\n",
        "            model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "\n",
        "            if type_optimizer == 'AdamW':\n",
        "                eps = random.choice(eps)\n",
        "                print(f\"\\nTrying LR: {lr}, Weight Decay: {wd}, Eps: {eps}\")\n",
        "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, eps=eps)\n",
        "            elif type_optimizer == 'SGDM':\n",
        "                print(f\"\\nTrying LR: {lr}, Weight Decay: {wd}, Momentum: {momentum}\")\n",
        "                momentum = random.choice(momentum_arr)\n",
        "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "\n",
        "\n",
        "            train_losses = []\n",
        "            val_accuracies = []\n",
        "\n",
        "            for epoch in range(10):\n",
        "                # Training\n",
        "                model.train()\n",
        "                total_loss = 0\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                # Validation\n",
        "                model.eval()\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                    for inputs, labels in val_loader:\n",
        "                        inputs, labels = inputs.to(device), labels.to(device)\n",
        "                        outputs = model(inputs)\n",
        "                        _, predicted = outputs.max(1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                train_losses.append(total_loss / len(train_loader))\n",
        "                val_accuracies.append(100. * correct / total)\n",
        "\n",
        "                print(f\"Epoch {epoch + 1}, Loss: {train_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "\n",
        "            if type_optimizer == 'AdamW':\n",
        "                results.append({\n",
        "                    'lr': lr,\n",
        "                    'weight_decay': wd,\n",
        "                    'eps': eps,\n",
        "                    'final_loss': train_losses[-1],\n",
        "                    'final_val_acc': val_accuracies[-1],\n",
        "                    'momentum': momentum\n",
        "                })\n",
        "            elif type_optimizer == 'SGDM':\n",
        "                results.append({\n",
        "                    'lr': lr,\n",
        "                    'weight_decay': wd,\n",
        "                    'momentum': momentum,\n",
        "                    'final_loss': train_losses[-1],\n",
        "                    'final_val_acc': val_accuracies[-1],\n",
        "                    'momentum': momentum\n",
        "                })\n",
        "\n",
        "    results.sort(key=lambda x: x['final_val_acc'], reverse=True)\n",
        "    return results[0]\n",
        "\n",
        "def train_model_with_early_stopping(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs, device, patience=5):\n",
        "    \"\"\"Train model with early stopping\"\"\"\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        train_accuracies.append(100. * correct / total)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "        val_accuracies.append(100. * correct / total)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, \"\n",
        "              f\"Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
        "              f\"Val Loss: {val_losses[-1]:.4f}, \"\n",
        "              f\"Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AMiB-6BlK_fM"
      },
      "outputs": [],
      "source": [
        "def use_grid_search(model, device, train_loader, val_loader, test_loader, type_optimizer='AdamW'):\n",
        "\n",
        "    if type_optimizer != 'AdamW' and type_optimizer != 'SGDM':\n",
        "        raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
        "\n",
        "    print(\"Starting grid search...\")\n",
        "    # Step 1: Check initial loss\n",
        "    if check_initial_loss(model, train_loader, device):\n",
        "        print(\"Initial loss check passed.\")\n",
        "\n",
        "    # Step 2: Overfit small sample\n",
        "    if overfit_small_sample(model, train_loader, device, type_optimizer):\n",
        "        print(\"Successfully overfit small sample.\")\n",
        "\n",
        "    # Step 3: Find learning rate\n",
        "    best_lr = find_initial_lr(model, train_loader, device, type_optimizer)\n",
        "    print(f\"Best learning rate: {best_lr}\")\n",
        "\n",
        "    # Step 4: Coarse grid search for LR and weight decay\n",
        "    best_model_params = coarse_grid_search(model, train_loader, val_loader, device, best_lr, type_optimizer)\n",
        "    print(f\"Best model parameters: {best_model_params}\")\n",
        "\n",
        "    for improvement in range(1, 3):\n",
        "        # Step 5: Refine grid search and train for longer\n",
        "        # Train with the best parameters found\n",
        "        best_lr = best_model_params['lr']\n",
        "        best_weight_decay = best_model_params['weight_decay']\n",
        "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
        "        if type_optimizer == 'AdamW':\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay, eps=best_model_params['eps'])\n",
        "        elif type_optimizer == 'SGDM':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=best_lr, weight_decay=best_weight_decay, momentum=best_model_params['momentum'])\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=20*improvement)\n",
        "\n",
        "\n",
        "        # Train the model for 10-20 epochs\n",
        "        model, train_losses, val_losses, train_accuracies, val_accuracies = train_model_with_early_stopping(\n",
        "            model, optimizer, F.cross_entropy, scheduler, train_loader, val_loader, num_epochs=20*improvement, device=device, patience=5\n",
        "        )\n",
        "\n",
        "    # Step 6: Look at learning curves\n",
        "    # Plot training and validation losses and accuracies\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
        "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ih1KvTcrK_fM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "outputId": "e3542a80-e987-43eb-8c7e-c49c7a7a6ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Trial 1/25 | lr: 0.002 | weight_decay: 0.01 | eps: 1e-08\n",
            "Epoch 1/10 | Train Loss: 4.4101 | Train Acc: 3.46% | Val Loss: 4.3152 | Val Acc: 4.75%\n",
            "Epoch 2/10 | Train Loss: 4.1956 | Train Acc: 6.77% | Val Loss: 4.1132 | Val Acc: 7.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-17 (_pin_memory_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 59, in _pin_memory_loop\n",
            "    do_one_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 35, in do_one_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
            "    c = SocketClient(address)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
            "    s.connect(address)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7cb1dc7eca72>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbest_hyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfind_alg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random_search'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mbest_hyperparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLeNet5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-f509c60d78b0>\u001b[0m in \u001b[0;36mrandom_search\u001b[0;34m(train_loader, val_loader, model_class, device, num_trials, num_epochs, type_optimizer)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         train_loss, val_loss, val_acc = train_and_evaluate_random(\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-11-9ce2d7b00bbf>\u001b[0m in \u001b[0;36mtrain_and_evaluate_random\u001b[0;34m(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device, type_optimizer)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtrain_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtrain_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    find_alg = 'random_search' # grid_search or random_search\n",
        "    type_optimizer = 'AdamW'  # AdamW or SGDM\n",
        "\n",
        "    model = LeNet5().to(device)\n",
        "    if find_alg == 'grid_search':\n",
        "        best_hyperparams = use_grid_search(model, device, train_loader, val_loader, test_loader, type_optimizer=type_optimizer)\n",
        "    elif find_alg == 'random_search':\n",
        "        best_hyperparams = random_search(train_loader, val_loader, model_class=LeNet5, device=device, num_trials=25, num_epochs=5, type_optimizer=type_optimizer)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}