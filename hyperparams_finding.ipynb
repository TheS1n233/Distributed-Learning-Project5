{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib\n",
    "!pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "from google.colab import drive\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Cutout\n",
    "\"\"\"class Cutout(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        y1 = np.clip(y - self.size // 2, 0, h)\n",
    "        y2 = np.clip(y + self.size // 2, 0, h)\n",
    "        x1 = np.clip(x - self.size // 2, 0, w)\n",
    "        x2 = np.clip(x + self.size // 2, 0, w)\n",
    "        if len(img.shape) == 2:  # Handle grayscale images\n",
    "            img = img * mask\n",
    "        else:\n",
    "            img = img * mask[:, :, np.newaxis]\n",
    "\n",
    "        return Image.fromarray(np.uint8(img))\"\"\"\n",
    "\n",
    "\"\"\"# Mixup function\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0.0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\"\"\"\n",
    "\n",
    "# Data transformations with additional augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomRotation(15),\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    #transforms.RandomGrayscale(p=0.1),\n",
    "    #transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)),\n",
    "    #Cutout(size=8),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "start_time = time.time()\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test\n",
    ")\n",
    "print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Split training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Debugging: Check DataLoader outputs\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
    "    if i == 10:  # Test first 10 batches\n",
    "        break\n",
    "print(f\"Data loading for 10 batches completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        # Layer convolutivi\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)  # 3 input channels, 64 output channels\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)  # 64 input channels, 64 output channels\n",
    "\n",
    "        # Layer fully connected\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 384)  # Dimensione calcolata per input 32x32 con due conv e max-pooling\n",
    "        self.fc2 = nn.Linear(384, 192)\n",
    "        self.fc3 = nn.Linear(192, 100)  # Classificatore lineare per CIFAR-100\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer convolutivi con ReLU e max-pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
    "\n",
    "        # Flatten per i layer fully connected\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Layer fully connected con ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Classificatore lineare\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # Softmax per probabilit√†\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0, path='/content/drive/MyDrive/Early1checkpoint.pt', verbose=False):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_random(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device, type_optimezer):\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(device='cuda')  # Initialize mixed precision scaler\n",
    "    accumulation_steps = 4  # Gradient accumulation\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # Training loop\n",
    "        train_loss_total = 0\n",
    "        train_correct, train_total = 0, 0\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, alpha)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda'):  # Mixed precision forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            train_loss_total += loss.item() * accumulation_steps\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = train_loss_total / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_acc = 100. * correct / total\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if epoch == 2 and train_acc < 1.0:\n",
    "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
    "            return None, None, None\n",
    "        if epoch == 3 and train_acc < 5.0:\n",
    "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
    "            return None, None, None\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    return train_loss, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(train_loader, val_loader, model_class, device, num_trials, num_epochs, type_optimizer):\n",
    "    if type_optimizer == 'adamw':\n",
    "        param_space = {\n",
    "            'lr': [1e-4, 5e-4, 1e-3, 1e-2, 5e-2],\n",
    "            'weight_decay': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "            'betas': [(0.8, 0.9), (0.9, 0.999), (0.95, 0.98)],\n",
    "            'eps': [1e-8, 1e-4, 1e-6]\n",
    "        }\n",
    "    elif type_optimizer == 'sdgm':\n",
    "        param_space = {\n",
    "            'lr': [1e-4, 5e-4, 1e-3, 1e-2, 5e-2],\n",
    "            'momentum': [0.9, 0.95],\n",
    "            'weight_decay': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "\n",
    "        model = model_class().to(device)\n",
    "        if type_optimizer == 'Adamw':\n",
    "            lr = random.choice(param_space['lr'])\n",
    "            weight_decay = random.choice(param_space['weight_decay'])\n",
    "            betas = random.choice(param_space['betas'])\n",
    "            eps = random.choice(param_space['eps'])\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(), \n",
    "                lr=lr, \n",
    "                weight_decay=weight_decay, \n",
    "                betas=betas, \n",
    "                eps=eps\n",
    "            )\n",
    "            \n",
    "            print(f\"Trial {trial + 1}/{num_trials} | lr: {lr} | weight_decay: {weight_decay} | betas: {betas} | eps: {eps}\")\n",
    "\n",
    "            \n",
    "        elif type_optimizer == 'SDGM':\n",
    "            lr = random.choice(param_space['lr'])\n",
    "            weight_decay = random.choice(param_space['weight_decay'])\n",
    "            momentum = random.choice(param_space['momentum'])\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=lr,\n",
    "                momentum=momentum,\n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "            \n",
    "            print(f\"Trial {trial + 1}/{num_trials} | lr: {lr} | weight_decay: {weight_decay} | momentum: {momentum} \")\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
    "        \n",
    "\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        train_loss, val_loss, val_acc = train_and_evaluate_random(\n",
    "            model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device=device,\n",
    "        )\n",
    "\n",
    "        if train_loss is None:\n",
    "            print(f\"Skipping trial {trial + 1} due to low accuracy.\")\n",
    "            continue\n",
    "\n",
    "        if type_optimizer == 'adamw':\n",
    "            results.append({\n",
    "                'lr': lr,\n",
    "                'weight_decay': weight_decay,\n",
    "                'betas': betas,\n",
    "                'eps': eps,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "        elif type_optimizer == 'sdgm':\n",
    "            results.append({\n",
    "                'lr': lr,\n",
    "                'weight_decay': weight_decay,\n",
    "                'momentum': momentum,\n",
    "                'val_loss': val_loss,\n",
    "                'val_acc': val_acc\n",
    "            })\n",
    "\n",
    "    results.sort(key=lambda x: x['val_acc'], reverse=True)\n",
    "    best_hyperparams = results[0]\n",
    "\n",
    "    with open('/content/drive/MyDrive/best_hyperparams_AdamW.json', 'w') as f:\n",
    "        json.dump(best_hyperparams, f)\n",
    "    print(\"\\nBest Hyperparameters saved to /content/drive/MyDrive/best_hyperparams_AdamW.json\")\n",
    "\n",
    "    return best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_initial_loss(model, train_loader, device):\n",
    "    \"\"\"Step 1: Check initial loss at initialization\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(train_loader))\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        expected_loss = math.log(100)\n",
    "        print(f\"Initial loss: {loss.item():.4f}\")\n",
    "        print(f\"Expected loss: {expected_loss:.4f}\")\n",
    "        return abs(loss.item() - expected_loss) < 1.0\n",
    "\n",
    "def overfit_small_sample(model, train_loader, device, type_optimizer, max_iterations=1000):\n",
    "    \"\"\"Step 2: Overfit a small sample\"\"\"\n",
    "    # Get small sample (5 minibatches)\n",
    "    small_dataset = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        small_dataset.extend(list(zip(inputs, labels)))\n",
    "\n",
    "    small_loader = torch.utils.data.DataLoader(small_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    if type_optimizer == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    elif type_optimizer == 'sdgm':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in small_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        avg_loss = total_loss / len(small_loader)\n",
    "\n",
    "        if iteration % 50 == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        if accuracy > 99:\n",
    "            print(\"Successfully overfit small sample!\")\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def find_initial_lr(model, train_loader, device, type_optimizer, lrs=[1e-1, 1e-2, 1e-3, 1e-4]):\n",
    "    \"\"\"Step 3: Find LR that makes loss go down\"\"\"\n",
    "    best_lr = None\n",
    "    min_loss_decrease = float('inf')\n",
    "\n",
    "    for lr in lrs:\n",
    "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        \n",
    "        if type_optimizer == 'adamw':\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        elif type_optimizer == 'sdgm':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        initial_loss = None\n",
    "        final_loss = None\n",
    "\n",
    "        print(f\"\\nTrying learning rate: {lr}\")\n",
    "\n",
    "        for iteration in range(100):\n",
    "            inputs, labels = next(iter(train_loader))\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "            if iteration == 0:\n",
    "                initial_loss = loss.item()\n",
    "            final_loss = loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if iteration % 20 == 0:\n",
    "                print(f\"Iteration {iteration}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        loss_decrease = initial_loss - final_loss\n",
    "        print(f\"Loss decrease: {loss_decrease:.4f}\")\n",
    "\n",
    "        if loss_decrease > 0 and loss_decrease < min_loss_decrease:\n",
    "            min_loss_decrease = loss_decrease\n",
    "            best_lr = lr\n",
    "\n",
    "    return best_lr\n",
    "\n",
    "def coarse_grid_search(model, train_loader, val_loader, device, best_lr, type_optimizer):\n",
    "    \"\"\"Step 4: Coarse grid search\"\"\"\n",
    "    \n",
    "    \n",
    "    lrs = [best_lr / 2, best_lr, best_lr * 2]\n",
    "    weight_decays = [0, 1e-5, 1e-4]\n",
    "    momentum_arr = [0.9, 0.95]\n",
    "    betas = [(0.8, 0.9), (0.9, 0.999), (0.95, 0.98)],\n",
    "    eps =  [1e-8, 1e-4, 1e-6]\n",
    "    results = []\n",
    "\n",
    "\n",
    "    for lr in lrs:\n",
    "        for wd in weight_decays:\n",
    "            model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "            \n",
    "            if type_optimizer == 'adamw':\n",
    "                betas = random.choice(betas)\n",
    "                eps = random.choice(eps)\n",
    "                print(f\"\\nTrying LR: {lr}, Weight Decay: {wd}, Betas: {betas}, Eps: {eps}\")\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas, eps=eps)\n",
    "            elif type_optimizer == 'sdgm':\n",
    "                print(f\"\\nTrying LR: {lr}, Weight Decay: {wd}, Momentum: {momentum}\")\n",
    "                momentum = random.choice(momentum_arr)\n",
    "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "            \n",
    "\n",
    "\n",
    "            train_losses = []\n",
    "            val_accuracies = []\n",
    "\n",
    "            for epoch in range(5):\n",
    "                # Training\n",
    "                model.train()\n",
    "                total_loss = 0\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = F.cross_entropy(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                # Validation\n",
    "                model.eval()\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in val_loader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        _, predicted = outputs.max(1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                train_losses.append(total_loss / len(train_loader))\n",
    "                val_accuracies.append(100. * correct / total)\n",
    "\n",
    "                print(f\"Epoch {epoch + 1}, Loss: {train_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "\n",
    "            if type_optimizer == 'adamw':\n",
    "                results.append({\n",
    "                    'lr': lr,\n",
    "                    'weight_decay': wd,\n",
    "                    'betas': betas,\n",
    "                    'eps': eps,\n",
    "                    'final_loss': train_losses[-1],\n",
    "                    'final_val_acc': val_accuracies[-1],\n",
    "                    'momentum': momentum\n",
    "                })\n",
    "            elif type_optimizer == 'sdgm':\n",
    "                results.append({\n",
    "                    'lr': lr,\n",
    "                    'weight_decay': wd,\n",
    "                    'momentum': momentum,\n",
    "                    'final_loss': train_losses[-1],\n",
    "                    'final_val_acc': val_accuracies[-1],\n",
    "                    'momentum': momentum\n",
    "                })\n",
    "\n",
    "    results.sort(key=lambda x: x['final_val_acc'], reverse=True)\n",
    "    return results[0]\n",
    "\n",
    "def train_model_with_early_stopping(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs, device, patience=5):\n",
    "    \"\"\"Train model with early stopping\"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(100. * correct / total)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracies.append(100. * correct / total)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, \"\n",
    "              f\"Train Acc: {train_accuracies[-1]:.2f}%, \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, \"\n",
    "              f\"Val Acc: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_grid_search(model, device, train_loader, val_loader, test_loader, type_optimizer='adamw'):\n",
    "\n",
    "    if type_optimizer != 'adamw' and type_optimizer != 'sdgm':\n",
    "        raise ValueError(f\"Invalid optimizer type: {type_optimizer}\")\n",
    "    \n",
    "    print(\"Starting grid search...\")\n",
    "    # Step 1: Check initial loss\n",
    "    if check_initial_loss(model, train_loader, device):\n",
    "        print(\"Initial loss check passed.\")\n",
    "\n",
    "    # Step 2: Overfit small sample\n",
    "    if overfit_small_sample(model, train_loader, device, type_optimizer):\n",
    "        print(\"Successfully overfit small sample.\")\n",
    "\n",
    "    # Step 3: Find learning rate\n",
    "    best_lr = find_initial_lr(model, train_loader, device, type_optimizer)\n",
    "    print(f\"Best learning rate: {best_lr}\")\n",
    "\n",
    "    # Step 4: Coarse grid search for LR and weight decay\n",
    "    best_model_params = coarse_grid_search(model, train_loader, val_loader, device, best_lr, type_optimizer)\n",
    "    print(f\"Best model parameters: {best_model_params}\")\n",
    "\n",
    "    for improvement in range(1, 3):\n",
    "        # Step 5: Refine grid search and train for longer\n",
    "        # Train with the best parameters found\n",
    "        best_lr = best_model_params['lr']\n",
    "        best_weight_decay = best_model_params['weight_decay']\n",
    "        model.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        if type_optimizer == 'adamw':\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay, betas=best_model_params['betas'], eps=best_model_params['eps'])\n",
    "        elif type_optimizer == 'sdgm':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=best_lr, weight_decay=best_weight_decay, momentum=best_model_params['momentum'])\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=20*improvement)\n",
    "        \n",
    "\n",
    "        # Train the model for 10-20 epochs    \n",
    "        model, train_losses, val_losses, train_accuracies, val_accuracies = train_model_with_early_stopping(\n",
    "            model, optimizer, F.cross_entropy, scheduler, train_loader, val_loader, num_epochs=20*improvement, device=device, patience=5\n",
    "        )\n",
    "\n",
    "    # Step 6: Look at learning curves\n",
    "    # Plot training and validation losses and accuracies\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epochs, val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    find_alg = 'grid_search' # or random_search\n",
    "    type_optimizer = 'adamw'\n",
    "    \n",
    "    model = LeNet5().to(device)\n",
    "    if find_alg == 'grid_search':\n",
    "        best_hyperparams = use_grid_search(model, device, train_loader, val_loader, test_loader, type_optimizer=type_optimizer)\n",
    "    elif find_alg == 'random_search':\n",
    "        best_hyperparams = random_search(train_loader, val_loader, model, device, num_trials=10, num_epochs=5, type_optimizer=type_optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
