{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision matplotlib\n",
    "!pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Cutout\n",
    "\"\"\"class Cutout(object):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if isinstance(img, Image.Image):\n",
    "            img = np.array(img)\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "        y1 = np.clip(y - self.size // 2, 0, h)\n",
    "        y2 = np.clip(y + self.size // 2, 0, h)\n",
    "        x1 = np.clip(x - self.size // 2, 0, w)\n",
    "        x2 = np.clip(x + self.size // 2, 0, w)\n",
    "        if len(img.shape) == 2:  # Handle grayscale images\n",
    "            img = img * mask\n",
    "        else:\n",
    "            img = img * mask[:, :, np.newaxis]\n",
    "\n",
    "        return Image.fromarray(np.uint8(img))\"\"\"\n",
    "\n",
    "\"\"\"# Mixup function\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0.0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\"\"\"\n",
    "\n",
    "# Data transformations with additional augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    #transforms.RandomRotation(15),\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    #transforms.RandomGrayscale(p=0.1),\n",
    "    #transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)),\n",
    "    #Cutout(size=8),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "start_time = time.time()\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_train\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_test\n",
    ")\n",
    "print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Split training and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Debugging: Check DataLoader outputs\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
    "    if i == 10:  # Test first 10 batches\n",
    "        break\n",
    "print(f\"Data loading for 10 batches completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        # Layer convolutivi\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)  # 3 input channels, 64 output channels\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)  # 64 input channels, 64 output channels\n",
    "\n",
    "        # Layer fully connected\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 384)  # Dimensione calcolata per input 32x32 con due conv e max-pooling\n",
    "        self.fc2 = nn.Linear(384, 192)\n",
    "        self.fc3 = nn.Linear(192, 100)  # Classificatore lineare per CIFAR-100\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer convolutivi con ReLU e max-pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling 2x2\n",
    "\n",
    "        # Flatten per i layer fully connected\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Layer fully connected con ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # Classificatore lineare\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        # Softmax per probabilit√†\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0, path='/content/drive/MyDrive/Early1checkpoint.pt', verbose=False):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\")\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training and evaluation function with gradient clipping and label smoothing\n",
    "# def train_and_evaluate(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device,      early_stopping, alpha=1.0):\n",
    "def train_and_evaluate(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device, type_optimezer):\n",
    "\n",
    "    model.train()\n",
    "    scaler = GradScaler(device='cuda')  # Initialize mixed precision scaler\n",
    "    accumulation_steps = 4  # Gradient accumulation\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # Training loop\n",
    "        train_loss_total = 0\n",
    "        train_correct, train_total = 0, 0\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            # inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, alpha)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type='cuda'):  # Mixed precision forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels) / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            train_loss_total += loss.item() * accumulation_steps\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_loss = train_loss_total / len(train_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss, correct, total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_acc = 100. * correct / total\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if epoch == 2 and train_acc < 1.0:\n",
    "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
    "            return None, None, None\n",
    "        if epoch == 3 and train_acc < 5.0:\n",
    "            print(f\"Epoch {epoch+1}: Train accuracy {train_acc:.2f}% too low, skipping to next trial...\")\n",
    "            return None, None, None\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # # Early stopping\n",
    "        # early_stopping(val_loss, model)\n",
    "        # if early_stopping.early_stop:\n",
    "        #     print(\"Early stopping at epoch:\", epoch + 1)\n",
    "        #     break\n",
    "\n",
    "\n",
    "\n",
    "    return train_loss, val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(train_loader, val_loader, model_class, device, num_trials, num_epochs):\n",
    "    param_space = {\n",
    "        'lr': [1e-4, 5e-4, 1e-3, 1e-2, 5e-2],\n",
    "        'weight_decay': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "        'betas': [(0.8, 0.9), (0.9, 0.999), (0.95, 0.98)],\n",
    "        'eps': [1e-8, 1e-4, 1e-6]\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "\n",
    "        lr = random.choice(param_space['lr'])\n",
    "        weight_decay = random.choice(param_space['weight_decay'])\n",
    "        betas = random.choice(param_space['betas'])\n",
    "        eps = random.choice(param_space['eps'])\n",
    "\n",
    "        print(f\"Trial {trial + 1}/{num_trials} | lr: {lr} | weight_decay: {weight_decay} | betas: {betas} | eps: {eps}\")\n",
    "\n",
    "        model = model_class().to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=betas, eps=eps)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        # early_stopping = EarlyStopping(verbose=True)\n",
    "\n",
    "        # train_loss, val_loss, val_acc = train_and_evaluate(\n",
    "        #     model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device=device, early_stopping=early_stopping\n",
    "        # )\n",
    "        train_loss, val_loss, val_acc = train_and_evaluate(\n",
    "            model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device=device,\n",
    "        )\n",
    "\n",
    "        if train_loss is None:\n",
    "            print(f\"Skipping trial {trial + 1} due to low accuracy.\")\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            'lr': lr,\n",
    "            'weight_decay': weight_decay,\n",
    "            'betas': betas,\n",
    "            'eps': eps,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "\n",
    "    results.sort(key=lambda x: x['val_acc'], reverse=True)\n",
    "    best_hyperparams = results[0]\n",
    "\n",
    "    with open('/content/drive/MyDrive/best_hyperparams_AdamW.json', 'w') as f:\n",
    "        json.dump(best_hyperparams, f)\n",
    "    print(\"\\nBest Hyperparameters saved to /content/drive/MyDrive/best_hyperparams_AdamW.json\")\n",
    "\n",
    "    return best_hyperparams"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
