{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheS1n233/Distributed-Learning-Project5/blob/experiments/test_large_batch_optimezer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch"
      ],
      "metadata": {
        "id": "KlHheqAGVUVY",
        "outputId": "1a5827ab-731c-431c-9c9b-a02145e64a0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "u2T4ia3AsnT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, SequentialLR, LinearLR, LambdaLR\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.optim.lr_scheduler\")\n",
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
      ],
      "metadata": {
        "id": "GPmLs0QzJQrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a30da00-8522-48d9-8ea1-eefad64574b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Costants"
      ],
      "metadata": {
        "id": "9vT0d5W_srbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.01       # Choose 'SGDM': \"lr\": 0.01, \"weight_decay\": 0.001, \"momentum\": 0.9,\n",
        "WD = 0.001      # Choose 'AdamW': \"lr\": 0.001, \"weight_decay\": 0.1,\n",
        "MOMENTUM = 0.9\n",
        "EPS = 0.0001\n",
        "OPTIMIZER = 'LARS'  # Choose from: 'SGDM', 'LARS', 'AdamW', 'LAMB'"
      ],
      "metadata": {
        "id": "Vtu0U8wLQISx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "bNvH-6hEsuxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "da7YHCPoKnNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to get train, test and val dataset"
      ],
      "metadata": {
        "id": "k8oCJ42etBK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calulcate_mean_std(batch_size=100, verbose=True):\n",
        "    # Transform only for caluclate meaning of the dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-100 training dataset\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize sums for calculating mean and std\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "\n",
        "    for images, _ in train_loader:\n",
        "        # Compute mean and std for each channel\n",
        "        mean += images.mean(dim=[0, 2, 3])\n",
        "        std += images.std(dim=[0, 2, 3])\n",
        "\n",
        "    mean /= len(train_loader)\n",
        "    std /= len(train_loader)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Mean: \", mean)\n",
        "      print(\"Std: \", std)\n",
        "\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "sMQZou82OL3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(batch_size, verbose=True):\n",
        "\n",
        "    print(\"Start loading data with batch_size\", batch_size)\n",
        "\n",
        "    mean, std = calulcate_mean_std()\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.CenterCrop(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-100 dataset\n",
        "    start_time = time.time()\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "    print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Split training and validation sets\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Debugging: Check DataLoader outputs\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "        if i == 10:  # Test first 10 batches\n",
        "            break\n",
        "    print(f\"Data loading for 10 batches completed.\")\n",
        "\n",
        "    return train_dataset, train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "DiDck4ULPQXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CheckpointSaver:\n",
        "\n",
        "  def __init__(self, path, OPTIMIZER, batch_size, hyperparams, epochs):\n",
        "    self.timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    self.path = f\"{path}/{OPTIMIZER}_{batch_size}_{self.timestamp}\"\n",
        "    self.hyperparams = hyperparams\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def create_files(self):\n",
        "    os.makedirs(self.path, exist_ok=True)\n",
        "\n",
        "    self.metrics_files = {\n",
        "        'train_acc': os.path.join(self.path, f'{OPTIMIZER}_{batch_size}_train_accuracy_{self.timestamp}.txt'),\n",
        "        'val_acc': os.path.join(self.path, f'{OPTIMIZER}_{batch_size}_val_accuracy_{self.timestamp}.txt'),\n",
        "        'train_loss': os.path.join(self.path, f'{OPTIMIZER}_{batch_size}_train_loss_{self.timestamp}.txt'),\n",
        "        'val_loss': os.path.join(self.path, f'{OPTIMIZER}_{batch_size}_val_loss_{self.timestamp}.txt')\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(self.path, f'{OPTIMIZER}_{batch_size}_experiment_config_{self.timestamp}.txt'), 'w') as f:\n",
        "        f.write(f\"Experiment Configuration:\\n\")\n",
        "        f.write(f\"Optimizer: {OPTIMIZER}\\n\")\n",
        "        f.write(f\"Batch Size: {batch_size}\\n\")\n",
        "        f.write(f\"Hyperparameters: {str(self.hyperparams)}\\n\")\n",
        "        f.write(f\"Number of epochs: {self.epochs}\\n\")\n",
        "        f.write(f\"Timestamp: {self.timestamp}\\n\")\n",
        "\n",
        "\n",
        "  def save_checkpoint(self, epoch, train_acc, val_acc, train_loss, val_loss):\n",
        "      try:\n",
        "          with open(self.metrics_files['train_acc'], 'a') as f:\n",
        "              f.write(f\"{epoch+1},{train_acc * 100:.2f}%\\n\")\n",
        "          with open(self.metrics_files['val_acc'], 'a') as f:\n",
        "              f.write(f\"{epoch+1},{val_acc * 100:.2f}%\\n\")\n",
        "          with open(self.metrics_files['train_loss'], 'a') as f:\n",
        "              f.write(f\"{epoch+1},{train_loss:.4f}\\n\")\n",
        "          with open(self.metrics_files['val_loss'], 'a') as f:\n",
        "              f.write(f\"{epoch+1},{val_loss:.4f}\\n\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n",
        "  def save_model_checkpoint(self, model, optimizer, epoch):\n",
        "    checkpoint_path = os.path.join(self.path, f'checkpoint_epoch_last.pth')\n",
        "    try:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, checkpoint_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model checkpoint: {e}\")\n",
        "\n",
        "  def load_model_checkpoint(self, model, optimizer, checkpoint_path):\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"Checkpoint loaded successfully. Resuming from epoch {start_epoch}\")\n",
        "        return start_epoch\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model checkpoint: {e}\")\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "VHn9fSvvLhlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining different optimizers"
      ],
      "metadata": {
        "id": "IgKVfV0Ts7GD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaAvJoRuJNPF"
      },
      "outputs": [],
      "source": [
        "class LARS(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, beta1=0.9, batch_size=32, weight_decay=1e-4, epsilon=1e-8):\n",
        "        defaults = dict(lr=lr, beta1=beta1, batch_size=batch_size,\n",
        "                       weight_decay=weight_decay, epsilon=epsilon)\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "\n",
        "        # Initialize momentum\n",
        "        self.m0 = {param: torch.zeros_like(param.data) for group in self.param_groups\n",
        "                   for param in group['params']}\n",
        "\n",
        "    def phi(self, norm):\n",
        "        \"\"\"Scaling function φ as defined in the paper\"\"\"\n",
        "        return torch.ones_like(norm)  # Can be modified for different scaling strategies\n",
        "\n",
        "    def get_batch_samples(self, dataset, batch_size):\n",
        "        \"\"\"Draw b samples from the dataset\"\"\"\n",
        "        indices = torch.randperm(len(dataset))[:batch_size]\n",
        "        return [dataset[i] for i in indices]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None, dataset=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            beta1 = group['beta1']\n",
        "            lr = group['lr']\n",
        "            weight_decay = group['weight_decay']\n",
        "            epsilon = group['epsilon']\n",
        "\n",
        "            # Draw batch samples if dataset provided\n",
        "            if dataset is not None:\n",
        "                batch = self.get_batch_samples(dataset, group['batch_size'])\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = param.grad\n",
        "                state = self.state[param]\n",
        "\n",
        "                # Compute gt (gradient plus weight decay)\n",
        "                gt = grad + weight_decay * param.data\n",
        "\n",
        "                # Update momentum mt\n",
        "                if 'momentum' not in state:\n",
        "                    state['momentum'] = torch.zeros_like(param.data)\n",
        "                mt = state['momentum']\n",
        "                mt.mul_(beta1).add_((1 - beta1) * (gt + weight_decay * param.data))\n",
        "\n",
        "                # Compute the update with φ scaling\n",
        "                param_norm = param.data.norm(2).clamp(min=epsilon)\n",
        "                mt_norm = mt.norm(2).clamp(min=epsilon)\n",
        "\n",
        "                update = lr * (self.phi(param_norm) / mt_norm) * mt\n",
        "\n",
        "                # Update parameters\n",
        "                param.data.sub_(update)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class LAMB(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta1 parameter: {betas[0]}\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta2 parameter: {betas[1]}\")\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # m_t = β₁m_{t-1} + (1-β₁)g_t\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "                # v_t = β₂v_{t-1} + (1-β₂)g_t²\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                # Bias correction\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "                # m̂_t = m_t/(1-β₁ᵗ)\n",
        "                exp_avg_hat = exp_avg / bias_correction1\n",
        "                # v̂_t = v_t/(1-β₂ᵗ)\n",
        "                exp_avg_sq_hat = exp_avg_sq / bias_correction2\n",
        "\n",
        "                # r_t = m̂_t/√(v̂_t + ε)\n",
        "                denom = exp_avg_sq_hat.sqrt().add_(group['eps'])\n",
        "                update = exp_avg_hat / denom\n",
        "\n",
        "                # r_t + λx_t\n",
        "                if group['weight_decay'] > 0:\n",
        "                    update.add_(p.data, alpha=group['weight_decay'])\n",
        "\n",
        "                # Compute trust ratio φ(‖x_t‖)/‖r_t + λx_t‖\n",
        "                w_norm = p.data.norm(p=2)\n",
        "                g_norm = update.norm(p=2)\n",
        "                trust_ratio = torch.where(\n",
        "                    w_norm > 0,\n",
        "                    w_norm / (g_norm + group['eps']),\n",
        "                    torch.ones_like(w_norm)\n",
        "                )\n",
        "\n",
        "                # x_{t+1} = x_t - η·trust_ratio·(r_t + λx_t)\n",
        "                p.data.add_(update, alpha=-group['lr'] * trust_ratio)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training function"
      ],
      "metadata": {
        "id": "MhXxXjgks2K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_hyperparams(train_loader, val_loader, test_loader, hyperparams, num_epochs, device, type_of_optimizer, batch_size, train_dataset):\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    if type_of_optimizer == \"SGDM\":\n",
        "        if hyperparams.get('momentum', None) is None:\n",
        "            raise ValueError(\"Momentum is required for SGDM\")\n",
        "        if hyperparams.get('weight_decay', None) is None:\n",
        "            raise ValueError(\"Weight decay is required for SGDM\")\n",
        "        if hyperparams.get('lr', None) is None:\n",
        "            raise ValueError(\"Learning rate is required for SGDM\")\n",
        "\n",
        "        optimizer = optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            momentum=hyperparams['momentum'],\n",
        "            weight_decay=hyperparams['weight_decay']\n",
        "        )\n",
        "\n",
        "    elif type_of_optimizer == \"AdamW\":\n",
        "        if hyperparams.get('lr', None) is None:\n",
        "            raise ValueError(\"Learning rate is required for AdamW\")\n",
        "        if hyperparams.get('weight_decay', None) is None:\n",
        "            raise ValueError(\"Weight decay is required for AdamW\")\n",
        "\n",
        "        optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            weight_decay=hyperparams['weight_decay'],\n",
        "        )\n",
        "\n",
        "    elif type_of_optimizer == \"LAMB\":\n",
        "        if hyperparams.get('lr', None) is None:\n",
        "            raise ValueError(\"Learning rate is required for LAMB\")\n",
        "        if hyperparams.get('weight_decay', None) is None:\n",
        "            raise ValueError(\"Weight decay is required for LAMB\")\n",
        "\n",
        "        optimizer = LAMB(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            weight_decay=hyperparams['weight_decay'],\n",
        "        )\n",
        "\n",
        "    elif type_of_optimizer == \"LARS\":\n",
        "        if hyperparams.get('lr', None) is None:\n",
        "            raise ValueError(\"Learning rate is required for LARS\")\n",
        "        if hyperparams.get('weight_decay', None) is None:\n",
        "            raise ValueError(\"Weight decay is required for LARS\")\n",
        "\n",
        "        optimizer = LARS(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            weight_decay=hyperparams['weight_decay'],\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer type\")\n",
        "\n",
        "    print(f\"Train running with {type_of_optimizer} optimizer.\")\n",
        "\n",
        "\n",
        "    iterations_per_epoch = len(train_dataset) // batch_size\n",
        "    warmup_epoch = 5\n",
        "    def warmup_scheduler_fn(epoch):\n",
        "        if epoch < warmup_epoch:\n",
        "            return float(epoch+1) / warmup_epoch\n",
        "        return 1.0\n",
        "\n",
        "    warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_scheduler_fn)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs - warmup_epoch - start_epoch)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    cs = CheckpointSaver(\n",
        "        path=f\"/content/drive/My Drive/Colab Notebooks/Traning_summary/\",\n",
        "        OPTIMIZER=OPTIMIZER,\n",
        "        batch_size=batch_size,\n",
        "        hyperparams=hyperparams,\n",
        "        epochs=num_epochs\n",
        "    )\n",
        "\n",
        "    # Check for existing checkpoint      Change path/LARS_512_20250124-200438/\n",
        "    checkpoint_path = \"/content/drive/My Drive/Colab Notebooks/Traning_summary/LARS_512_20250124-200438/checkpoint_epoch_last.pth\"\n",
        "    start_epoch = 0\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        start_epoch = cs.load_model_checkpoint(model, optimizer, checkpoint_path)\n",
        "        print(f\"Recover，from {start_epoch} epoch to train\")\n",
        "    else:\n",
        "        print(\"start training from beginning\")\n",
        "\n",
        "    cs.create_files()\n",
        "\n",
        "\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        train_loss_total, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_total += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss_total / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss_total, val_correct, val_total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss_total += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss_total / len(val_loader)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        cs.save_checkpoint(epoch, train_acc, val_acc, train_loss, val_loss)\n",
        "        cs.save_model_checkpoint(model, optimizer, epoch)\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "\n",
        "        if epoch < warmup_epoch:\n",
        "            warmup_scheduler.step()\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "    # Test loop\n",
        "    model.eval()\n",
        "    test_correct, test_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_acc = 100. * test_correct / test_total\n",
        "    print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies, test_acc, cs.path"
      ],
      "metadata": {
        "id": "mc0vfd9tNvNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(train_losses, val_losses, train_accuracies, val_accuracies, save_path):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{save_path}/training_results.png\")\n",
        "    print(f\"Training results saved to {save_path} as 'training_results.png'\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-P7JtvaSPVbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "UEqcoElZtH_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " if __name__ == \"__main__\":\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "    all_train_losses = {}\n",
        "    all_val_losses = {}\n",
        "    all_train_accuracies = {}\n",
        "    all_val_accuracies = {}\n",
        "    all_test_accuracies = {}\n",
        "\n",
        "    batch_sizes = [512]\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "\n",
        "        train_dataset, train_loader, val_loader, test_loader = get_dataset(batch_size)\n",
        "\n",
        "        base_lr = LR\n",
        "        base_batch_size = BATCH_SIZE\n",
        "        lr = base_lr * math.sqrt(batch_size/base_batch_size)\n",
        "        hyperparams = {\n",
        "            'lr': lr,\n",
        "            'weight_decay': WD,\n",
        "        }\n",
        "\n",
        "        print(f\"Using hyperparams: lr: {hyperparams['lr']}, weight_decay: {hyperparams['weight_decay']}\")\n",
        "\n",
        "        # Train with the best hyperparameters\n",
        "        train_losses, val_losses, train_accuracies, val_accuracies, test_acc, save_path= train_model_with_hyperparams(\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            test_loader=test_loader,\n",
        "            hyperparams=hyperparams,\n",
        "            num_epochs=NUM_EPOCHS,\n",
        "            device=device,\n",
        "            type_of_optimizer=OPTIMIZER,\n",
        "            batch_size=batch_size,\n",
        "            train_dataset=train_dataset\n",
        "        )\n",
        "\n",
        "        plot_results(train_losses, val_losses, train_accuracies, val_accuracies, save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "7_I7B_2eJX9x",
        "outputId": "8ddb55b0-f229-4bcf-9083-e99b1dcbbcb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start loading data with batch_size 512\n",
            "Files already downloaded and verified\n",
            "Mean:  tensor([0.5071, 0.4865, 0.4409])\n",
            "Std:  tensor([0.2667, 0.2558, 0.2754])\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset loading time: 1.73 seconds\n",
            "Batch 0: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 1: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 2: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 3: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 4: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 5: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 6: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 7: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 8: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 9: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Batch 10: inputs shape: torch.Size([512, 3, 32, 32]), labels shape: torch.Size([512])\n",
            "Data loading for 10 batches completed.\n",
            "Using hyperparams: lr: 0.028284271247461905, weight_decay: 0.001\n",
            "Train running with LARS optimizer.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'start_epoch' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-400ace798a5d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m        \u001b[0;31m# Train with the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m        train_losses, val_losses, train_accuracies, val_accuracies, test_acc, save_path= train_model_with_hyperparams(\n\u001b[0m\u001b[1;32m     30\u001b[0m            \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m            \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-bdcc2ff722ac>\u001b[0m in \u001b[0;36mtrain_model_with_hyperparams\u001b[0;34m(train_loader, val_loader, test_loader, hyperparams, num_epochs, device, type_of_optimizer, batch_size, train_dataset)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mwarmup_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambdaLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup_scheduler_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCosineAnnealingLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwarmup_epoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'start_epoch' where it is not associated with a value"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "bNvH-6hEsuxY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}