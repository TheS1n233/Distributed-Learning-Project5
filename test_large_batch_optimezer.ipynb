{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    def __init__(self, params, lr=required, momentum=0.9, weight_decay=0.0, eta=0.001, eps=1e-8):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if momentum < 0.0 or momentum >= 1.0:\n",
    "            raise ValueError(f\"Invalid momentum: {momentum}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay: {weight_decay}\")\n",
    "        if eta <= 0.0:\n",
    "            raise ValueError(f\"Invalid eta: {eta}\")\n",
    "        \n",
    "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, eta=eta, eps=eps)\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            weight_decay = group['weight_decay']\n",
    "            eta = group['eta']\n",
    "            eps = group['eps']\n",
    "\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = param.grad.data\n",
    "                if weight_decay != 0:\n",
    "                    grad.add_(param.data, alpha=weight_decay)\n",
    "\n",
    "                # calculate learning rate layer-wise\n",
    "                param_norm = param.data.norm(2)\n",
    "                grad_norm = grad.norm(2)\n",
    "                local_lr = eta * param_norm / (grad_norm + eps)\n",
    "\n",
    "                # apply momentum updates\n",
    "                if 'momentum_buffer' not in self.state[param]:\n",
    "                    buf = self.state[param]['momentum_buffer'] = torch.clone(grad).detach()\n",
    "                else:\n",
    "                    buf = self.state[param]['momentum_buffer']\n",
    "                    buf.mul_(momentum).add_(grad)\n",
    "\n",
    "                param.data.add_(buf, alpha=-local_lr * lr)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = param.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdamW does not support sparse gradients')\n",
    "\n",
    "                state = self.state[param]\n",
    "\n",
    "                # initial state\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(param.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(param.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # update moving averages\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                denom = (exp_avg_sq.sqrt() / (1 - beta2 ** state['step'])).add_(group['eps'])\n",
    "                step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "\n",
    "                # apply weight decay\n",
    "                if group['weight_decay'] != 0:\n",
    "                    param.data.add_(param.data, alpha=-group['weight_decay'] * group['lr'])\n",
    "\n",
    "                # update params\n",
    "                param.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class LAMB(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super(LAMB, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = param.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('LAMB does not support sparse gradients')\n",
    "\n",
    "                state = self.state[param]\n",
    "\n",
    "                # Initial state\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(param.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(param.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Update moving averages\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                denom = (exp_avg_sq.sqrt() / (1 - beta2 ** state['step'])).add_(group['eps'])\n",
    "                r1 = param.data.norm(2)\n",
    "                r2 = exp_avg.norm(2)\n",
    "\n",
    "                if r1 == 0 or r2 == 0:\n",
    "                    trust_ratio = 1.0\n",
    "                else:\n",
    "                    trust_ratio = r1 / r2\n",
    "\n",
    "                step_size = group['lr'] * trust_ratio\n",
    "\n",
    "                # apply weight decay\n",
    "                if group['weight_decay'] != 0:\n",
    "                    param.data.add_(param.data, alpha=-group['weight_decay'])\n",
    "\n",
    "                # upadates params\n",
    "                param.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
