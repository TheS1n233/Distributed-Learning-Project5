{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch\n"
      ],
      "metadata": {
        "id": "KlHheqAGVUVY",
        "outputId": "f4a5499a-c2c5-4801-8d0a-3b709ccd88c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "u2T4ia3AsnT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import argparse\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, SequentialLR, LinearLR\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "GPmLs0QzJQrO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1yoNOiteP6yy",
        "outputId": "01ab32c1-7253-4157-a1e3-42c35e528175"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive.mount(\\'/content/drive\\')\\nif not os.path.exists(\\'/content/drive/MyDrive\\'):\\n    raise RuntimeError(\"Google Drive not mounted correctly!\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Costants"
      ],
      "metadata": {
        "id": "9vT0d5W_srbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "Vtu0U8wLQISx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "bNvH-6hEsuxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n",
        "\n",
        "        # Layer fully connected\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer convolutivi con ReLU e max-pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Flatten per i layer fully connected\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Layer fully connected con ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "da7YHCPoKnNK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early stopping"
      ],
      "metadata": {
        "id": "t_8hLuI1syee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping Class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, delta=0, path='/content/drive/MyDrive/Early2checkpoint.pt', verbose=False):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, optimizer=None):\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
        "        checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss_min': val_loss\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "HgFe_H-kNzFM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training function"
      ],
      "metadata": {
        "id": "MhXxXjgks2K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_hyperparams(train_loader, val_loader, test_loader, num_epochs, device, checkpoint_path, optimizer, patience=5):\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    warmup_scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=5)\n",
        "    scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, CosineAnnealingLR(optimizer, T_max=num_epochs - 5)])\n",
        "    #scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    # Checkpoint recovery\n",
        "    start_epoch = 0\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Checkpoint found at {checkpoint_path}. Loading model state...\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
        "    else:\n",
        "        print(f\"No checkpoint found at {checkpoint_path}. Training a new model from scratch.\")\n",
        "        checkpoint = None\n",
        "\n",
        "    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        model.train()\n",
        "        train_loss_total, train_correct, train_total = 0, 0, 0\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_total += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_loss = train_loss_total / len(train_loader)\n",
        "        train_acc = 100. * train_correct / train_total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss_total, val_correct, val_total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss_total += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss_total / len(val_loader)\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # Update scheduler after validation\n",
        "        scheduler.step()\n",
        "\n",
        "        early_stopping(val_loss, model)\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_accuracies': train_accuracies,\n",
        "            'val_accuracies': val_accuracies\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"Checkpoint saved to {checkpoint_path} at epoch {epoch + 1}\")\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    # Load the best model\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Test loop\n",
        "    model.eval()\n",
        "    test_correct, test_total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    test_acc = 100. * test_correct / test_total\n",
        "    print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies, test_acc\n"
      ],
      "metadata": {
        "id": "mc0vfd9tNvNt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining different optimizers"
      ],
      "metadata": {
        "id": "IgKVfV0Ts7GD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YaAvJoRuJNPF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LARS(Optimizer):\n",
        "    def __init__(self, params, lr=0.1, momentum=0.9, weight_decay=0.0, eta=0.001, eps=1e-8):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if momentum < 0.0 or momentum >= 1.0:\n",
        "            raise ValueError(f\"Invalid momentum: {momentum}\")\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(f\"Invalid weight_decay: {weight_decay}\")\n",
        "        if eta <= 0.0:\n",
        "            raise ValueError(f\"Invalid eta: {eta}\")\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, eta=eta, eps=eps)\n",
        "        super(LARS, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "            eta = group['eta']\n",
        "            eps = group['eps']\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = param.grad.data\n",
        "                if grad.isinf().any() or grad.isnan().any():\n",
        "                    print(f\"Skipping update for parameter {param} due to NaN/Inf gradients.\")\n",
        "                    continue\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    grad.add_(param.data, alpha=weight_decay)\n",
        "\n",
        "                param_norm = param.data.norm(2).clamp(min=eps)\n",
        "                grad_norm = grad.norm(2).clamp(min=eps)\n",
        "                local_lr = eta * param_norm / grad_norm\n",
        "\n",
        "                if 'momentum_buffer' not in self.state[param]:\n",
        "                    buf = self.state[param]['momentum_buffer'] = torch.clone(grad).detach()\n",
        "                else:\n",
        "                    buf = self.state[param]['momentum_buffer']\n",
        "                    buf.mul_(momentum).add_(grad)\n",
        "\n",
        "                param.data.add_(buf, alpha=-local_lr * lr)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class LAMB(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.01):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super(LAMB, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            betas = group['betas']\n",
        "            eps = group['eps']\n",
        "            weight_decay = group['weight_decay']\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = param.grad.data\n",
        "                if grad.isinf().any() or grad.isnan().any():\n",
        "                    print(f\"Skipping update for parameter {param} due to NaN/Inf gradients.\")\n",
        "                    continue\n",
        "\n",
        "                state = self.state[param]\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(param.data)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(param.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = betas\n",
        "                state['step'] += 1\n",
        "\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                denom = exp_avg_sq.sqrt().add_(eps)\n",
        "                r1 = param.data.norm(2).clamp(min=eps)\n",
        "                r2 = exp_avg.norm(2).clamp(min=eps)\n",
        "                trust_ratio = r1 / r2 if r2 != 0 else 1.0\n",
        "\n",
        "                step_size = lr * trust_ratio\n",
        "                if weight_decay != 0:\n",
        "                    param.data.add_(param.data, alpha=-weight_decay)\n",
        "\n",
        "                param.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class AdamW(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super(AdamW, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = param.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('AdamW does not support sparse gradients')\n",
        "\n",
        "                if grad.isinf().any() or grad.isnan().any():\n",
        "                    print(f\"Inf or NaN gradients found for {param}\")\n",
        "                    continue\n",
        "\n",
        "                state = self.state[param]\n",
        "\n",
        "                # initial state\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(param.data)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(param.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # update moving averages\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                denom = (exp_avg_sq.sqrt() / (1 - beta2 ** state['step'])).add_(group['eps'])\n",
        "                step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
        "\n",
        "                # apply weight decay\n",
        "                if group['weight_decay'] != 0:\n",
        "                    param.data.add_(param.data, alpha=-group['weight_decay'] * group['lr'])\n",
        "\n",
        "                # update params\n",
        "                param.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class SDGM(Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, momentum=0.9, weight_decay=0.0, eps=1e-8):\n",
        "        if lr < 0.0:\n",
        "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "        if momentum < 0.0 or momentum >= 1.0:\n",
        "            raise ValueError(f\"Invalid momentum: {momentum}\")\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(f\"Invalid weight_decay: {weight_decay}\")\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, eps=eps)\n",
        "        super(SDGM, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            lr = group['lr']\n",
        "            momentum = group['momentum']\n",
        "            weight_decay = group['weight_decay']\n",
        "            eps = group['eps']\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "\n",
        "                grad = param.grad.data\n",
        "\n",
        "                if grad.isinf().any() or grad.isnan().any():\n",
        "                    print(f\"Inf or NaN gradients found for {param}\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    grad.add_(param.data, alpha=weight_decay)\n",
        "\n",
        "                # Dual gradient updates\n",
        "                param_norm = param.data.norm(2)\n",
        "                grad_norm = grad.norm(2)\n",
        "                dual_lr = lr * param_norm / (grad_norm + eps)\n",
        "\n",
        "                # apply momentum updates\n",
        "                if 'momentum_buffer' not in self.state[param]:\n",
        "                    buf = self.state[param]['momentum_buffer'] = torch.clone(grad).detach()\n",
        "                else:\n",
        "                    buf = self.state[param]['momentum_buffer']\n",
        "                    buf.mul_(momentum).add_(grad)\n",
        "\n",
        "                param.data.add_(buf, alpha=-dual_lr)\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Large-Batch Optimizer Selection')\n",
        "    parser.add_argument('--optimizer', type=str, default='SGDM', choices=['SGDM', 'AdamW', 'LARS', 'LAMB'],\n",
        "                        help='Select optimizer (SGDM, AdamW, LARS, LAMB)')\n",
        "    #parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate')\n",
        "    #parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
        "    args = parser.parse_args()\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "iybuJXWdLzIQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to get train, test and val dataset"
      ],
      "metadata": {
        "id": "k8oCJ42etBK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(batch_size=100):\n",
        "\n",
        "    # Define the transform to only convert the images to tensors (without normalization yet)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-100 training dataset\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize sums for calculating mean and std\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "\n",
        "    for images, _ in train_loader:\n",
        "        # Compute mean and std for each channel\n",
        "        mean += images.mean(dim=[0, 2, 3])\n",
        "        std += images.std(dim=[0, 2, 3])\n",
        "\n",
        "    mean /= len(train_loader)\n",
        "    std /= len(train_loader)\n",
        "\n",
        "    print(\"Mean: \", mean)\n",
        "    print(\"Std: \", std)\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-100 dataset\n",
        "    start_time = time.time()\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "    print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Split training and validation sets\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Debugging: Check DataLoader outputs\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "        if i == 10:  # Test first 10 batches\n",
        "            break\n",
        "    print(f\"Data loading for 10 batches completed.\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "sMQZou82OL3K"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "UEqcoElZtH_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = LeNet5().to(device)\n",
        "\n",
        "    #args = parse_args()\n",
        "\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.optimizer = 'SDGM'  # Choose from: 'SGDM', 'AdamW', 'LARS', 'LAMB'\n",
        "            #self.batch_size = 32\n",
        "\n",
        "    args = Args() # replace \"args = parse_args()\" with this line\n",
        "\n",
        "\n",
        "\n",
        "    for batch_size in [64, 128, 256, 512, 1024]:\n",
        "        train_loader, val_loader, test_loader = get_dataset(batch_size)\n",
        "        #train_loader, val_loader, test_loader = get_dataset(batch_size = args.batch_size)\n",
        "\n",
        "        if args.optimizer == \"LARS\":\n",
        "            optimizer = LARS(model.parameters(), lr=0.1 * (batch_size / 256), momentum=0.9, weight_decay=0.01)\n",
        "        elif args.optimizer == \"AdamW\":\n",
        "            optimizer = AdamW(model.parameters(), lr=0.001 * (batch_size / 64), weight_decay=0.01)\n",
        "        elif args.optimizer == \"LAMB\":\n",
        "            optimizer = LAMB(model.parameters(), lr=0.001  * (batch_size / 64), weight_decay=0.01)\n",
        "        elif args.optimizer == \"SDGM\":\n",
        "            optimizer = SDGM(model.parameters(), lr=0.01 * (batch_size / 64), weight_decay=0.005)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid optimizer name\")\n",
        "\n",
        "\n",
        "\n",
        "        train_losses, val_losses, train_accuracies, val_accuracies, test_acc = train_model_with_hyperparams(\n",
        "              train_loader,\n",
        "              val_loader,\n",
        "              test_loader,\n",
        "              50,\n",
        "              device,\n",
        "              \"/content/drive/MyDrive/checkpoint1.pth\",\n",
        "              optimizer,\n",
        "              patience=5\n",
        "            )\n",
        "\n",
        "\n",
        "        # Plot results\n",
        "        epochs = range(1, len(train_losses) + 1)\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Plot losses\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, train_losses, label='Train Loss')\n",
        "        plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot accuracies\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
        "        plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot before showing it\n",
        "        save_path = '/content/drive/My Drive/Colab Notebooks/Traning_summary/Traning2'\n",
        "        plt.savefig(save_path + 'training_results.png')\n",
        "        print(\"Training results saved to Google Drive as 'training_results.png'\")\n",
        "        plt.show()\n",
        "\n",
        "        # Save the summary\n",
        "        with open(save_path + 'training_summary.txt', 'w') as f:\n",
        "            f.write(f\"Final Test Accuracy: {test_acc:.2f}%\\n\")\n",
        "            f.write(\"Training and Validation Results:\\n\")\n",
        "            f.write(f\"Train Losses: {train_losses}\\n\")\n",
        "            f.write(f\"Validation Losses: {val_losses}\\n\")\n",
        "            f.write(f\"Train Accuracies: {train_accuracies}\\n\")\n",
        "            f.write(f\"Validation Accuracies: {val_accuracies}\\n\")\n",
        "\n",
        "        print(\"Training summary saved to Google Drive as 'training_summary.txt'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "7_I7B_2eJX9x",
        "outputId": "a681d910-0d7b-42a2-d92a-20b58c01e6e3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-310c52830ce3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#train_loader, val_loader, test_loader = get_dataset(batch_size = args.batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-f8da8517b612>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Compute mean and std for each channel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Mean per channel (R, G, B)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}