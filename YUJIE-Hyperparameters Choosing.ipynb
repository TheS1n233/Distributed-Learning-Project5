{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheS1n233/Distributed-Learning-Project5/blob/test/YUJIE-Hyperparameters%20Choosing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install torch and torchvision"
      ],
      "metadata": {
        "id": "rPFlmzQnEloZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaveP5eCDkCO",
        "outputId": "9793fe40-cef6-4223-cc55-7f0c945caec9",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# install dataset CIFAR-100"
      ],
      "metadata": {
        "id": "LqSl_5NUFTDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "# Data preprocessing\n",
        "class Cutout(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        h, w = img.size(1), img.size(2)\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "        y1 = np.clip(y - self.size // 2, 0, h)\n",
        "        y2 = np.clip(y + self.size // 2, 0, h)\n",
        "        x1 = np.clip(x - self.size // 2, 0, w)\n",
        "        x2 = np.clip(x + self.size // 2, 0, w)\n",
        "        mask[y1: y2, x1: x2] = 0\n",
        "        img = img * torch.from_numpy(mask)\n",
        "        return img\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    Cutout(size=8),  # Add Cutout\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "# Download CIFAR-100 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "# Split training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oP1XDeqExVw",
        "outputId": "98816fc8-8e36-482c-c790-5c00c5e34160"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:03<00:00, 42.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters choosing"
      ],
      "metadata": {
        "id": "egS4XMPKlE-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data preprocessing\n",
        "class Cutout(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # 如果 img 是 PIL.Image，则先转换为 NumPy 数组\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "        y1 = np.clip(y - self.size // 2, 0, h)\n",
        "        y2 = np.clip(y + self.size // 2, 0, h)\n",
        "        x1 = np.clip(x - self.size // 2, 0, w)\n",
        "        x2 = np.clip(x + self.size // 2, 0, w)\n",
        "        mask[y1: y2, x1: x2] = 0\n",
        "        img = img * mask[:, :, np.newaxis]  # 针对 RGB 通道应用遮罩\n",
        "\n",
        "        return Image.fromarray(np.uint8(img))\n",
        "\n",
        "# Data transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    Cutout(size=8),  # Add Cutout\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Download CIFAR-100 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform_test\n",
        ")\n",
        "\n",
        "# Split training and validation sets\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define LeNet-5 model\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.fc3 = nn.Linear(84, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Define random search function\n",
        "def random_search(train_loader, val_loader, model_class, device, num_trials=10):\n",
        "    param_space = {\n",
        "        'lr': [1e-4, 1e-3, 1e-2, 1e-1],\n",
        "        'weight_decay': [1e-5, 1e-4, 1e-3, 1e-2],\n",
        "        'momentum': [0.9, 0.95]\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for trial in range(num_trials):\n",
        "        print(f\"Trial {trial + 1}/{num_trials}\")\n",
        "        lr = random.choice(param_space['lr'])\n",
        "        weight_decay = random.choice(param_space['weight_decay'])\n",
        "        momentum = random.choice(param_space['momentum'])\n",
        "\n",
        "        model = model_class().to(device)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "        scheduler = CosineAnnealingLR(optimizer, T_max=5)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        train_loss, val_loss, val_acc = train_and_evaluate(\n",
        "            model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs=5, device=device\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            'lr': lr,\n",
        "            'weight_decay': weight_decay,\n",
        "            'momentum': momentum,\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc\n",
        "        })\n",
        "\n",
        "    results.sort(key=lambda x: x['val_acc'], reverse=True)\n",
        "    print(\"\\nBest Hyperparameters:\")\n",
        "    print(results[0])\n",
        "\n",
        "    return results[0]  # Return the best result\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_and_evaluate(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs, device):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_acc = 100. * correct / total\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    return loss.item(), val_loss, val_acc\n",
        "\n",
        "# Main function\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    best_hyperparams = random_search(\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        model_class=LeNet5,\n",
        "        device=device,\n",
        "        num_trials=10\n",
        "    )\n",
        "\n",
        "    print(\"\\nUsing Best Hyperparameters:\")\n",
        "    print(best_hyperparams)\n"
      ],
      "metadata": {
        "id": "8EjAWV9jlNZ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37ca7bec-150e-4642-ccc5-ef52b6fb9d2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Trial 1/10\n",
            "Trial 2/10\n",
            "Trial 3/10\n",
            "Trial 4/10\n",
            "Trial 5/10\n",
            "Trial 6/10\n",
            "Trial 7/10\n",
            "Trial 8/10\n",
            "Trial 9/10\n",
            "Trial 10/10\n",
            "\n",
            "Best Hyperparameters:\n",
            "{'lr': 0.01, 'weight_decay': 1e-05, 'momentum': 0.95, 'val_loss': 3.7585234657214706, 'val_acc': 11.8}\n",
            "\n",
            "Using Best Hyperparameters:\n",
            "{'lr': 0.01, 'weight_decay': 1e-05, 'momentum': 0.95, 'val_loss': 3.7585234657214706, 'val_acc': 11.8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Centralized baseline"
      ],
      "metadata": {
        "id": "KpPjPiOTFatY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Initialize device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Training function\n",
        "def train_model(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        train_accuracies.append(100. * correct / total)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "        val_accuracies.append(100. * correct / total)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.2f}%\")\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# Test function\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {100. * correct / total:.2f}%\")\n",
        "\n",
        "# # Training configurations\n",
        "# num_epochs = 150\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # Train with SGDM\n",
        "# model = LeNet5().to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "# print(\"Training with SGDM optimizer...\")\n",
        "# train_losses_sgdm, val_losses_sgdm, train_acc_sgdm, val_acc_sgdm = train_model(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs)\n",
        "# test_model(model, test_loader)\n",
        "\n",
        "# # Train with AdamW\n",
        "# model = LeNet5().to(device)\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=0.0015, weight_decay=0.005)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "# print(\"Training with AdamW optimizer...\")\n",
        "# train_losses_adamw, val_losses_adamw, train_acc_adamw, val_acc_adamw = train_model(model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs)\n",
        "# test_model(model, test_loader)\n",
        "\n",
        "# Plot training and validation results\n",
        "def plot_results(train_losses, val_losses, train_accuracies, val_accuracies, title):\n",
        "    epochs = range(1, num_epochs + 1)\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Plot losses\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss')\n",
        "    plt.plot(epochs, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{title} Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot accuracies\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(epochs, val_accuracies, label='Val Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title(f'{title} Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 从 Hyperparameters choosing 模块加载最佳超参数\n",
        "best_hyperparams = {\n",
        "    'lr': 0.01,  # 示例值，实际应从 random_search 的结果中提取\n",
        "    'weight_decay': 0.005,\n",
        "    'momentum': 0.9\n",
        "}\n",
        "\n",
        "# 使用最佳超参数训练模型\n",
        "model = LeNet5().to(device)\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=best_hyperparams['lr'],\n",
        "    momentum=best_hyperparams['momentum'],\n",
        "    weight_decay=best_hyperparams['weight_decay']\n",
        ")\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=150)  # 假设训练150个epoch\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 开始训练\n",
        "print(\"Training with Best Hyperparameters...\")\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
        "    model, optimizer, criterion, scheduler, train_loader, val_loader, num_epochs=150\n",
        ")\n",
        "\n",
        "# 测试模型\n",
        "test_model(model, test_loader)\n",
        "\n",
        "# 可视化结果\n",
        "plot_results(train_losses, val_losses, train_accuracies, val_accuracies, 'Best Hyperparameters')\n",
        "\n",
        "# # Plot results for SGDM\n",
        "# plot_results(train_losses_sgdm, val_losses_sgdm, train_acc_sgdm, val_acc_sgdm, 'SGDM')\n",
        "\n",
        "# # Plot results for AdamW\n",
        "# plot_results(train_losses_adamw, val_losses_adamw, train_acc_adamw, val_acc_adamw, 'AdamW')\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import runtime\n",
        "print(\"All code has finished running. Disconnecting GPU to save free time...\")\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6FFqW7ZFkWQ",
        "outputId": "64426cfa-9bf1-41bd-dd31-51651ab6d07c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with Best Hyperparameters...\n",
            "Epoch 1/150, Train Loss: 4.5692, Train Acc: 1.59%, Val Loss: 4.4131, Val Acc: 2.75%\n",
            "Epoch 2/150, Train Loss: 4.2829, Train Acc: 4.38%, Val Loss: 4.1755, Val Acc: 5.47%\n",
            "Epoch 3/150, Train Loss: 4.1347, Train Acc: 6.08%, Val Loss: 4.0520, Val Acc: 7.15%\n",
            "Epoch 4/150, Train Loss: 4.0498, Train Acc: 7.36%, Val Loss: 4.0052, Val Acc: 7.78%\n",
            "Epoch 5/150, Train Loss: 4.0131, Train Acc: 8.12%, Val Loss: 3.9771, Val Acc: 8.83%\n",
            "Epoch 6/150, Train Loss: 3.9802, Train Acc: 8.52%, Val Loss: 3.9172, Val Acc: 8.89%\n",
            "Epoch 7/150, Train Loss: 3.9371, Train Acc: 9.13%, Val Loss: 3.9121, Val Acc: 9.13%\n",
            "Epoch 8/150, Train Loss: 3.9091, Train Acc: 9.53%, Val Loss: 3.8591, Val Acc: 10.12%\n",
            "Epoch 9/150, Train Loss: 3.8821, Train Acc: 10.05%, Val Loss: 3.8219, Val Acc: 10.85%\n",
            "Epoch 10/150, Train Loss: 3.8480, Train Acc: 10.58%, Val Loss: 3.8137, Val Acc: 11.60%\n",
            "Epoch 11/150, Train Loss: 3.8261, Train Acc: 11.03%, Val Loss: 3.7444, Val Acc: 12.20%\n",
            "Epoch 12/150, Train Loss: 3.7940, Train Acc: 11.53%, Val Loss: 3.7539, Val Acc: 11.94%\n",
            "Epoch 13/150, Train Loss: 3.7682, Train Acc: 12.00%, Val Loss: 3.7017, Val Acc: 12.66%\n",
            "Epoch 14/150, Train Loss: 3.7440, Train Acc: 12.23%, Val Loss: 3.7146, Val Acc: 13.09%\n",
            "Epoch 15/150, Train Loss: 3.7272, Train Acc: 12.46%, Val Loss: 3.6843, Val Acc: 13.33%\n",
            "Epoch 16/150, Train Loss: 3.7059, Train Acc: 12.79%, Val Loss: 3.6505, Val Acc: 13.68%\n",
            "Epoch 17/150, Train Loss: 3.6902, Train Acc: 12.97%, Val Loss: 3.6990, Val Acc: 12.69%\n",
            "Epoch 18/150, Train Loss: 3.6818, Train Acc: 13.31%, Val Loss: 3.6440, Val Acc: 14.07%\n",
            "Epoch 19/150, Train Loss: 3.6741, Train Acc: 13.39%, Val Loss: 3.6324, Val Acc: 13.90%\n",
            "Epoch 20/150, Train Loss: 3.6626, Train Acc: 13.66%, Val Loss: 3.6083, Val Acc: 14.53%\n",
            "Epoch 21/150, Train Loss: 3.6572, Train Acc: 13.74%, Val Loss: 3.6132, Val Acc: 14.13%\n",
            "Epoch 22/150, Train Loss: 3.6498, Train Acc: 14.00%, Val Loss: 3.5883, Val Acc: 14.41%\n",
            "Epoch 23/150, Train Loss: 3.6495, Train Acc: 14.03%, Val Loss: 3.6197, Val Acc: 14.61%\n",
            "Epoch 24/150, Train Loss: 3.6471, Train Acc: 13.79%, Val Loss: 3.5530, Val Acc: 15.37%\n",
            "Epoch 25/150, Train Loss: 3.6320, Train Acc: 14.09%, Val Loss: 3.6202, Val Acc: 14.71%\n",
            "Epoch 26/150, Train Loss: 3.6333, Train Acc: 14.32%, Val Loss: 3.6320, Val Acc: 13.83%\n",
            "Epoch 27/150, Train Loss: 3.6256, Train Acc: 14.32%, Val Loss: 3.5683, Val Acc: 15.25%\n",
            "Epoch 28/150, Train Loss: 3.6229, Train Acc: 14.34%, Val Loss: 3.5348, Val Acc: 15.85%\n",
            "Epoch 29/150, Train Loss: 3.6171, Train Acc: 14.54%, Val Loss: 3.5648, Val Acc: 15.28%\n",
            "Epoch 30/150, Train Loss: 3.6091, Train Acc: 14.62%, Val Loss: 3.5845, Val Acc: 14.93%\n",
            "Epoch 31/150, Train Loss: 3.6016, Train Acc: 14.65%, Val Loss: 3.5802, Val Acc: 15.04%\n",
            "Epoch 32/150, Train Loss: 3.6029, Train Acc: 14.86%, Val Loss: 3.5796, Val Acc: 14.77%\n",
            "Epoch 33/150, Train Loss: 3.6087, Train Acc: 14.63%, Val Loss: 3.5633, Val Acc: 16.52%\n",
            "Epoch 34/150, Train Loss: 3.6023, Train Acc: 14.69%, Val Loss: 3.5998, Val Acc: 15.40%\n",
            "Epoch 35/150, Train Loss: 3.5975, Train Acc: 14.96%, Val Loss: 3.5835, Val Acc: 15.02%\n",
            "Epoch 36/150, Train Loss: 3.5893, Train Acc: 14.84%, Val Loss: 3.5384, Val Acc: 15.64%\n",
            "Epoch 37/150, Train Loss: 3.5903, Train Acc: 14.77%, Val Loss: 3.5300, Val Acc: 16.45%\n",
            "Epoch 38/150, Train Loss: 3.5775, Train Acc: 15.31%, Val Loss: 3.5184, Val Acc: 15.94%\n",
            "Epoch 39/150, Train Loss: 3.5760, Train Acc: 15.26%, Val Loss: 3.5048, Val Acc: 16.87%\n",
            "Epoch 40/150, Train Loss: 3.5727, Train Acc: 15.37%, Val Loss: 3.5091, Val Acc: 16.53%\n",
            "Epoch 41/150, Train Loss: 3.5717, Train Acc: 15.28%, Val Loss: 3.5029, Val Acc: 16.78%\n",
            "Epoch 42/150, Train Loss: 3.5720, Train Acc: 15.11%, Val Loss: 3.5162, Val Acc: 16.30%\n",
            "Epoch 43/150, Train Loss: 3.5579, Train Acc: 15.22%, Val Loss: 3.5549, Val Acc: 15.56%\n",
            "Epoch 44/150, Train Loss: 3.5631, Train Acc: 15.42%, Val Loss: 3.4898, Val Acc: 16.81%\n",
            "Epoch 45/150, Train Loss: 3.5583, Train Acc: 15.45%, Val Loss: 3.5011, Val Acc: 17.23%\n",
            "Epoch 46/150, Train Loss: 3.5590, Train Acc: 15.49%, Val Loss: 3.4995, Val Acc: 16.17%\n",
            "Epoch 47/150, Train Loss: 3.5565, Train Acc: 15.53%, Val Loss: 3.5197, Val Acc: 15.69%\n",
            "Epoch 48/150, Train Loss: 3.5541, Train Acc: 15.63%, Val Loss: 3.5043, Val Acc: 16.72%\n",
            "Epoch 49/150, Train Loss: 3.5567, Train Acc: 15.68%, Val Loss: 3.4935, Val Acc: 16.50%\n",
            "Epoch 50/150, Train Loss: 3.5413, Train Acc: 15.85%, Val Loss: 3.4685, Val Acc: 16.65%\n",
            "Epoch 51/150, Train Loss: 3.5447, Train Acc: 15.52%, Val Loss: 3.5098, Val Acc: 16.98%\n",
            "Epoch 52/150, Train Loss: 3.5340, Train Acc: 15.85%, Val Loss: 3.4881, Val Acc: 16.50%\n",
            "Epoch 53/150, Train Loss: 3.5386, Train Acc: 15.92%, Val Loss: 3.5001, Val Acc: 16.96%\n",
            "Epoch 54/150, Train Loss: 3.5385, Train Acc: 15.74%, Val Loss: 3.5023, Val Acc: 17.00%\n",
            "Epoch 55/150, Train Loss: 3.5264, Train Acc: 16.16%, Val Loss: 3.4482, Val Acc: 17.16%\n",
            "Epoch 56/150, Train Loss: 3.5315, Train Acc: 16.17%, Val Loss: 3.6058, Val Acc: 14.61%\n",
            "Epoch 57/150, Train Loss: 3.5263, Train Acc: 15.99%, Val Loss: 3.4815, Val Acc: 17.07%\n",
            "Epoch 58/150, Train Loss: 3.5227, Train Acc: 16.12%, Val Loss: 3.4829, Val Acc: 17.22%\n",
            "Epoch 59/150, Train Loss: 3.5142, Train Acc: 16.48%, Val Loss: 3.4820, Val Acc: 16.94%\n",
            "Epoch 60/150, Train Loss: 3.5176, Train Acc: 15.91%, Val Loss: 3.4554, Val Acc: 17.05%\n",
            "Epoch 61/150, Train Loss: 3.5147, Train Acc: 16.34%, Val Loss: 3.4603, Val Acc: 16.90%\n",
            "Epoch 62/150, Train Loss: 3.5088, Train Acc: 16.23%, Val Loss: 3.4406, Val Acc: 17.32%\n",
            "Epoch 63/150, Train Loss: 3.5017, Train Acc: 16.47%, Val Loss: 3.4425, Val Acc: 18.05%\n",
            "Epoch 64/150, Train Loss: 3.4995, Train Acc: 16.55%, Val Loss: 3.4712, Val Acc: 17.29%\n",
            "Epoch 65/150, Train Loss: 3.4987, Train Acc: 16.57%, Val Loss: 3.4329, Val Acc: 18.23%\n",
            "Epoch 66/150, Train Loss: 3.4939, Train Acc: 16.49%, Val Loss: 3.4494, Val Acc: 17.52%\n",
            "Epoch 67/150, Train Loss: 3.4906, Train Acc: 16.71%, Val Loss: 3.4316, Val Acc: 17.63%\n",
            "Epoch 68/150, Train Loss: 3.4936, Train Acc: 16.75%, Val Loss: 3.4447, Val Acc: 17.87%\n",
            "Epoch 69/150, Train Loss: 3.4869, Train Acc: 16.50%, Val Loss: 3.4088, Val Acc: 18.78%\n",
            "Epoch 70/150, Train Loss: 3.4820, Train Acc: 16.62%, Val Loss: 3.4157, Val Acc: 17.58%\n",
            "Epoch 71/150, Train Loss: 3.4762, Train Acc: 16.74%, Val Loss: 3.4661, Val Acc: 17.19%\n",
            "Epoch 72/150, Train Loss: 3.4701, Train Acc: 17.05%, Val Loss: 3.4140, Val Acc: 17.69%\n",
            "Epoch 73/150, Train Loss: 3.4668, Train Acc: 17.10%, Val Loss: 3.3778, Val Acc: 18.12%\n",
            "Epoch 74/150, Train Loss: 3.4592, Train Acc: 17.30%, Val Loss: 3.4072, Val Acc: 18.17%\n",
            "Epoch 75/150, Train Loss: 3.4569, Train Acc: 17.23%, Val Loss: 3.4076, Val Acc: 18.38%\n",
            "Epoch 76/150, Train Loss: 3.4651, Train Acc: 17.20%, Val Loss: 3.3939, Val Acc: 19.06%\n",
            "Epoch 77/150, Train Loss: 3.4536, Train Acc: 17.07%, Val Loss: 3.4217, Val Acc: 17.97%\n",
            "Epoch 78/150, Train Loss: 3.4492, Train Acc: 17.53%, Val Loss: 3.4024, Val Acc: 18.93%\n",
            "Epoch 79/150, Train Loss: 3.4454, Train Acc: 17.25%, Val Loss: 3.3909, Val Acc: 18.96%\n",
            "Epoch 80/150, Train Loss: 3.4375, Train Acc: 17.50%, Val Loss: 3.3757, Val Acc: 18.76%\n",
            "Epoch 81/150, Train Loss: 3.4421, Train Acc: 17.45%, Val Loss: 3.4008, Val Acc: 18.45%\n",
            "Epoch 82/150, Train Loss: 3.4373, Train Acc: 17.42%, Val Loss: 3.3556, Val Acc: 18.90%\n",
            "Epoch 83/150, Train Loss: 3.4385, Train Acc: 17.58%, Val Loss: 3.3744, Val Acc: 19.14%\n",
            "Epoch 84/150, Train Loss: 3.4190, Train Acc: 17.93%, Val Loss: 3.3888, Val Acc: 18.42%\n",
            "Epoch 85/150, Train Loss: 3.4183, Train Acc: 17.88%, Val Loss: 3.3930, Val Acc: 19.00%\n",
            "Epoch 86/150, Train Loss: 3.4218, Train Acc: 17.91%, Val Loss: 3.3924, Val Acc: 19.01%\n",
            "Epoch 87/150, Train Loss: 3.4261, Train Acc: 17.78%, Val Loss: 3.3568, Val Acc: 18.91%\n",
            "Epoch 88/150, Train Loss: 3.4105, Train Acc: 18.07%, Val Loss: 3.3734, Val Acc: 18.68%\n",
            "Epoch 89/150, Train Loss: 3.4067, Train Acc: 18.31%, Val Loss: 3.3619, Val Acc: 19.29%\n",
            "Epoch 90/150, Train Loss: 3.4110, Train Acc: 18.36%, Val Loss: 3.3844, Val Acc: 19.15%\n",
            "Epoch 91/150, Train Loss: 3.4085, Train Acc: 18.09%, Val Loss: 3.3267, Val Acc: 20.23%\n",
            "Epoch 92/150, Train Loss: 3.4018, Train Acc: 18.20%, Val Loss: 3.3677, Val Acc: 19.36%\n",
            "Epoch 93/150, Train Loss: 3.3916, Train Acc: 18.50%, Val Loss: 3.3495, Val Acc: 19.54%\n",
            "Epoch 94/150, Train Loss: 3.3912, Train Acc: 18.28%, Val Loss: 3.3785, Val Acc: 19.70%\n",
            "Epoch 95/150, Train Loss: 3.3895, Train Acc: 18.50%, Val Loss: 3.3402, Val Acc: 19.25%\n",
            "Epoch 96/150, Train Loss: 3.3830, Train Acc: 18.75%, Val Loss: 3.3168, Val Acc: 19.60%\n",
            "Epoch 97/150, Train Loss: 3.3797, Train Acc: 18.69%, Val Loss: 3.3375, Val Acc: 19.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEW OPTIMIZERS"
      ],
      "metadata": {
        "id": "d0lR0zWZC8oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with NEW optimizer in this module"
      ],
      "metadata": {
        "id": "cowCfvAUDjgW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}