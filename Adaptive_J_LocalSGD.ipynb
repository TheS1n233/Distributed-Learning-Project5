{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheS1n233/Distributed-Learning-Project5/blob/experiments/Adaptive_J_LocalSGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KlHheqAGVUVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ef6dd3-c15f-40c9-f761-ac8cd316d9e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2T4ia3AsnT6"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GPmLs0QzJQrO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import argparse\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, SequentialLR, LinearLR\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yoNOiteP6yy"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-sEx0f8JJxK"
      },
      "source": [
        "# Costants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmn-hp0TJNzX"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 25\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKER_LIST = [2]\n",
        "LOCAL_STEPS = [8]\n",
        "LR = 0.01\n",
        "WD = 0.001\n",
        "MOMENTUM = 0.9\n",
        "ALPHA = 1.0\n",
        "BETA = 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNvH-6hEsuxY"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da7YHCPoKnNK"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8oCJ42etBK_"
      },
      "source": [
        "## Function to get train, test and val dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqrDqFDIJbRw"
      },
      "outputs": [],
      "source": [
        "def calulcate_mean_std(batch_size=100, verbose=True):\n",
        "    # Transform only for caluclate meaning of the dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-100 training dataset\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Initialize sums for calculating mean and std\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "\n",
        "    for images, _ in train_loader:\n",
        "        # Compute mean and std for each channel\n",
        "        mean += images.mean(dim=[0, 2, 3])\n",
        "        std += images.std(dim=[0, 2, 3])\n",
        "\n",
        "    mean /= len(train_loader)\n",
        "    std /= len(train_loader)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Mean: \", mean)\n",
        "      print(\"Std: \", std)\n",
        "\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMQZou82OL3K"
      },
      "outputs": [],
      "source": [
        "def get_dataset(batch_size, verbose=True):\n",
        "\n",
        "    print(\"Start loading data with batch_size\", batch_size)\n",
        "\n",
        "    mean, std = calulcate_mean_std()\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.CenterCrop(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-100 dataset\n",
        "    start_time = time.time()\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Split training and validation sets\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "      for i, (inputs, labels) in enumerate(train_loader):\n",
        "          print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "          if i == 10:\n",
        "              break\n",
        "      print(f\"Data loading for 10 batches completed.\")\n",
        "      print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "      print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "      print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "\n",
        "    print(\"Data load correctly...\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BLezwNAJfva"
      },
      "outputs": [],
      "source": [
        "class CheckpointSaver:\n",
        "\n",
        "  def __init__(self, path, additional_info, k, hyperparams, epochs):\n",
        "    self.timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    self.path = f\"{path}/K={k}_Comparison_{self.timestamp}\"\n",
        "    self.additional_info = additional_info\n",
        "    self.k = k\n",
        "    self.hyperparams = hyperparams\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def create_files(self):\n",
        "    os.makedirs(self.path, exist_ok=True)\n",
        "\n",
        "    self.metrics_files = {\n",
        "        'global_train_acc': os.path.join(self.path, f'adaptive_train_accuracy_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'val_acc': os.path.join(self.path, f'adaptive_val_accuracy_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'global_train_loss': os.path.join(self.path, f'adaptive_train_loss_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'val_loss': os.path.join(self.path, f'adaptive_val_loss_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'summary': os.path.join(self.path, f'adaptive_summary_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(self.path, f'adaptive_experiment_config_{self.timestamp}_{str(self.additional_info)}.txt'), 'w') as f:\n",
        "        f.write(f\"Experiment Configuration:\\n\")\n",
        "        f.write(f\"LocalSGD\\n\")\n",
        "        f.write(f\"K = {self.k};\\n\")\n",
        "        f.write(f\"Hyperparameters: {str(self.hyperparams)}\\n\")\n",
        "        f.write(f\"Number of epochs: {self.epochs}\\n\")\n",
        "        f.write(f\"Timestamp: {self.timestamp}\\n\")\n",
        "\n",
        "  def save_sumamry(self, epoch, global_train_accuracy, val_acc, global_train_loss, val_loss, stats):\n",
        "      try:\n",
        "          with open(self.metrics_files['summary'], 'a') as f:\n",
        "            f.write(f\"Epoch {epoch}\\n\")\n",
        "            f.write(f\"Train Loss: {global_train_loss:.4f}, Train Acc: {global_train_accuracy * 100:.2f}%\\n\")\n",
        "            f.write(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc * 100:.2f}%\\n\")\n",
        "            f.write(f\"Statistics: \\n{stats}\\n\")\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n",
        "  def save_checkpoint(self, epoch, global_train_accuracy, val_acc, global_train_loss, val_loss):\n",
        "      try:\n",
        "          with open(self.metrics_files['global_train_acc'], 'a') as f:\n",
        "              f.write(f\"{epoch},{global_train_accuracy * 100:.2f}%\\n\")\n",
        "          with open(self.metrics_files['val_acc'], 'a') as f:\n",
        "              f.write(f\"{epoch},{val_acc * 100:.2f}%\\n\")\n",
        "          with open(self.metrics_files['global_train_loss'], 'a') as f:\n",
        "              f.write(f\"{epoch},{global_train_loss:.4f}\\n\")\n",
        "          with open(self.metrics_files['val_loss'], 'a') as f:\n",
        "              f.write(f\"{epoch},{val_loss:.4f}\\n\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n",
        "  def end_sumamry(self, test_acc):\n",
        "      try:\n",
        "          with open(self.metrics_files['summary'], 'a') as f:\n",
        "            f.write(f\"test_acc {test_acc}\\n\")\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeTracker:\n",
        "    def __init__(self):\n",
        "        self.computation_times = []\n",
        "        self.communication_times = []\n",
        "        self.epoch_avg_communication_time = []\n",
        "        self.total_computation = 0.0\n",
        "        self.total_communication = 0.0\n",
        "        self.epoch_comp_times = []\n",
        "        self.epoch_comm_times = []\n",
        "        self.epoch_ratio_times = []\n",
        "        self.epoch_local_step = []\n",
        "\n",
        "    def start_computation(self):\n",
        "        self.comp_start = time.time()\n",
        "\n",
        "    def end_computation(self):\n",
        "        comp_time = time.time() - self.comp_start\n",
        "        self.total_computation += comp_time\n",
        "        self.computation_times.append(comp_time)\n",
        "        return comp_time\n",
        "\n",
        "    def start_communication(self):\n",
        "        self.comm_start = time.time()\n",
        "\n",
        "    def end_communication(self):\n",
        "        comm_time = time.time() - self.comm_start\n",
        "        self.total_communication += comm_time\n",
        "        self.communication_times.append(comm_time)\n",
        "        return comm_time\n",
        "\n",
        "    def record_epoch(self,local_step):\n",
        "        \"\"\"Record times for the current epoch\"\"\"\n",
        "        self.epoch_avg_communication_time.append(np.mean(self.communication_times))\n",
        "        self.epoch_comp_times.append(self.total_computation)\n",
        "        self.epoch_comm_times.append(self.total_communication)\n",
        "        self.epoch_ratio_times.append(self.total_computation / (self.total_communication + 1e-10))\n",
        "        self.epoch_local_step.append(local_step)\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"Return summary statistics\"\"\"\n",
        "        stats = {\n",
        "            'avg_computation_time': np.mean(self.computation_times),\n",
        "            'avg_communication_time': np.mean(self.communication_times),\n",
        "            'total_computation_time': self.total_computation,\n",
        "            'total_communication_time': self.total_communication,\n",
        "            'computation_to_communication_ratio': np.mean(self.epoch_ratio_times)\n",
        "        }\n",
        "        return stats\n",
        "\n",
        "    def str_statistics(self):\n",
        "        \"\"\"Return summary statistics\"\"\"\n",
        "        stats = f\"\"\"\n",
        "            'avg_computation_time': {np.mean(self.computation_times)},\n",
        "            'avg_communication_time': {np.mean(self.communication_times)},\n",
        "            'total_computation_time': {self.total_computation},\n",
        "            'total_communication_time': {self.total_communication},\n",
        "            'computation_to_communication_ratio': {np.mean(self.epoch_ratio_times)}\n",
        "        \"\"\"\n",
        "        return stats\n",
        "\n",
        "    def plot_times(self):\n",
        "        \"\"\"Plot computation and communication times\"\"\"\n",
        "        plt.figure(figsize=(12, 10))\n",
        "\n",
        "        # First graph: Plot cumulative times per epoch\n",
        "        plt.subplot(2, 1, 1)\n",
        "        epochs = range(1, len(self.epoch_comp_times) + 1)\n",
        "        plt.plot(epochs, self.epoch_ratio_times, label='ratio_times')\n",
        "\n",
        "        # Aggiungere linee verticali quando i valori in `epoch_local_step` cambiano\n",
        "        for i in range(1, len(self.epoch_local_step)):\n",
        "            if self.epoch_local_step[i] != self.epoch_local_step[i - 1]:\n",
        "                plt.axvline(x=i + 1, color='red', linestyle='--', alpha=0.7, label='Change' if i == 1 else \"\")\n",
        "\n",
        "        plt.title('Cumulative Times per Epoch - Ratio')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Total Time (s)')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(2, 1, 2)\n",
        "        plt.plot(epochs, self.epoch_avg_communication_time, label='comm_time')\n",
        "\n",
        "        # Aggiungere linee verticali quando i valori in `epoch_local_step` cambiano\n",
        "        for i in range(1, len(self.epoch_local_step)):\n",
        "            if self.epoch_local_step[i] != self.epoch_local_step[i - 1]:\n",
        "                if self.epoch_local_step[i] > self.epoch_local_step[i - 1]:\n",
        "                    plt.axvline(x=i + 1, color='red', linestyle='--', alpha=0.7, label='Change' if i == 1 else \"\")\n",
        "                else:\n",
        "                    plt.axvline(x=i + 1, color='green', linestyle='--', alpha=0.7, label='Change' if i == 1 else \"\")\n",
        "\n",
        "\n",
        "        plt.title('Cumulative Times per Epoch - Comm time')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Total Time (s)')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "K9KMDGQ3xHkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJaPDN4Qna1P"
      },
      "source": [
        "# Adaptive J LocalSGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFiaXpZUnfDQ"
      },
      "outputs": [],
      "source": [
        "def split_cifar100(dataset, num_workers):\n",
        "    indices = np.random.permutation(len(dataset))\n",
        "    splits = np.array_split(indices, num_workers)\n",
        "    return [torch.utils.data.Subset(dataset, split) for split in splits]\n",
        "\n",
        "\n",
        "def local_sgd_adaptive_steps(train_dataset, val_loader, test_loader, device, num_workers, ini_local_steps, num_epochs, batch_size, hyperparams):\n",
        "    dataset_size = len(train_dataset)\n",
        "    datasets = split_cifar100(train_dataset, num_workers)\n",
        "    workers = [\n",
        "        torch.utils.data.DataLoader(\n",
        "            datasets[i],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "        for i in range(num_workers)\n",
        "    ]\n",
        "\n",
        "    TOTAL_ITERATIONS = (num_epochs * (dataset_size // batch_size))\n",
        "    total_iterations = (num_epochs * (dataset_size // batch_size)) // (num_workers * ini_local_steps)\n",
        "    iterations_per_epoch = total_iterations // num_epochs\n",
        "    INI_iterations_per_epoch = iterations_per_epoch\n",
        "    remain_iterations = TOTAL_ITERATIONS\n",
        "\n",
        "    time_tracker = TimeTracker()\n",
        "    check_point_saver = CheckpointSaver(\n",
        "        path=f\"/content/drive/My Drive/Colab Notebooks/Traning_summary/\",\n",
        "        additional_info=f\"K={num_workers}\",\n",
        "        k=num_workers,\n",
        "        hyperparams=hyperparams,\n",
        "        epochs=num_epochs\n",
        "    )\n",
        "\n",
        "    check_point_saver.create_files()\n",
        "\n",
        "    global_model = LeNet5().to(device)\n",
        "    local_models = [copy.deepcopy(global_model).to(device) for _ in range(num_workers)]\n",
        "    local_optimizers = [\n",
        "        optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            weight_decay=hyperparams['weight_decay'],\n",
        "            momentum=hyperparams['momentum'],\n",
        "        )\n",
        "        for model in local_models\n",
        "    ]\n",
        "\n",
        "    schedulers = [CosineAnnealingLR(opt, T_max=num_epochs) for opt in local_optimizers]\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    local_weights = [len(datasets[i].indices) / dataset_size for i in range(num_workers)]\n",
        "\n",
        "    val_losses, val_accuracies = [], []\n",
        "    train_losses, train_accuracies = [], []\n",
        "    local_steps = ini_local_steps\n",
        "    worker_iters = [iter(workers[i]) for i in range(num_workers)]\n",
        "\n",
        "    for loop_index in range(2):\n",
        "\n",
        "      for epoch in range(1, num_epochs + 1):\n",
        "          # Update synchronization steps dynamically based on local steps\n",
        "          iterations_per_epoch = (dataset_size // batch_size) // (num_workers * local_steps)\n",
        "          print(f\"Adaptive_iterations_per_epoch: {iterations_per_epoch}\")\n",
        "          for iteration in range(iterations_per_epoch):\n",
        "              # Local training for local_steps\n",
        "\n",
        "              time_tracker.start_computation()\n",
        "\n",
        "              for worker_id in range(num_workers):\n",
        "                  local_models[worker_id].train()\n",
        "                  for _ in range(local_steps):\n",
        "                      try:\n",
        "                          inputs, labels = next(worker_iters[worker_id])\n",
        "                      except StopIteration:\n",
        "                          worker_iters[worker_id] = iter(workers[worker_id])\n",
        "                          inputs, labels = next(worker_iters[worker_id])\n",
        "\n",
        "                      inputs, labels = inputs.to(device), labels.to(device)\n",
        "                      local_optimizers[worker_id].zero_grad()\n",
        "                      outputs = local_models[worker_id](inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "                      loss.backward()\n",
        "                      local_optimizers[worker_id].step()\n",
        "\n",
        "              time_tracker.end_computation()\n",
        "              time_tracker.start_communication()\n",
        "\n",
        "\n",
        "              # Model averaging after local_steps\n",
        "              with torch.no_grad():\n",
        "                  global_state_dict = global_model.state_dict()\n",
        "                  for key in global_state_dict.keys():\n",
        "                      global_state_dict[key] = torch.sum(\n",
        "                          torch.stack([\n",
        "                              local_weights[i] * local_models[i].state_dict()[key]\n",
        "                              for i in range(num_workers)\n",
        "                          ]),\n",
        "                          dim=0\n",
        "                      )\n",
        "\n",
        "                  global_model.load_state_dict(global_state_dict)\n",
        "                  # Update local models with averaged weights\n",
        "                  for local_model in local_models:\n",
        "                      local_model.load_state_dict(global_state_dict)\n",
        "\n",
        "              time_tracker.end_communication()\n",
        "\n",
        "          global_model.eval()\n",
        "          global_train_loss, global_correct, global_total = 0.0, 0, 0\n",
        "          with torch.no_grad():\n",
        "                # Train on the global model after local updates\n",
        "                for inputs, labels in workers[0]:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = global_model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    global_train_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    global_correct += predicted.eq(labels).sum().item()\n",
        "                    global_total += labels.size(0)\n",
        "          global_train_loss /= len(workers[0])\n",
        "          global_train_acc = global_correct / global_total\n",
        "          train_losses.append(global_train_loss)\n",
        "          train_accuracies.append(global_train_acc)\n",
        "\n",
        "          val_acc, val_loss = val_model(global_model, val_loader, criterion, device)\n",
        "          val_losses.append(val_loss)\n",
        "          val_accuracies.append(val_acc)\n",
        "          remain_iterations = remain_iterations - (iterations_per_epoch * num_workers * local_steps)\n",
        "\n",
        "          time_tracker.record_epoch(local_steps)\n",
        "          print(f\"Statistics\", time_tracker.str_statistics())\n",
        "\n",
        "          # adjustment of local steps\n",
        "          if epoch > 1:\n",
        "              # Adjust delta_min based on epoch range\n",
        "\n",
        "              delta_loss = (val_losses[-2] - val_loss) / val_losses[-2]\n",
        "              if epoch <= num_epochs * 0.2:\n",
        "                  delta_min = hyperparams['delta_min_initial']\n",
        "              elif epoch <= num_epochs * 0.8:\n",
        "                  delta_min = hyperparams['delta_min_mid']\n",
        "              else:\n",
        "                  delta_min = hyperparams['delta_min_final']\n",
        "\n",
        "              if 0 < delta_loss < delta_min:  # Slow convergence\n",
        "                  local_steps = min(local_steps * 2, hyperparams['max_local_steps'])\n",
        "                  print(f\"Delta loss: {delta_loss} | Epoch {epoch}: Increasing local steps000 to {local_steps}\")\n",
        "\n",
        "              if epoch > num_epochs * 0.3:\n",
        "                if delta_loss < 0 and abs(delta_loss) > hyperparams['delta_min_initial']:  # Significant oscillation\n",
        "                    local_steps = max(local_steps // 2, hyperparams['min_local_steps'])\n",
        "                    print(f\"Delta loss: {delta_loss} | Epoch {epoch}: Decreasing local steps222 due to significant oscillation to {local_steps}\")\n",
        "\n",
        "\n",
        "          # Log training and validation metrics\n",
        "\n",
        "          check_point_saver.save_checkpoint(1, global_train_acc, val_acc, global_train_loss, val_loss)\n",
        "\n",
        "          check_point_saver.save_sumamry(epoch, global_train_acc, val_acc, global_train_loss, val_loss, time_tracker.str_statistics())\n",
        "          print(f\"Epoch {epoch} / {num_epochs} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc * 100:.2f}%\")\n",
        "\n",
        "          # Step schedulers after each epoch\n",
        "          for scheduler in schedulers:\n",
        "              scheduler.step()\n",
        "          for worker_id, optimizer in enumerate(local_optimizers):\n",
        "              current_lr = optimizer.param_groups[0]['lr']\n",
        "              print(f\"Epoch {epoch}, Worker {worker_id}: Current Learning Rate = {current_lr:.6f}\")\n",
        "      # Calculate how many iterations we still have, and keep training\n",
        "      if loop_index == 0:\n",
        "        remain_epochs = max(1, remain_iterations // (num_workers * ini_local_steps * INI_iterations_per_epoch))\n",
        "      num_epochs = remain_epochs\n",
        "      print(f\"Remain iterations:{remain_iterations}| Remain Epoch: {num_epochs}\")\n",
        "      local_steps = ini_local_steps\n",
        "      last_lr = 0.0001 #initial 2nd loop learning rate\n",
        "      for optimizer in local_optimizers:\n",
        "        for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = last_lr\n",
        "      new_schedulers = [\n",
        "        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "        for optimizer in local_optimizers\n",
        "      ]\n",
        "      schedulers = new_schedulers\n",
        "    # Test the final model\n",
        "    test_acc = test_model(global_model, test_loader, device)\n",
        "\n",
        "    time_tracker.plot_times()\n",
        "    check_point_saver.end_sumamry(test_acc)\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies, test_acc, num_epochs\n",
        "\n",
        "\n",
        "\n",
        "def val_model(global_model, val_loader, criterion, device):\n",
        "    global_model.eval()\n",
        "    val_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    return val_acc, val_loss\n",
        "\n",
        "def test_model(global_model, test_loader, device):\n",
        "    global_model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "    return test_acc\n",
        "\n",
        "\n",
        "def plot_results(train_losses, val_losses, train_accuracies, val_accuracies, NUM_EPOCHS, num_epochs):\n",
        "    total_epochs = NUM_EPOCHS + num_epochs\n",
        "    x_vals = list(range(1, total_epochs + 1))\n",
        "    assert len(train_losses) == total_epochs, f\"Mismatch: train_losses={len(train_losses)} vs total_epochs={total_epochs}\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x_vals, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(x_vals, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss over Epochs\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x_vals, train_accuracies, label=\"Train Accuracy\")\n",
        "    plt.plot(x_vals, val_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Accuracy over Epochs\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    num_epochs = NUM_EPOCHS\n",
        "    batch_size = BATCH_SIZE\n",
        "    num_workers_list = NUM_WORKER_LIST\n",
        "    local_steps_list = LOCAL_STEPS\n",
        "    hyperparams = {\n",
        "        'lr': LR,\n",
        "        'weight_decay': WD,\n",
        "        'momentum': MOMENTUM,\n",
        "        'delta_min_initial': 0.02,\n",
        "        'delta_min_mid': 0.005,\n",
        "        'delta_min_final': 0.002,\n",
        "        'max_local_steps': 64,\n",
        "        'min_local_steps': 4\n",
        "    }\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_dataset(batch_size)\n",
        "    train_dataset = train_loader.dataset\n",
        "\n",
        "    for num_workers in num_workers_list:\n",
        "        for ini_local_steps in local_steps_list:\n",
        "            print(f\"In the beginning running LocalSGD with {num_workers} workers and {ini_local_steps} initial local steps\")\n",
        "            train_losses, val_losses, train_accuracies, val_accuracies, test_acc, num_epochs = local_sgd_adaptive_steps(\n",
        "                train_dataset=train_dataset,\n",
        "                val_loader=val_loader,\n",
        "                test_loader=test_loader,\n",
        "                device=device,\n",
        "                num_workers=num_workers,\n",
        "                ini_local_steps=ini_local_steps,\n",
        "                num_epochs=num_epochs,\n",
        "                batch_size=batch_size,\n",
        "                hyperparams=hyperparams,\n",
        "            )\n",
        "\n",
        "\n",
        "            plot_results(train_losses, val_losses, train_accuracies, val_accuracies, NUM_EPOCHS, num_epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yq14ax2G2tcx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bNvH-6hEsuxY"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}