{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheS1n233/Distributed-Learning-Project5/blob/experiments/Adaptive_J_LocalSGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlHheqAGVUVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11b5ab8-6c72-411f-cba9-14e23b900e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2T4ia3AsnT6"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPmLs0QzJQrO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import argparse\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, SequentialLR, LinearLR\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yoNOiteP6yy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4fb1a1-7dcc-4bbe-91c6-ac933a4d6116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-sEx0f8JJxK"
      },
      "source": [
        "# Costants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmn-hp0TJNzX"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 150\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKER_LIST = [2]\n",
        "LOCAL_STEPS = [8]\n",
        "LR = 0.01\n",
        "WD = 0.001\n",
        "MOMENTUM = 0.9\n",
        "ALPHA = 1.0\n",
        "BETA = 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNvH-6hEsuxY"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da7YHCPoKnNK"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8oCJ42etBK_"
      },
      "source": [
        "## Function to get train, test and val dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqrDqFDIJbRw"
      },
      "outputs": [],
      "source": [
        "def calulcate_mean_std(batch_size=100, verbose=True):\n",
        "    # Transform only for caluclate meaning of the dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-100 training dataset\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Initialize sums for calculating mean and std\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "\n",
        "    for images, _ in train_loader:\n",
        "        # Compute mean and std for each channel\n",
        "        mean += images.mean(dim=[0, 2, 3])\n",
        "        std += images.std(dim=[0, 2, 3])\n",
        "\n",
        "    mean /= len(train_loader)\n",
        "    std /= len(train_loader)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Mean: \", mean)\n",
        "      print(\"Std: \", std)\n",
        "\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMQZou82OL3K"
      },
      "outputs": [],
      "source": [
        "def get_dataset(batch_size, verbose=True):\n",
        "\n",
        "    print(\"Start loading data with batch_size\", batch_size)\n",
        "\n",
        "    mean, std = calulcate_mean_std()\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.CenterCrop(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-100 dataset\n",
        "    start_time = time.time()\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Split training and validation sets\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "      for i, (inputs, labels) in enumerate(train_loader):\n",
        "          print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "          if i == 10:\n",
        "              break\n",
        "      print(f\"Data loading for 10 batches completed.\")\n",
        "      print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "      print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "      print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "\n",
        "    print(\"Data load correctly...\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BLezwNAJfva"
      },
      "outputs": [],
      "source": [
        "class CheckpointSaver:\n",
        "\n",
        "  def __init__(self, path, additional_info, k, j, hyperparams, epochs):\n",
        "    self.timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    self.path = f\"{path}/K={k}_J={j}_Comparison_{self.timestamp}\"\n",
        "    self.additional_info = additional_info\n",
        "    self.k = k\n",
        "    self.j = j\n",
        "    self.hyperparams = hyperparams\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def create_files(self):\n",
        "    os.makedirs(self.path, exist_ok=True)\n",
        "\n",
        "    self.metrics_files = {\n",
        "        'global_train_acc': os.path.join(self.path, f'slowmo_train_accuracy_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'val_acc': os.path.join(self.path, f'slowmo_val_accuracy_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'global_train_loss': os.path.join(self.path, f'slowmo_train_loss_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'val_loss': os.path.join(self.path, f'slowmo_val_loss_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'summary': os.path.join(self.path, f'slowmo_summary_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(self.path, f'slowmo_experiment_config_{self.timestamp}_{str(self.additional_info)}.txt'), 'w') as f:\n",
        "        f.write(f\"Experiment Configuration:\\n\")\n",
        "        f.write(f\"LocalSGD\\n\")\n",
        "        f.write(f\"K = {self.k}; J = {self.j}\\n\")\n",
        "        f.write(f\"Hyperparameters: {str(self.hyperparams)}\\n\")\n",
        "        f.write(f\"Number of epochs: {self.epochs}\\n\")\n",
        "        f.write(f\"Timestamp: {self.timestamp}\\n\")\n",
        "\n",
        "  def save_sumamry(self, epoch, global_train_accuracy, val_acc, global_train_loss, val_loss):\n",
        "      try:\n",
        "          with open(self.metrics_files['summary'], 'a') as f:\n",
        "            f.write(f\"Epoch {epoch}\\n\")\n",
        "            f.write(f\"Train Loss: {global_train_loss:.4f}, Train Acc: {global_train_accuracy * 100:.2f}%\\n\")\n",
        "            f.write(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc * 100:.2f}%\\n\")\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n",
        "  def save_checkpoint(self, epoch, global_train_accuracy, val_acc, global_train_loss, val_loss):\n",
        "      try:\n",
        "          with open(self.metrics_files['global_train_acc'], 'a') as f:\n",
        "              f.write(f\"{epoch},{global_train_accuracy * 100:.2f}%\\n\")\n",
        "          with open(self.metrics_files['val_acc'], 'a') as f:\n",
        "              f.write(f\"{epoch},{val_acc * 100:.2f}%\\n\")\n",
        "          with open(self.metrics_files['global_train_loss'], 'a') as f:\n",
        "              f.write(f\"{epoch},{global_train_loss:.4f}\\n\")\n",
        "          with open(self.metrics_files['val_loss'], 'a') as f:\n",
        "              f.write(f\"{epoch},{val_loss:.4f}\\n\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n",
        "  def end_sumamry(self, test_acc):\n",
        "      try:\n",
        "          with open(self.metrics_files['summary'], 'a') as f:\n",
        "            f.write(f\"test_acc {test_acc}\\n\")\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJaPDN4Qna1P"
      },
      "source": [
        "# LocalSGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFiaXpZUnfDQ",
        "outputId": "aff0f5e1-922f-404b-cb6a-53b761c9bce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start loading data with batch_size 64\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:03<00:00, 43.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Mean:  tensor([0.5071, 0.4865, 0.4409])\n",
            "Std:  tensor([0.2667, 0.2558, 0.2754])\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset loading time: 2.56 seconds\n",
            "Batch 0: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 1: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 2: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 3: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 4: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 5: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 6: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 7: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 8: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 9: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 10: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Data loading for 10 batches completed.\n",
            "Training dataset size: 40000\n",
            "Validation dataset size: 10000\n",
            "Test dataset size: 10000\n",
            "Data load correctly...\n",
            "In the beginning running LocalSGD with 2 workers and 8 initial local steps\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 1 / 150 | Val Loss: 4.0564, Val Acc: 7.51%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 2 / 150 | Val Loss: 3.7420, Val Acc: 12.44%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 3 / 150 | Val Loss: 3.4830, Val Acc: 16.71%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 4 / 150 | Val Loss: 3.2879, Val Acc: 20.55%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 5 / 150 | Val Loss: 3.1317, Val Acc: 22.76%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 6 / 150 | Val Loss: 2.9960, Val Acc: 25.74%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 7 / 150 | Val Loss: 2.8951, Val Acc: 27.96%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 8 / 150 | Val Loss: 2.7885, Val Acc: 29.54%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Delta loss: 0.00729092735107743 | Epoch 9: Increasing local steps000 to 16\n",
            "Epoch 9 / 150 | Val Loss: 2.7681, Val Acc: 30.95%\n",
            "Adaptive_iterations_per_epoch: 19\n",
            "Epoch 10 / 150 | Val Loss: 2.7030, Val Acc: 31.94%\n",
            "Adaptive_iterations_per_epoch: 19\n",
            "Delta loss: 0.018542570970174563 | Epoch 11: Increasing local steps000 to 32\n",
            "Epoch 11 / 150 | Val Loss: 2.6529, Val Acc: 32.66%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Delta loss: 0.016321189835988317 | Epoch 12: Increasing local steps000 to 64\n",
            "Epoch 12 / 150 | Val Loss: 2.6096, Val Acc: 33.69%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.018012725614385508 | Epoch 13: Increasing local steps000 to 64\n",
            "Epoch 13 / 150 | Val Loss: 2.5626, Val Acc: 35.14%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.010666138267176752 | Epoch 14: Increasing local steps000 to 64\n",
            "Epoch 14 / 150 | Val Loss: 2.5352, Val Acc: 35.89%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.008354848509994807 | Epoch 15: Increasing local steps000 to 64\n",
            "Epoch 15 / 150 | Val Loss: 2.5141, Val Acc: 35.55%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.007770583237490612 | Epoch 16: Increasing local steps000 to 64\n",
            "Epoch 16 / 150 | Val Loss: 2.4945, Val Acc: 35.72%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.011591988002147885 | Epoch 17: Increasing local steps000 to 64\n",
            "Epoch 17 / 150 | Val Loss: 2.4656, Val Acc: 36.97%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.00938401776054351 | Epoch 18: Increasing local steps000 to 64\n",
            "Epoch 18 / 150 | Val Loss: 2.4425, Val Acc: 37.11%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 19 / 150 | Val Loss: 2.4595, Val Acc: 36.72%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.016289198735004715 | Epoch 20: Increasing local steps000 to 64\n",
            "Epoch 20 / 150 | Val Loss: 2.4194, Val Acc: 38.05%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.013110139074161838 | Epoch 21: Increasing local steps000 to 64\n",
            "Epoch 21 / 150 | Val Loss: 2.3877, Val Acc: 39.13%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.004508226263702511 | Epoch 22: Increasing local steps000 to 64\n",
            "Epoch 22 / 150 | Val Loss: 2.3769, Val Acc: 39.06%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.008197219389478998 | Epoch 23: Increasing local steps000 to 64\n",
            "Epoch 23 / 150 | Val Loss: 2.3574, Val Acc: 39.20%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 24 / 150 | Val Loss: 2.3912, Val Acc: 38.73%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 25 / 150 | Val Loss: 2.3375, Val Acc: 39.88%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.004608157625571045 | Epoch 26: Increasing local steps000 to 64\n",
            "Epoch 26 / 150 | Val Loss: 2.3267, Val Acc: 40.13%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.007478341845185689 | Epoch 27: Increasing local steps000 to 64\n",
            "Epoch 27 / 150 | Val Loss: 2.3093, Val Acc: 40.14%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.003621335134721999 | Epoch 28: Increasing local steps000 to 64\n",
            "Epoch 28 / 150 | Val Loss: 2.3009, Val Acc: 41.11%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0074582483866031364 | Epoch 29: Increasing local steps000 to 64\n",
            "Epoch 29 / 150 | Val Loss: 2.2838, Val Acc: 41.24%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.00824843081764437 | Epoch 30: Increasing local steps000 to 64\n",
            "Epoch 30 / 150 | Val Loss: 2.2649, Val Acc: 41.60%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 31 / 150 | Val Loss: 2.2920, Val Acc: 41.29%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 32 / 150 | Val Loss: 2.2479, Val Acc: 42.12%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 33 / 150 | Val Loss: 2.2760, Val Acc: 41.74%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 34 / 150 | Val Loss: 2.2273, Val Acc: 41.80%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 35 / 150 | Val Loss: 2.2474, Val Acc: 42.30%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.003909492430165405 | Epoch 36: Increasing local steps000 to 64\n",
            "Epoch 36 / 150 | Val Loss: 2.2386, Val Acc: 42.67%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0023427846263812855 | Epoch 37: Increasing local steps000 to 64\n",
            "Epoch 37 / 150 | Val Loss: 2.2334, Val Acc: 42.46%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 38 / 150 | Val Loss: 2.2053, Val Acc: 43.30%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 39 / 150 | Val Loss: 2.2487, Val Acc: 42.54%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 40 / 150 | Val Loss: 2.1948, Val Acc: 42.95%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 41 / 150 | Val Loss: 2.2023, Val Acc: 42.92%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0012833684070575572 | Epoch 42: Increasing local steps000 to 64\n",
            "Epoch 42 / 150 | Val Loss: 2.1995, Val Acc: 43.58%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 43 / 150 | Val Loss: 2.2132, Val Acc: 42.56%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 44 / 150 | Val Loss: 2.1595, Val Acc: 44.04%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 45 / 150 | Val Loss: 2.1914, Val Acc: 43.98%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 46 / 150 | Val Loss: 2.1657, Val Acc: 43.85%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 47 / 150 | Val Loss: 2.1988, Val Acc: 43.98%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 48 / 150 | Val Loss: 2.1776, Val Acc: 44.08%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 49 / 150 | Val Loss: 2.1585, Val Acc: 44.41%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0032229628193013943 | Epoch 50: Increasing local steps000 to 64\n",
            "Epoch 50 / 150 | Val Loss: 2.1515, Val Acc: 45.34%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 51 / 150 | Val Loss: 2.1392, Val Acc: 44.91%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.004824824578139012 | Epoch 52: Increasing local steps000 to 64\n",
            "Epoch 52 / 150 | Val Loss: 2.1289, Val Acc: 45.02%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: -0.0295269437192466 | Epoch 53: Decreasing local steps222 due to significant oscillation to 32\n",
            "Epoch 53 / 150 | Val Loss: 2.1918, Val Acc: 44.45%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 54 / 150 | Val Loss: 2.1656, Val Acc: 44.69%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Delta loss: 0.0025501159411540466 | Epoch 55: Increasing local steps000 to 64\n",
            "Epoch 55 / 150 | Val Loss: 2.1601, Val Acc: 45.85%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.002568642819653772 | Epoch 56: Increasing local steps000 to 64\n",
            "Epoch 56 / 150 | Val Loss: 2.1546, Val Acc: 45.52%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 57 / 150 | Val Loss: 2.1619, Val Acc: 44.92%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0019921491263161398 | Epoch 58: Increasing local steps000 to 64\n",
            "Epoch 58 / 150 | Val Loss: 2.1576, Val Acc: 45.30%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0039506572713134906 | Epoch 59: Increasing local steps000 to 64\n",
            "Epoch 59 / 150 | Val Loss: 2.1491, Val Acc: 45.73%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 60 / 150 | Val Loss: 2.1750, Val Acc: 45.29%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 61 / 150 | Val Loss: 2.1817, Val Acc: 45.91%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 62 / 150 | Val Loss: 2.1137, Val Acc: 45.56%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 63 / 150 | Val Loss: 2.1341, Val Acc: 45.55%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 64 / 150 | Val Loss: 2.1398, Val Acc: 46.41%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 65 / 150 | Val Loss: 2.1444, Val Acc: 45.81%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 66 / 150 | Val Loss: 2.1464, Val Acc: 46.42%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 67 / 150 | Val Loss: 2.1137, Val Acc: 46.28%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 68 / 150 | Val Loss: 2.1467, Val Acc: 45.86%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 69 / 150 | Val Loss: 2.1106, Val Acc: 47.10%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 70 / 150 | Val Loss: 2.1405, Val Acc: 45.66%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.004033176913637793 | Epoch 71: Increasing local steps000 to 64\n",
            "Epoch 71 / 150 | Val Loss: 2.1318, Val Acc: 46.79%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 72 / 150 | Val Loss: 2.1730, Val Acc: 46.66%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 73 / 150 | Val Loss: 2.1252, Val Acc: 46.82%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0011500626062438007 | Epoch 74: Increasing local steps000 to 64\n",
            "Epoch 74 / 150 | Val Loss: 2.1227, Val Acc: 46.61%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 75 / 150 | Val Loss: 2.1395, Val Acc: 47.34%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0020903698685555694 | Epoch 76: Increasing local steps000 to 64\n",
            "Epoch 76 / 150 | Val Loss: 2.1350, Val Acc: 46.85%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 77 / 150 | Val Loss: 2.1648, Val Acc: 46.07%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 78 / 150 | Val Loss: 2.1213, Val Acc: 47.01%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 79 / 150 | Val Loss: 2.1618, Val Acc: 46.83%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 80 / 150 | Val Loss: 2.1486, Val Acc: 47.35%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 81 / 150 | Val Loss: 2.1537, Val Acc: 46.92%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 82 / 150 | Val Loss: 2.1336, Val Acc: 47.92%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 83 / 150 | Val Loss: 2.1584, Val Acc: 47.64%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 84 / 150 | Val Loss: 2.1596, Val Acc: 47.25%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 85 / 150 | Val Loss: 2.1605, Val Acc: 46.90%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 86 / 150 | Val Loss: 2.1721, Val Acc: 47.22%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 87 / 150 | Val Loss: 2.1606, Val Acc: 47.61%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 88 / 150 | Val Loss: 2.1616, Val Acc: 47.34%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 89 / 150 | Val Loss: 2.1708, Val Acc: 46.81%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 90 / 150 | Val Loss: 2.1535, Val Acc: 47.92%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 91 / 150 | Val Loss: 2.1885, Val Acc: 47.35%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 92 / 150 | Val Loss: 2.2011, Val Acc: 46.74%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 93 / 150 | Val Loss: 2.1640, Val Acc: 47.53%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 94 / 150 | Val Loss: 2.1819, Val Acc: 47.62%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 95 / 150 | Val Loss: 2.1871, Val Acc: 48.04%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.004534552156372922 | Epoch 96: Increasing local steps000 to 64\n",
            "Epoch 96 / 150 | Val Loss: 2.1772, Val Acc: 47.70%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 97 / 150 | Val Loss: 2.2055, Val Acc: 48.07%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 98 / 150 | Val Loss: 2.1907, Val Acc: 47.86%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 99 / 150 | Val Loss: 2.2115, Val Acc: 48.26%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 100 / 150 | Val Loss: 2.1763, Val Acc: 47.77%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 101 / 150 | Val Loss: 2.2047, Val Acc: 47.50%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.004031905752753224 | Epoch 102: Increasing local steps000 to 64\n",
            "Epoch 102 / 150 | Val Loss: 2.1959, Val Acc: 48.08%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: -0.020526530315001082 | Epoch 103: Decreasing local steps222 due to significant oscillation to 32\n",
            "Epoch 103 / 150 | Val Loss: 2.2409, Val Acc: 48.06%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 104 / 150 | Val Loss: 2.2437, Val Acc: 48.32%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 105 / 150 | Val Loss: 2.2438, Val Acc: 47.71%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 106 / 150 | Val Loss: 2.2635, Val Acc: 48.21%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 107 / 150 | Val Loss: 2.2264, Val Acc: 48.41%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 108 / 150 | Val Loss: 2.2704, Val Acc: 47.97%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 109 / 150 | Val Loss: 2.2205, Val Acc: 48.84%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 110 / 150 | Val Loss: 2.2443, Val Acc: 48.17%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 111 / 150 | Val Loss: 2.2499, Val Acc: 48.18%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 112 / 150 | Val Loss: 2.2589, Val Acc: 47.94%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Delta loss: 0.00027885493924422556 | Epoch 113: Increasing local steps000 to 64\n",
            "Epoch 113 / 150 | Val Loss: 2.2583, Val Acc: 48.24%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 114 / 150 | Val Loss: 2.2632, Val Acc: 48.16%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.004074359797010508 | Epoch 115: Increasing local steps000 to 64\n",
            "Epoch 115 / 150 | Val Loss: 2.2540, Val Acc: 48.32%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 116 / 150 | Val Loss: 2.2643, Val Acc: 48.08%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 117 / 150 | Val Loss: 2.2756, Val Acc: 48.02%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0010711633118165927 | Epoch 118: Increasing local steps000 to 64\n",
            "Epoch 118 / 150 | Val Loss: 2.2732, Val Acc: 48.49%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 119 / 150 | Val Loss: 2.2925, Val Acc: 48.65%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.004909923676193453 | Epoch 120: Increasing local steps000 to 64\n",
            "Epoch 120 / 150 | Val Loss: 2.2812, Val Acc: 48.19%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 121 / 150 | Val Loss: 2.3158, Val Acc: 48.19%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 122 / 150 | Val Loss: 2.2784, Val Acc: 48.50%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 123 / 150 | Val Loss: 2.2918, Val Acc: 48.34%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 124 / 150 | Val Loss: 2.2872, Val Acc: 48.83%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 125 / 150 | Val Loss: 2.3155, Val Acc: 48.73%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 126 / 150 | Val Loss: 2.2938, Val Acc: 48.64%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 127 / 150 | Val Loss: 2.2764, Val Acc: 49.48%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 128 / 150 | Val Loss: 2.3131, Val Acc: 48.48%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0017220202568084394 | Epoch 129: Increasing local steps000 to 64\n",
            "Epoch 129 / 150 | Val Loss: 2.3092, Val Acc: 48.43%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.00017680373419891857 | Epoch 130: Increasing local steps000 to 64\n",
            "Epoch 130 / 150 | Val Loss: 2.3088, Val Acc: 48.80%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 131 / 150 | Val Loss: 2.3034, Val Acc: 49.23%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 132 / 150 | Val Loss: 2.3312, Val Acc: 48.95%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0016882057979472856 | Epoch 133: Increasing local steps000 to 64\n",
            "Epoch 133 / 150 | Val Loss: 2.3273, Val Acc: 48.75%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 134 / 150 | Val Loss: 2.3216, Val Acc: 48.28%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 135 / 150 | Val Loss: 2.3337, Val Acc: 49.15%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 136 / 150 | Val Loss: 2.3195, Val Acc: 48.37%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 137 / 150 | Val Loss: 2.3281, Val Acc: 49.35%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 138 / 150 | Val Loss: 2.3463, Val Acc: 48.73%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 139 / 150 | Val Loss: 2.2977, Val Acc: 49.30%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 140 / 150 | Val Loss: 2.3201, Val Acc: 49.01%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 141 / 150 | Val Loss: 2.3304, Val Acc: 48.54%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 142 / 150 | Val Loss: 2.3045, Val Acc: 49.32%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 143 / 150 | Val Loss: 2.3380, Val Acc: 48.60%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 144 / 150 | Val Loss: 2.3322, Val Acc: 48.83%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 145 / 150 | Val Loss: 2.3057, Val Acc: 49.30%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 146 / 150 | Val Loss: 2.3200, Val Acc: 49.23%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 147 / 150 | Val Loss: 2.3295, Val Acc: 49.00%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 148 / 150 | Val Loss: 2.3430, Val Acc: 48.88%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 149 / 150 | Val Loss: 2.3279, Val Acc: 49.02%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 150 / 150 | Val Loss: 2.3280, Val Acc: 49.29%\n",
            "Remain iterations:14918| Epoch: 932\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 1 / 932 | Val Loss: 2.3315, Val Acc: 48.90%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 2 / 932 | Val Loss: 2.3323, Val Acc: 48.91%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 3 / 932 | Val Loss: 2.3353, Val Acc: 48.74%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Epoch 4 / 932 | Val Loss: 2.3363, Val Acc: 48.97%\n",
            "Adaptive_iterations_per_epoch: 39\n",
            "Delta loss: 0.0005014490523538561 | Epoch 5: Increasing local steps000 to 16\n",
            "Epoch 5 / 932 | Val Loss: 2.3351, Val Acc: 49.38%\n",
            "Adaptive_iterations_per_epoch: 19\n",
            "Delta loss: 0.005383472774726626 | Epoch 6: Increasing local steps000 to 32\n",
            "Epoch 6 / 932 | Val Loss: 2.3226, Val Acc: 49.24%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Epoch 7 / 932 | Val Loss: 2.3256, Val Acc: 49.21%\n",
            "Adaptive_iterations_per_epoch: 9\n",
            "Delta loss: 0.001704306502502789 | Epoch 8: Increasing local steps000 to 64\n",
            "Epoch 8 / 932 | Val Loss: 2.3217, Val Acc: 49.27%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 9 / 932 | Val Loss: 2.3411, Val Acc: 48.76%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 10 / 932 | Val Loss: 2.3414, Val Acc: 49.14%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0009700537649796491 | Epoch 11: Increasing local steps000 to 64\n",
            "Epoch 11 / 932 | Val Loss: 2.3392, Val Acc: 49.05%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0087251425832906 | Epoch 12: Increasing local steps000 to 64\n",
            "Epoch 12 / 932 | Val Loss: 2.3188, Val Acc: 49.25%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 13 / 932 | Val Loss: 2.3534, Val Acc: 48.81%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.016280676005366073 | Epoch 14: Increasing local steps000 to 64\n",
            "Epoch 14 / 932 | Val Loss: 2.3151, Val Acc: 49.19%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.0042708509741662196 | Epoch 15: Increasing local steps000 to 64\n",
            "Epoch 15 / 932 | Val Loss: 2.3052, Val Acc: 48.93%\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 16 / 932 | Val Loss: 2.3303, Val Acc: 48.58%\n",
            "Adaptive_iterations_per_epoch: 4\n"
          ]
        }
      ],
      "source": [
        "def split_cifar100(dataset, num_workers):\n",
        "    indices = np.random.permutation(len(dataset))\n",
        "    splits = np.array_split(indices, num_workers)\n",
        "    return [torch.utils.data.Subset(dataset, split) for split in splits]\n",
        "\n",
        "\n",
        "def local_sgd_adaptive_steps(train_dataset, val_loader, test_loader, device, num_workers, ini_local_steps, num_epochs, batch_size, hyperparams):\n",
        "    dataset_size = len(train_dataset)\n",
        "    datasets = split_cifar100(train_dataset, num_workers)\n",
        "    workers = [\n",
        "        torch.utils.data.DataLoader(\n",
        "            datasets[i],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "        for i in range(num_workers)\n",
        "    ]\n",
        "    TOTAL_ITERATIONS = (num_epochs * (dataset_size // batch_size))\n",
        "    total_iterations = (num_epochs * (dataset_size // batch_size)) // (num_workers * ini_local_steps)\n",
        "    iterations_per_epoch = total_iterations // num_epochs\n",
        "    INI_iterations_per_epoch = iterations_per_epoch\n",
        "    remain_iterations = TOTAL_ITERATIONS\n",
        "\n",
        "    global_model = LeNet5().to(device)\n",
        "    local_models = [copy.deepcopy(global_model).to(device) for _ in range(num_workers)]\n",
        "    local_optimizers = [\n",
        "        optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            weight_decay=hyperparams['weight_decay'],\n",
        "            momentum=hyperparams['momentum'],\n",
        "        )\n",
        "        for model in local_models\n",
        "    ]\n",
        "\n",
        "    schedulers = [CosineAnnealingLR(opt, T_max=num_epochs) for opt in local_optimizers]\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    local_weights = [len(datasets[i].indices) / dataset_size for i in range(num_workers)]\n",
        "\n",
        "    val_losses, val_accuracies = [], []\n",
        "    train_losses, train_accuracies = [], []\n",
        "    local_steps = ini_local_steps\n",
        "    worker_iters = [iter(workers[i]) for i in range(num_workers)]\n",
        "\n",
        "    for _ in range(2):\n",
        "\n",
        "      for epoch in range(1, num_epochs + 1):\n",
        "          # Update synchronization steps dynamically based on local steps\n",
        "          iterations_per_epoch = (dataset_size // batch_size) // (num_workers * local_steps)\n",
        "          print(f\"Adaptive_iterations_per_epoch: {iterations_per_epoch}\")\n",
        "          for iteration in range(iterations_per_epoch):\n",
        "              # Local training for local_steps\n",
        "              for worker_id in range(num_workers):\n",
        "                  local_models[worker_id].train()\n",
        "                  for _ in range(local_steps):\n",
        "                      try:\n",
        "                          inputs, labels = next(worker_iters[worker_id])\n",
        "                      except StopIteration:\n",
        "                          worker_iters[worker_id] = iter(workers[worker_id])\n",
        "                          inputs, labels = next(worker_iters[worker_id])\n",
        "\n",
        "                      inputs, labels = inputs.to(device), labels.to(device)\n",
        "                      local_optimizers[worker_id].zero_grad()\n",
        "                      outputs = local_models[worker_id](inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "                      loss.backward()\n",
        "                      local_optimizers[worker_id].step()\n",
        "\n",
        "              # Model averaging after local_steps\n",
        "              with torch.no_grad():\n",
        "                  global_state_dict = global_model.state_dict()\n",
        "                  for key in global_state_dict.keys():\n",
        "                      global_state_dict[key] = torch.sum(\n",
        "                          torch.stack([\n",
        "                              local_weights[i] * local_models[i].state_dict()[key]\n",
        "                              for i in range(num_workers)\n",
        "                          ]),\n",
        "                          dim=0\n",
        "                      )\n",
        "\n",
        "                  global_model.load_state_dict(global_state_dict)\n",
        "                  # Update local models with averaged weights\n",
        "                  for local_model in local_models:\n",
        "                      local_model.load_state_dict(global_state_dict)\n",
        "\n",
        "          # Validation and dynamic adjustment of local steps\n",
        "          val_acc, val_loss = val_model(global_model, val_loader, criterion, device)\n",
        "          val_losses.append(val_loss)\n",
        "          val_accuracies.append(val_acc)\n",
        "          remain_iterations = remain_iterations - (iterations_per_epoch * num_workers * local_steps)\n",
        "\n",
        "          if epoch > 1:\n",
        "              # Adjust delta_min based on epoch range\n",
        "\n",
        "              delta_loss = (val_losses[-2] - val_loss) / val_losses[-2]\n",
        "\n",
        "              if epoch <= num_epochs * 0.2:\n",
        "                  delta_min = hyperparams['delta_min_initial']\n",
        "              elif epoch <= num_epochs * 0.8:\n",
        "                  delta_min = hyperparams['delta_min_mid']\n",
        "              else:\n",
        "                  delta_min = hyperparams['delta_min_final']\n",
        "\n",
        "\n",
        "\n",
        "              if 0 < delta_loss < delta_min:  # Slow convergence\n",
        "                  local_steps = min(local_steps * 2, hyperparams['max_local_steps'])\n",
        "                  print(f\"Delta loss: {delta_loss} | Epoch {epoch}: Increasing local steps000 to {local_steps}\")\n",
        "\n",
        "              # elif delta_loss < 0 and abs(delta_loss) < delta_min * 2:  # Oscillation or divergence\n",
        "              #     local_steps = max(local_steps // 2, hyperparams['min_local_steps'])\n",
        "              #     print(f\"Delta loss: {delta_loss} | Epoch {epoch}: Decreasing local steps111 to {local_steps}\")\n",
        "              # significant oscillations after 30% of epochs\n",
        "              if epoch > num_epochs * 0.3:\n",
        "                if delta_loss < 0 and abs(delta_loss) > hyperparams['delta_min_initial']:  # Significant oscillation\n",
        "                    local_steps = max(local_steps // 2, hyperparams['min_local_steps'])\n",
        "                    print(f\"Delta loss: {delta_loss} | Epoch {epoch}: Decreasing local steps222 due to significant oscillation to {local_steps}\")\n",
        "\n",
        "              # # Gradient norm monitoring\n",
        "              # if epoch > num_epochs * 0.3:\n",
        "              #   with torch.no_grad():\n",
        "              #       gradients = []\n",
        "              #       for worker_id in range(num_workers):\n",
        "              #           for param in local_models[worker_id].parameters():\n",
        "              #               if param.grad is not None:\n",
        "              #                   gradients.append(param.grad.norm().item())\n",
        "              #       if gradients:\n",
        "              #           grad_norm = max(gradients) - min(gradients)\n",
        "              #           print(f\"Epoch {epoch}: Gradient norm fluctuation = {grad_norm}\")\n",
        "              #           if grad_norm > hyperparams['grad_norm_threshold']:  # Severe fluctuations\n",
        "              #               local_steps = ini_local_steps\n",
        "              #               print(f\"Epoch {epoch}: Resetting local steps to {ini_local_steps} due to gradient norm fluctuations\")\n",
        "\n",
        "\n",
        "          # Log training and validation metrics\n",
        "          print(f\"Epoch {epoch} / {num_epochs} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc * 100:.2f}%\")\n",
        "          # Step schedulers after each epoch\n",
        "          for scheduler in schedulers:\n",
        "              scheduler.step()\n",
        "      # Calculate how many iterations we still have, and keep training\n",
        "      num_epochs = remain_iterations // (num_workers * ini_local_steps * INI_iterations_per_epoch)\n",
        "      print(f\"Remain iterations:{remain_iterations}| Epoch: {num_epochs}\")\n",
        "      local_steps = ini_local_steps\n",
        "\n",
        "    # Test the final model\n",
        "    test_acc = test_model(global_model, test_loader, device)\n",
        "\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies, test_acc, TOTAL_ITERATIONS, INI_iterations_per_epoch\n",
        "\n",
        "\n",
        "\n",
        "def val_model(global_model, val_loader, criterion, device):\n",
        "    global_model.eval()\n",
        "    val_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    return val_acc, val_loss\n",
        "\n",
        "def test_model(global_model, test_loader, device):\n",
        "    global_model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "\n",
        "# Visualization function\n",
        "def plot_results(train_losses, val_losses, train_accuracies, val_accuracies, iterations_per_epoch, total_iterations):\n",
        "    # Dynamic calculation record points\n",
        "    recorded_iterations = len(train_losses)\n",
        "    iterations = range(iterations_per_epoch, iterations_per_epoch * recorded_iterations + 1, iterations_per_epoch)\n",
        "\n",
        "    # Ensure data length is consistent\n",
        "    assert len(train_losses) == len(iterations), f\"Train losses and iterations mismatch: {len(train_losses)} vs {len(iterations)}\"\n",
        "    assert len(val_losses) == len(iterations), f\"Validation losses and iterations mismatch: {len(val_losses)} vs {len(iterations)}\"\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(iterations, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(iterations, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(iterations, train_accuracies, label=\"Train Accuracy\")\n",
        "    plt.plot(iterations, val_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"Training and Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    num_epochs = NUM_EPOCHS\n",
        "    batch_size = BATCH_SIZE\n",
        "    num_workers_list = NUM_WORKER_LIST\n",
        "    local_steps_list = LOCAL_STEPS\n",
        "    hyperparams = {\n",
        "        'lr': LR,\n",
        "        'weight_decay': WD,\n",
        "        'momentum': MOMENTUM,\n",
        "        'delta_min_initial': 0.02,\n",
        "        'delta_min_mid' : 0.005,\n",
        "        'delta_min_final': 0.002,\n",
        "        # 'grad_norm_threshold': 1.0,\n",
        "        'max_local_steps': 64,\n",
        "        'min_local_steps': 4\n",
        "    }\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_dataset(batch_size)\n",
        "    train_dataset = train_loader.dataset\n",
        "\n",
        "    for num_workers in num_workers_list:\n",
        "        for ini_local_steps in local_steps_list:\n",
        "            print(f\"In the beginning running LocalSGD with {num_workers} workers and {ini_local_steps} initial local steps\")\n",
        "            train_losses, val_losses, train_accuracies, val_accuracies, test_acc,  TOTAL_ITERATIONS, INI_iterations_per_epoch = local_sgd_adaptive_steps(\n",
        "                train_dataset=train_dataset,\n",
        "                val_loader=val_loader,\n",
        "                test_loader=test_loader,\n",
        "                device=device,\n",
        "                num_workers=num_workers,\n",
        "                ini_local_steps=ini_local_steps,\n",
        "                num_epochs=num_epochs,\n",
        "                batch_size=batch_size,\n",
        "                hyperparams=hyperparams,\n",
        "            )\n",
        "\n",
        "            plot_results(train_losses, val_losses, train_accuracies, val_accuracies, TOTAL_ITERATIONS, INI_iterations_per_epoch)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bNvH-6hEsuxY"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}