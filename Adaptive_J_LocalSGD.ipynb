{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheS1n233/Distributed-Learning-Project5/blob/experiments/Adaptive_J_LocalSGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KlHheqAGVUVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a20e6f-0a3d-42d0-8d88-7fe52651048d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2T4ia3AsnT6"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GPmLs0QzJQrO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import argparse\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, SequentialLR, LinearLR\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1yoNOiteP6yy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1759c9f6-c619-472a-bbf6-a26ecbf4af46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-sEx0f8JJxK"
      },
      "source": [
        "# Costants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Pmn-hp0TJNzX"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 150\n",
        "BATCH_SIZE = 64\n",
        "NUM_WORKER_LIST = [2]\n",
        "LOCAL_STEPS = [8]\n",
        "LR = 0.01\n",
        "WD = 0.001\n",
        "MOMENTUM = 0.9\n",
        "ALPHA = 1.0\n",
        "BETA = 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNvH-6hEsuxY"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "da7YHCPoKnNK"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8oCJ42etBK_"
      },
      "source": [
        "## Function to get train, test and val dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OqrDqFDIJbRw"
      },
      "outputs": [],
      "source": [
        "def calulcate_mean_std(batch_size=100, verbose=True):\n",
        "    # Transform only for caluclate meaning of the dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-100 training dataset\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Initialize sums for calculating mean and std\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "\n",
        "    for images, _ in train_loader:\n",
        "        # Compute mean and std for each channel\n",
        "        mean += images.mean(dim=[0, 2, 3])\n",
        "        std += images.std(dim=[0, 2, 3])\n",
        "\n",
        "    mean /= len(train_loader)\n",
        "    std /= len(train_loader)\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Mean: \", mean)\n",
        "      print(\"Std: \", std)\n",
        "\n",
        "    return mean, std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sMQZou82OL3K"
      },
      "outputs": [],
      "source": [
        "def get_dataset(batch_size, verbose=True):\n",
        "\n",
        "    print(\"Start loading data with batch_size\", batch_size)\n",
        "\n",
        "    mean, std = calulcate_mean_std()\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.CenterCrop(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-100 dataset\n",
        "    start_time = time.time()\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "      print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Split training and validation sets\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "      for i, (inputs, labels) in enumerate(train_loader):\n",
        "          print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "          if i == 10:\n",
        "              break\n",
        "      print(f\"Data loading for 10 batches completed.\")\n",
        "      print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "      print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "      print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "\n",
        "    print(\"Data load correctly...\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5BLezwNAJfva"
      },
      "outputs": [],
      "source": [
        "class CheckpointSaver:\n",
        "\n",
        "  def __init__(self, path, additional_info, k, j, hyperparams, epochs):\n",
        "    self.timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    self.path = f\"{path}/K={k}_J={j}_Comparison_{self.timestamp}\"\n",
        "    self.additional_info = additional_info\n",
        "    self.k = k\n",
        "    self.j = j\n",
        "    self.hyperparams = hyperparams\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def create_files(self):\n",
        "    os.makedirs(self.path, exist_ok=True)\n",
        "\n",
        "    self.metrics_files = {\n",
        "        'global_train_acc': os.path.join(self.path, f'slowmo_train_accuracy_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'val_acc': os.path.join(self.path, f'slowmo_val_accuracy_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'global_train_loss': os.path.join(self.path, f'slowmo_train_loss_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'val_loss': os.path.join(self.path, f'slowmo_val_loss_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "        'summary': os.path.join(self.path, f'slowmo_summary_{self.timestamp}_{str(self.additional_info)}.txt'),\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(self.path, f'slowmo_experiment_config_{self.timestamp}_{str(self.additional_info)}.txt'), 'w') as f:\n",
        "        f.write(f\"Experiment Configuration:\\n\")\n",
        "        f.write(f\"LocalSGD\\n\")\n",
        "        f.write(f\"K = {self.k}; J = {self.j}\\n\")\n",
        "        f.write(f\"Hyperparameters: {str(self.hyperparams)}\\n\")\n",
        "        f.write(f\"Number of epochs: {self.epochs}\\n\")\n",
        "        f.write(f\"Timestamp: {self.timestamp}\\n\")\n",
        "\n",
        "  def save_sumamry(self, epoch, global_train_accuracy, val_acc, global_train_loss, val_loss):\n",
        "      try:\n",
        "          with open(self.metrics_files['summary'], 'a') as f:\n",
        "            f.write(f\"Epoch {epoch}\\n\")\n",
        "            f.write(f\"Train Loss: {global_train_loss:.4f}, Train Acc: {global_train_accuracy * 100:.2f}%\\n\")\n",
        "            f.write(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc * 100:.2f}%\\n\")\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n",
        "  def save_checkpoint(self, epoch, global_train_accuracy, val_acc, global_train_loss, val_loss):\n",
        "      try:\n",
        "          with open(self.metrics_files['global_train_acc'], 'a') as f:\n",
        "              f.write(f\"{epoch},{global_train_accuracy * 100:.2f}%\\n\")\n",
        "          with open(self.metrics_files['val_acc'], 'a') as f:\n",
        "              f.write(f\"{epoch},{val_acc * 100:.2f}%\\n\")\n",
        "          with open(self.metrics_files['global_train_loss'], 'a') as f:\n",
        "              f.write(f\"{epoch},{global_train_loss:.4f}\\n\")\n",
        "          with open(self.metrics_files['val_loss'], 'a') as f:\n",
        "              f.write(f\"{epoch},{val_loss:.4f}\\n\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n",
        "  def end_sumamry(self, test_acc):\n",
        "      try:\n",
        "          with open(self.metrics_files['summary'], 'a') as f:\n",
        "            f.write(f\"test_acc {test_acc}\\n\")\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error saving metrics: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJaPDN4Qna1P"
      },
      "source": [
        "# LocalSGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SFiaXpZUnfDQ",
        "outputId": "24b0ce9c-7e49-4454-d9f1-74892447dbbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start loading data with batch_size 64\n",
            "Files already downloaded and verified\n",
            "Mean:  tensor([0.5071, 0.4865, 0.4409])\n",
            "Std:  tensor([0.2667, 0.2558, 0.2754])\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset loading time: 1.74 seconds\n",
            "Batch 0: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 1: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 2: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 3: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 4: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 5: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 6: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 7: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 8: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 9: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 10: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Data loading for 10 batches completed.\n",
            "Training dataset size: 40000\n",
            "Validation dataset size: 10000\n",
            "Test dataset size: 10000\n",
            "Data load correctly...\n",
            "In the beginning running LocalSGD with 2 workers and 64 initial local steps\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 1 / 15 | Val Loss: 4.1252, Val Acc: 5.76%\n",
            "Epoch 1, Worker 0: Current Learning Rate = 0.009891\n",
            "Epoch 1, Worker 1: Current Learning Rate = 0.009891\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 2 / 15 | Val Loss: 3.8587, Val Acc: 10.39%\n",
            "Epoch 2, Worker 0: Current Learning Rate = 0.009568\n",
            "Epoch 2, Worker 1: Current Learning Rate = 0.009568\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 3 / 15 | Val Loss: 3.6786, Val Acc: 13.77%\n",
            "Epoch 3, Worker 0: Current Learning Rate = 0.009045\n",
            "Epoch 3, Worker 1: Current Learning Rate = 0.009045\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 4 / 15 | Val Loss: 3.5351, Val Acc: 16.25%\n",
            "Epoch 4, Worker 0: Current Learning Rate = 0.008346\n",
            "Epoch 4, Worker 1: Current Learning Rate = 0.008346\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 5 / 15 | Val Loss: 3.3760, Val Acc: 18.61%\n",
            "Epoch 5, Worker 0: Current Learning Rate = 0.007500\n",
            "Epoch 5, Worker 1: Current Learning Rate = 0.007500\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 6 / 15 | Val Loss: 3.2896, Val Acc: 19.60%\n",
            "Epoch 6, Worker 0: Current Learning Rate = 0.006545\n",
            "Epoch 6, Worker 1: Current Learning Rate = 0.006545\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 7 / 15 | Val Loss: 3.1603, Val Acc: 22.84%\n",
            "Epoch 7, Worker 0: Current Learning Rate = 0.005523\n",
            "Epoch 7, Worker 1: Current Learning Rate = 0.005523\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 8 / 15 | Val Loss: 3.0725, Val Acc: 24.52%\n",
            "Epoch 8, Worker 0: Current Learning Rate = 0.004477\n",
            "Epoch 8, Worker 1: Current Learning Rate = 0.004477\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 9 / 15 | Val Loss: 3.0022, Val Acc: 26.20%\n",
            "Epoch 9, Worker 0: Current Learning Rate = 0.003455\n",
            "Epoch 9, Worker 1: Current Learning Rate = 0.003455\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 10 / 15 | Val Loss: 2.9640, Val Acc: 26.74%\n",
            "Epoch 10, Worker 0: Current Learning Rate = 0.002500\n",
            "Epoch 10, Worker 1: Current Learning Rate = 0.002500\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 11 / 15 | Val Loss: 2.9155, Val Acc: 27.41%\n",
            "Epoch 11, Worker 0: Current Learning Rate = 0.001654\n",
            "Epoch 11, Worker 1: Current Learning Rate = 0.001654\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 12 / 15 | Val Loss: 2.8718, Val Acc: 28.58%\n",
            "Epoch 12, Worker 0: Current Learning Rate = 0.000955\n",
            "Epoch 12, Worker 1: Current Learning Rate = 0.000955\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 13 / 15 | Val Loss: 2.8491, Val Acc: 28.79%\n",
            "Epoch 13, Worker 0: Current Learning Rate = 0.000432\n",
            "Epoch 13, Worker 1: Current Learning Rate = 0.000432\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 14 / 15 | Val Loss: 2.8221, Val Acc: 29.76%\n",
            "Epoch 14, Worker 0: Current Learning Rate = 0.000109\n",
            "Epoch 14, Worker 1: Current Learning Rate = 0.000109\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 15 / 15 | Val Loss: 2.8111, Val Acc: 30.55%\n",
            "Epoch 15, Worker 0: Current Learning Rate = 0.000000\n",
            "Epoch 15, Worker 1: Current Learning Rate = 0.000000\n",
            "Remain iterations:1695| Remain Epoch: 3\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 1 / 3 | Val Loss: 2.8130, Val Acc: 30.17%\n",
            "Epoch 1, Worker 0: Current Learning Rate = 0.000077\n",
            "Epoch 1, Worker 1: Current Learning Rate = 0.000077\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Delta loss: 0.001643990620300646 | Epoch 2: Increasing local steps000 to 64\n",
            "Epoch 2 / 3 | Val Loss: 2.8084, Val Acc: 30.00%\n",
            "Epoch 2, Worker 0: Current Learning Rate = 0.000033\n",
            "Epoch 2, Worker 1: Current Learning Rate = 0.000033\n",
            "Adaptive_iterations_per_epoch: 4\n",
            "Epoch 3 / 3 | Val Loss: 2.8196, Val Acc: 30.04%\n",
            "Epoch 3, Worker 0: Current Learning Rate = 0.000010\n",
            "Epoch 3, Worker 1: Current Learning Rate = 0.000010\n",
            "Remain iterations:159| Remain Epoch: 1\n",
            "Test Accuracy: 32.84%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Validation losses and iterations mismatch: 18 vs 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-60a98a48b321>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    267\u001b[0m             )\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m             \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTOTAL_ITERATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINI_iterations_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-60a98a48b321>\u001b[0m in \u001b[0;36mplot_results\u001b[0;34m(train_losses, val_losses, train_accuracies, val_accuracies, iterations_per_epoch, total_iterations)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m# Ensure data length is consistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Train losses and iterations mismatch: {len(train_losses)} vs {len(iterations)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Validation losses and iterations mismatch: {len(val_losses)} vs {len(iterations)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Validation losses and iterations mismatch: 18 vs 0"
          ]
        }
      ],
      "source": [
        "def split_cifar100(dataset, num_workers):\n",
        "    indices = np.random.permutation(len(dataset))\n",
        "    splits = np.array_split(indices, num_workers)\n",
        "    return [torch.utils.data.Subset(dataset, split) for split in splits]\n",
        "\n",
        "\n",
        "def local_sgd_adaptive_steps(train_dataset, val_loader, test_loader, device, num_workers, ini_local_steps, num_epochs, batch_size, hyperparams):\n",
        "    dataset_size = len(train_dataset)\n",
        "    datasets = split_cifar100(train_dataset, num_workers)\n",
        "    workers = [\n",
        "        torch.utils.data.DataLoader(\n",
        "            datasets[i],\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "        for i in range(num_workers)\n",
        "    ]\n",
        "    TOTAL_ITERATIONS = (num_epochs * (dataset_size // batch_size))\n",
        "    total_iterations = (num_epochs * (dataset_size // batch_size)) // (num_workers * ini_local_steps)\n",
        "    iterations_per_epoch = total_iterations // num_epochs\n",
        "    INI_iterations_per_epoch = iterations_per_epoch\n",
        "    remain_iterations = TOTAL_ITERATIONS\n",
        "\n",
        "    global_model = LeNet5().to(device)\n",
        "    local_models = [copy.deepcopy(global_model).to(device) for _ in range(num_workers)]\n",
        "    local_optimizers = [\n",
        "        optim.SGD(\n",
        "            model.parameters(),\n",
        "            lr=hyperparams['lr'],\n",
        "            weight_decay=hyperparams['weight_decay'],\n",
        "            momentum=hyperparams['momentum'],\n",
        "        )\n",
        "        for model in local_models\n",
        "    ]\n",
        "\n",
        "    schedulers = [CosineAnnealingLR(opt, T_max=num_epochs) for opt in local_optimizers]\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    local_weights = [len(datasets[i].indices) / dataset_size for i in range(num_workers)]\n",
        "\n",
        "    val_losses, val_accuracies = [], []\n",
        "    train_losses, train_accuracies = [], []\n",
        "    local_steps = ini_local_steps\n",
        "    worker_iters = [iter(workers[i]) for i in range(num_workers)]\n",
        "\n",
        "    for _ in range(2):\n",
        "\n",
        "      for epoch in range(1, num_epochs + 1):\n",
        "          # Update synchronization steps dynamically based on local steps\n",
        "          iterations_per_epoch = (dataset_size // batch_size) // (num_workers * local_steps)\n",
        "          print(f\"Adaptive_iterations_per_epoch: {iterations_per_epoch}\")\n",
        "          for iteration in range(iterations_per_epoch):\n",
        "              # Local training for local_steps\n",
        "              for worker_id in range(num_workers):\n",
        "                  local_models[worker_id].train()\n",
        "                  for _ in range(local_steps):\n",
        "                      try:\n",
        "                          inputs, labels = next(worker_iters[worker_id])\n",
        "                      except StopIteration:\n",
        "                          worker_iters[worker_id] = iter(workers[worker_id])\n",
        "                          inputs, labels = next(worker_iters[worker_id])\n",
        "\n",
        "                      inputs, labels = inputs.to(device), labels.to(device)\n",
        "                      local_optimizers[worker_id].zero_grad()\n",
        "                      outputs = local_models[worker_id](inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "                      loss.backward()\n",
        "                      local_optimizers[worker_id].step()\n",
        "\n",
        "              # Model averaging after local_steps\n",
        "              with torch.no_grad():\n",
        "                  global_state_dict = global_model.state_dict()\n",
        "                  for key in global_state_dict.keys():\n",
        "                      global_state_dict[key] = torch.sum(\n",
        "                          torch.stack([\n",
        "                              local_weights[i] * local_models[i].state_dict()[key]\n",
        "                              for i in range(num_workers)\n",
        "                          ]),\n",
        "                          dim=0\n",
        "                      )\n",
        "\n",
        "                  global_model.load_state_dict(global_state_dict)\n",
        "                  # Update local models with averaged weights\n",
        "                  for local_model in local_models:\n",
        "                      local_model.load_state_dict(global_state_dict)\n",
        "\n",
        "          # Validation and dynamic adjustment of local steps\n",
        "          val_acc, val_loss = val_model(global_model, val_loader, criterion, device)\n",
        "          val_losses.append(val_loss)\n",
        "          val_accuracies.append(val_acc)\n",
        "          remain_iterations = remain_iterations - (iterations_per_epoch * num_workers * local_steps)\n",
        "\n",
        "          if epoch > 1:\n",
        "              # Adjust delta_min based on epoch range\n",
        "\n",
        "              delta_loss = (val_losses[-2] - val_loss) / val_losses[-2]\n",
        "\n",
        "              if epoch <= num_epochs * 0.2:\n",
        "                  delta_min = hyperparams['delta_min_initial']\n",
        "              elif epoch <= num_epochs * 0.8:\n",
        "                  delta_min = hyperparams['delta_min_mid']\n",
        "              else:\n",
        "                  delta_min = hyperparams['delta_min_final']\n",
        "\n",
        "\n",
        "\n",
        "              if 0 < delta_loss < delta_min:  # Slow convergence\n",
        "                  local_steps = min(local_steps * 2, hyperparams['max_local_steps'])\n",
        "                  print(f\"Delta loss: {delta_loss} | Epoch {epoch}: Increasing local steps000 to {local_steps}\")\n",
        "\n",
        "              # elif delta_loss < 0 and abs(delta_loss) < delta_min * 2:  # Oscillation or divergence\n",
        "              #     local_steps = max(local_steps // 2, hyperparams['min_local_steps'])\n",
        "              #     print(f\"Delta loss: {delta_loss} | Epoch {epoch}: Decreasing local steps111 to {local_steps}\")\n",
        "              # significant oscillations after 30% of epochs\n",
        "              if epoch > num_epochs * 0.3:\n",
        "                if delta_loss < 0 and abs(delta_loss) > hyperparams['delta_min_initial']:  # Significant oscillation\n",
        "                    local_steps = max(local_steps // 2, hyperparams['min_local_steps'])\n",
        "                    print(f\"Delta loss: {delta_loss} | Epoch {epoch}: Decreasing local steps222 due to significant oscillation to {local_steps}\")\n",
        "\n",
        "              # # Gradient norm monitoring\n",
        "              # if epoch > num_epochs * 0.3:\n",
        "              #   with torch.no_grad():\n",
        "              #       gradients = []\n",
        "              #       for worker_id in range(num_workers):\n",
        "              #           for param in local_models[worker_id].parameters():\n",
        "              #               if param.grad is not None:\n",
        "              #                   gradients.append(param.grad.norm().item())\n",
        "              #       if gradients:\n",
        "              #           grad_norm = max(gradients) - min(gradients)\n",
        "              #           print(f\"Epoch {epoch}: Gradient norm fluctuation = {grad_norm}\")\n",
        "              #           if grad_norm > hyperparams['grad_norm_threshold']:  # Severe fluctuations\n",
        "              #               local_steps = ini_local_steps\n",
        "              #               print(f\"Epoch {epoch}: Resetting local steps to {ini_local_steps} due to gradient norm fluctuations\")\n",
        "\n",
        "\n",
        "          # Log training and validation metrics\n",
        "          print(f\"Epoch {epoch} / {num_epochs} | Val Loss: {val_loss:.4f}, Val Acc: {val_acc * 100:.2f}%\")\n",
        "          # Step schedulers after each epoch\n",
        "          for scheduler in schedulers:\n",
        "              scheduler.step()\n",
        "          for worker_id, optimizer in enumerate(local_optimizers):\n",
        "              current_lr = optimizer.param_groups[0]['lr']\n",
        "              print(f\"Epoch {epoch}, Worker {worker_id}: Current Learning Rate = {current_lr:.6f}\")\n",
        "      # Calculate how many iterations we still have, and keep training\n",
        "      num_epochs = max(1, remain_iterations // (num_workers * ini_local_steps * INI_iterations_per_epoch))\n",
        "      print(f\"Remain iterations:{remain_iterations}| Remain Epoch: {num_epochs}\")\n",
        "      local_steps = ini_local_steps\n",
        "      last_lr = 0.0001 #initial 2nd loop learning rate\n",
        "      for optimizer in local_optimizers:\n",
        "        for param_group in optimizer.param_groups:\n",
        "          param_group['lr'] = last_lr\n",
        "      new_schedulers = [\n",
        "        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=last_lr * 0.1)\n",
        "        for optimizer in local_optimizers\n",
        "      ]\n",
        "      schedulers = new_schedulers\n",
        "    # Test the final model\n",
        "    test_acc = test_model(global_model, test_loader, device)\n",
        "\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies, test_acc, TOTAL_ITERATIONS, INI_iterations_per_epoch\n",
        "\n",
        "\n",
        "\n",
        "def val_model(global_model, val_loader, criterion, device):\n",
        "    global_model.eval()\n",
        "    val_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    return val_acc, val_loss\n",
        "\n",
        "def test_model(global_model, test_loader, device):\n",
        "    global_model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = global_model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    test_acc = correct / total\n",
        "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "\n",
        "# Visualization function\n",
        "def plot_results(train_losses, val_losses, train_accuracies, val_accuracies, iterations_per_epoch, total_iterations):\n",
        "    # Dynamic calculation record points\n",
        "    recorded_iterations = len(train_losses)\n",
        "    iterations = range(iterations_per_epoch, iterations_per_epoch * recorded_iterations + 1, iterations_per_epoch)\n",
        "\n",
        "    # Ensure data length is consistent\n",
        "    assert len(train_losses) == len(iterations), f\"Train losses and iterations mismatch: {len(train_losses)} vs {len(iterations)}\"\n",
        "    assert len(val_losses) == len(iterations), f\"Validation losses and iterations mismatch: {len(val_losses)} vs {len(iterations)}\"\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(iterations, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(iterations, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(iterations, train_accuracies, label=\"Train Accuracy\")\n",
        "    plt.plot(iterations, val_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"Training and Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    num_epochs = NUM_EPOCHS\n",
        "    batch_size = BATCH_SIZE\n",
        "    num_workers_list = NUM_WORKER_LIST\n",
        "    local_steps_list = LOCAL_STEPS\n",
        "    hyperparams = {\n",
        "        'lr': LR,\n",
        "        'weight_decay': WD,\n",
        "        'momentum': MOMENTUM,\n",
        "        'delta_min_initial': 0.02,\n",
        "        'delta_min_mid' : 0.005,\n",
        "        'delta_min_final': 0.002,\n",
        "        # 'grad_norm_threshold': 1.0,\n",
        "        'max_local_steps': 64,\n",
        "        'min_local_steps': 4\n",
        "    }\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_dataset(batch_size)\n",
        "    train_dataset = train_loader.dataset\n",
        "\n",
        "    for num_workers in num_workers_list:\n",
        "        for ini_local_steps in local_steps_list:\n",
        "            print(f\"In the beginning running LocalSGD with {num_workers} workers and {ini_local_steps} initial local steps\")\n",
        "            train_losses, val_losses, train_accuracies, val_accuracies, test_acc,  TOTAL_ITERATIONS, INI_iterations_per_epoch = local_sgd_adaptive_steps(\n",
        "                train_dataset=train_dataset,\n",
        "                val_loader=val_loader,\n",
        "                test_loader=test_loader,\n",
        "                device=device,\n",
        "                num_workers=num_workers,\n",
        "                ini_local_steps=ini_local_steps,\n",
        "                num_epochs=num_epochs,\n",
        "                batch_size=batch_size,\n",
        "                hyperparams=hyperparams,\n",
        "            )\n",
        "\n",
        "            plot_results(train_losses, val_losses, train_accuracies, val_accuracies, TOTAL_ITERATIONS, INI_iterations_per_epoch)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "bNvH-6hEsuxY"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}