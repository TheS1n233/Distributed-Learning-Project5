{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheS1n233/Distributed-Learning-Project5/blob/experiments/other_optimezers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlHheqAGVUVY",
        "outputId": "a5ab9a25-8895-4891-b7ca-3c9337d33f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2T4ia3AsnT6"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GPmLs0QzJQrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0eff7f-3994-4e93-8c2b-0a6dcbfa55c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import argparse\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, SequentialLR, LinearLR\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time\n",
        "from torch.amp import GradScaler, autocast\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    raise RuntimeError(\"Google Drive not mounted correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNvH-6hEsuxY"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "da7YHCPoKnNK"
      },
      "outputs": [],
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5)\n",
        "\n",
        "        # Layer fully connected\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 384)\n",
        "        self.fc2 = nn.Linear(384, 192)\n",
        "        self.fc3 = nn.Linear(192, 100)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer convolutivi con ReLU e max-pooling\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Flatten per i layer fully connected\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Layer fully connected con ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_8hLuI1syee"
      },
      "source": [
        "## Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HgFe_H-kNzFM"
      },
      "outputs": [],
      "source": [
        "# Early Stopping Class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=10, delta=0, path='/content/drive/MyDrive/Early2checkpoint.pt', verbose=False):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, optimizer=None):\n",
        "        if self.verbose:\n",
        "            print(f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...\")\n",
        "        checkpoint = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict() if optimizer else None,\n",
        "            'val_loss_min': val_loss\n",
        "        }\n",
        "        torch.save(checkpoint, self.path)\n",
        "        self.val_loss_min = val_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8oCJ42etBK_"
      },
      "source": [
        "## Function to get train, test and val dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sMQZou82OL3K"
      },
      "outputs": [],
      "source": [
        "def get_dataset(batch_size):\n",
        "    batch_size=batch_size\n",
        "    print(\"batch_size\", batch_size)\n",
        "    # Define the transform to only convert the images to tensors (without normalization yet)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-100 training dataset\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Initialize sums for calculating mean and std\n",
        "    mean = torch.zeros(3)\n",
        "    std = torch.zeros(3)\n",
        "\n",
        "    for images, _ in train_loader:\n",
        "        # Compute mean and std for each channel\n",
        "        mean += images.mean(dim=[0, 2, 3])\n",
        "        std += images.std(dim=[0, 2, 3])\n",
        "\n",
        "    mean /= len(train_loader)\n",
        "    std /= len(train_loader)\n",
        "\n",
        "    print(\"Mean: \", mean)\n",
        "    print(\"Std: \", std)\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[mean[0].item(), mean[1].item(), mean[2].item()],\n",
        "                            std=[std[0].item(), std[1].item(), std[2].item()])\n",
        "    ])\n",
        "\n",
        "    # Load CIFAR-100 dataset\n",
        "    start_time = time.time()\n",
        "    train_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR100(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform_test\n",
        "    )\n",
        "    print(f\"Dataset loading time: {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    # Split training and validation sets\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Debugging: Check DataLoader outputs\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        print(f\"Batch {i}: inputs shape: {inputs.shape}, labels shape: {labels.shape}\")\n",
        "        if i == 10:  # Test first 10 batches\n",
        "            break\n",
        "    print(f\"Data loading for 10 batches completed.\")\n",
        "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJaPDN4Qna1P"
      },
      "source": [
        "# LocalSGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFiaXpZUnfDQ",
        "outputId": "6c37b6b2-34a2-451f-9e2e-ead461d44ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size 64\n",
            "Files already downloaded and verified\n",
            "Mean:  tensor([0.5070, 0.4865, 0.4408])\n",
            "Std:  tensor([0.2664, 0.2555, 0.2750])\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset loading time: 1.73 seconds\n",
            "Batch 0: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 1: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 2: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 3: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 4: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 5: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 6: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 7: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 8: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 9: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Batch 10: inputs shape: torch.Size([64, 3, 32, 32]), labels shape: torch.Size([64])\n",
            "Data loading for 10 batches completed.\n",
            "Training dataset size: 40000\n",
            "Validation dataset size: 10000\n",
            "Test dataset size: 10000\n",
            "Running LocalSGD with 2 workers and 4 local steps\n",
            "Iteration 1/11718\n",
            "Iteration 2/11718\n",
            "Iteration 3/11718\n",
            "Iteration 4/11718\n",
            "Iteration 5/11718\n",
            "Iteration 6/11718\n",
            "Iteration 7/11718\n",
            "Iteration 8/11718\n",
            "Iteration 9/11718\n",
            "Iteration 10/11718\n",
            "Iteration 11/11718\n",
            "Iteration 12/11718\n",
            "Iteration 13/11718\n",
            "Iteration 14/11718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 15/11718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "def vector_to_parameters(vector, parameters):\n",
        "    pointer = 0\n",
        "    for param in parameters:\n",
        "        # The length of the current parameter as a flat vector\n",
        "        num_param = param.numel()\n",
        "\n",
        "        # Reshape the vector segment into the shape of the current parameter\n",
        "        param.data = vector[pointer:pointer + num_param].view_as(param.data)\n",
        "\n",
        "        # Move pointer ahead\n",
        "        pointer += num_param\n",
        "\n",
        "\n",
        "def local_sgd(train_dataset, val_loader, test_loader, device, num_workers, local_steps, num_epoch, batch_size, hyperparams, validation_interval, slowmo_params):\n",
        "\n",
        "      dataset_size = len(train_dataset)\n",
        "      shard_size = dataset_size // num_workers\n",
        "      datasets = [\n",
        "          torch.utils.data.Subset(train_dataset, range(i * shard_size, (i + 1) * shard_size))\n",
        "          for i in range(num_workers)\n",
        "      ]\n",
        "      workers = [\n",
        "          DataLoader(datasets[i], batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
        "          for i in range(num_workers)\n",
        "      ]\n",
        "\n",
        "      # Calcoliamo il numero di iterazioni totali\n",
        "      total_iterations = (num_epoch * (dataset_size // batch_size)) // (num_workers * local_steps)\n",
        "\n",
        "      # Inizializziamo il modello globale e l'ottimizzatore\n",
        "      global_model = LeNet5().to(device)\n",
        "      global_optimizer = optim.SGD(global_model.parameters(),\n",
        "                                  lr=hyperparams['lr'],\n",
        "                                  weight_decay=hyperparams['weight_decay'],\n",
        "                                  momentum=hyperparams['momentum'])\n",
        "\n",
        "      # Media dei parametri globali\n",
        "      averaged_params = torch.nn.utils.parameters_to_vector(global_model.parameters()).clone()\n",
        "\n",
        "      # Momentum lento\n",
        "      slow_momentum = torch.zeros_like(averaged_params)\n",
        "\n",
        "      # Scheduler per il learning rate\n",
        "      scheduler = CosineAnnealingLR(global_optimizer, T_max=num_epoch)\n",
        "\n",
        "      # Funzione di perdita\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      # Monitoraggio delle metriche\n",
        "      train_losses = []\n",
        "      train_accuracies = []\n",
        "      val_losses = []\n",
        "      val_accuracies = []\n",
        "\n",
        "      for iteration in range(1, total_iterations + 1):\n",
        "\n",
        "          # Salvataggio dei parametri iniziali per calcolare il momentum lento\n",
        "          initial_params = torch.nn.utils.parameters_to_vector(global_model.parameters()).clone()\n",
        "\n",
        "          print(f\"Iteration {iteration}/{total_iterations}\")\n",
        "\n",
        "          # Aggiornamento locale\n",
        "          local_models = []\n",
        "          worker_iters = [iter(workers[i]) for i in range(num_workers)]\n",
        "          worker_train_losses = []\n",
        "          worker_train_accuracies = []\n",
        "\n",
        "          for worker_id in range(num_workers):\n",
        "              local_model = copy.deepcopy(global_model).to(device)\n",
        "              local_optimizer = optim.SGD(local_model.parameters(),\n",
        "                                          lr=hyperparams['lr'],\n",
        "                                          weight_decay=hyperparams['weight_decay'],\n",
        "                                          momentum=hyperparams['momentum'])\n",
        "\n",
        "              worker_loss, correct, total = 0.0, 0, 0\n",
        "              for _ in range(local_steps):\n",
        "                  try:\n",
        "                      # Carica il batch dal worker\n",
        "                      inputs, labels = next(worker_iters[worker_id])\n",
        "                  except StopIteration:\n",
        "                      worker_iters[worker_id] = iter(workers[worker_id])\n",
        "                      inputs, labels = next(worker_iters[worker_id])\n",
        "\n",
        "                  inputs, labels = inputs.to(device), labels.to(device)\n",
        "                  local_optimizer.zero_grad()\n",
        "                  outputs = local_model(inputs)\n",
        "                  loss = criterion(outputs, labels)\n",
        "                  loss.backward()\n",
        "                  local_optimizer.step()\n",
        "\n",
        "                  worker_loss += loss.item()\n",
        "                  _, predicted = outputs.max(1)\n",
        "                  correct += predicted.eq(labels).sum().item()\n",
        "                  total += labels.size(0)\n",
        "\n",
        "              worker_loss /= local_steps\n",
        "              worker_accuracy = correct / total\n",
        "              worker_train_losses.append(worker_loss)\n",
        "              worker_train_accuracies.append(worker_accuracy)\n",
        "\n",
        "              local_models.append(local_model)\n",
        "\n",
        "          # Media dei parametri tra tutti i modelli locali\n",
        "          averaged_params_cpu = torch.zeros_like(averaged_params, device='cpu')\n",
        "          for local_model in local_models:\n",
        "              averaged_params_cpu += torch.nn.utils.parameters_to_vector(local_model.parameters()).cpu()\n",
        "          averaged_params_cpu.div_(num_workers)\n",
        "          averaged_params = averaged_params_cpu.to(device)\n",
        "\n",
        "          # Calcolo del momentum lento\n",
        "          gamma = hyperparams['lr']\n",
        "          slow_momentum.mul_(slowmo_params['beta']).add_((1 / gamma) * (initial_params - averaged_params))\n",
        "\n",
        "          # Aggiornamento del modello globale con il momentum lento\n",
        "          new_params = initial_params - slowmo_params['alpha'] * gamma * slow_momentum\n",
        "          torch.nn.utils.vector_to_parameters(new_params, global_model.parameters())\n",
        "\n",
        "          # Calcolo delle metriche di allenamento globali\n",
        "          global_train_loss = sum(worker_train_losses) / num_workers\n",
        "          global_train_accuracy = sum(worker_train_accuracies) / num_workers\n",
        "\n",
        "          # Validazione ogni `validation_interval`\n",
        "          if iteration % validation_interval == 0 or iteration == total_iterations:\n",
        "              train_losses.append(global_train_loss)\n",
        "              train_accuracies.append(global_train_accuracy)\n",
        "\n",
        "              global_model.eval()\n",
        "              val_loss, correct, total = 0.0, 0, 0\n",
        "              with torch.no_grad():\n",
        "                  for inputs, labels in val_loader:\n",
        "                      inputs, labels = inputs.to(device), labels.to(device)\n",
        "                      outputs = global_model(inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "                      val_loss += loss.item()\n",
        "                      _, predicted = outputs.max(1)\n",
        "                      correct += predicted.eq(labels).sum().item()\n",
        "                      total += labels.size(0)\n",
        "\n",
        "              val_loss /= len(val_loader)\n",
        "              val_acc = correct / total\n",
        "              val_losses.append(val_loss)\n",
        "              val_accuracies.append(val_acc)\n",
        "              print(f\"[Train @ Iter {iteration}] Loss: {global_train_loss:.4f}, Accuracy: {global_train_accuracy * 100:.2f}%\")\n",
        "              print(f\"[Validation @ Iter {iteration}] Loss: {val_loss:.4f}, Accuracy: {val_acc * 100:.2f}%\")\n",
        "\n",
        "          scheduler.step()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "      # Test finale\n",
        "      global_model.eval()\n",
        "      correct, total = 0, 0\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in test_loader:\n",
        "              inputs, labels = inputs.to(device), labels.to(device)\n",
        "              outputs = global_model(inputs)\n",
        "              _, predicted = outputs.max(1)\n",
        "              correct += predicted.eq(labels).sum().item()\n",
        "              total += labels.size(0)\n",
        "\n",
        "      test_acc = correct / total\n",
        "      print(f\"Test Accuracy: {test_acc * 100:.2f}\")\n",
        "\n",
        "      return train_losses, val_losses, train_accuracies, val_accuracies, test_acc, total_iterations\n",
        "\n",
        "\n",
        "# Visualization function\n",
        "def plot_results(train_losses, val_losses, train_accuracies, val_accuracies, validation_interval, total_iterations):\n",
        "    # Dynamic calculation record points\n",
        "    recorded_iterations = len(train_losses)\n",
        "    iterations = range(validation_interval, validation_interval * recorded_iterations + 1, validation_interval)\n",
        "\n",
        "    # Ensure data length is consistent\n",
        "    assert len(train_losses) == len(iterations), f\"Train losses and iterations mismatch: {len(train_losses)} vs {len(iterations)}\"\n",
        "    assert len(val_losses) == len(iterations), f\"Validation losses and iterations mismatch: {len(val_losses)} vs {len(iterations)}\"\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(iterations, train_losses, label=\"Train Loss\")\n",
        "    plt.plot(iterations, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(iterations, train_accuracies, label=\"Train Accuracy\")\n",
        "    plt.plot(iterations, val_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"Training and Validation Accuracy\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    num_epoch = 150\n",
        "    batch_size = 64\n",
        "    num_workers_list = [2, 4, 8]\n",
        "    local_steps_list = [4, 8, 16, 32, 64]\n",
        "    validation_interval = 15\n",
        "    hyperparams = {\n",
        "        'lr': 0.01,\n",
        "        'weight_decay': 0.001,\n",
        "        'momentum': 0.9,\n",
        "    }\n",
        "\n",
        "\n",
        "    slowmo_params = {\n",
        "        'alpha': 1.0,\n",
        "        'beta': 0.6\n",
        "    }\n",
        "\n",
        "\n",
        "    train_loader, val_loader, test_loader = get_dataset(batch_size)\n",
        "    train_dataset = train_loader.dataset\n",
        "\n",
        "    for num_workers in num_workers_list:\n",
        "        for local_steps in local_steps_list:\n",
        "            print(f\"Running LocalSGD with {num_workers} workers and {local_steps} local steps\")\n",
        "            train_losses, val_losses, train_accuracies, val_accuracies, test_acc, total_iterations = local_sgd(\n",
        "                train_dataset=train_dataset,\n",
        "                val_loader=val_loader,\n",
        "                test_loader=test_loader,\n",
        "                device=device,\n",
        "                num_workers=num_workers,\n",
        "                local_steps=local_steps,\n",
        "                num_epoch=num_epoch,\n",
        "                batch_size=batch_size,\n",
        "                hyperparams=hyperparams,\n",
        "                validation_interval=validation_interval,\n",
        "                slowmo_params=slowmo_params\n",
        "            )\n",
        "\n",
        "            plot_results(train_losses, val_losses, train_accuracies, val_accuracies, validation_interval, total_iterations)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSDg5xbm4fIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "bNvH-6hEsuxY",
        "t_8hLuI1syee"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}